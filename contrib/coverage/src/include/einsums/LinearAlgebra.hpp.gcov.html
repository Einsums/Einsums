<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - coverage.info - src/include/einsums/LinearAlgebra.hpp</title>
  <link rel="stylesheet" type="text/css" href="../../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../../index.html">top level</a> - <a href="index.html">src/include/einsums</a> - LinearAlgebra.hpp<span style="font-size: 80%;"> (source / <a href="LinearAlgebra.hpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">coverage.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">217</td>
            <td class="headerCovTableEntry">234</td>
            <td class="headerCovTableEntryHi">92.7 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2024-06-05 11:13:07</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">114</td>
            <td class="headerCovTableEntry">115</td>
            <td class="headerCovTableEntryHi">99.1 %</td>
          </tr>
          <tr><td><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : //----------------------------------------------------------------------------------------------</a>
<a name="2"><span class="lineNum">       2 </span>            : // Copyright (c) The Einsums Developers. All rights reserved.</a>
<a name="3"><span class="lineNum">       3 </span>            : // Licensed under the MIT License. See LICENSE.txt in the project root for license information.</a>
<a name="4"><span class="lineNum">       4 </span>            : //----------------------------------------------------------------------------------------------</a>
<a name="5"><span class="lineNum">       5 </span>            : </a>
<a name="6"><span class="lineNum">       6 </span>            : #pragma once</a>
<a name="7"><span class="lineNum">       7 </span>            : </a>
<a name="8"><span class="lineNum">       8 </span>            : #include &quot;einsums/_Common.hpp&quot;</a>
<a name="9"><span class="lineNum">       9 </span>            : #include &quot;einsums/_Compiler.hpp&quot;</a>
<a name="10"><span class="lineNum">      10 </span>            : </a>
<a name="11"><span class="lineNum">      11 </span>            : #include &quot;einsums/Blas.hpp&quot;</a>
<a name="12"><span class="lineNum">      12 </span>            : #include &quot;einsums/BlockTensor.hpp&quot;</a>
<a name="13"><span class="lineNum">      13 </span>            : #include &quot;einsums/STL.hpp&quot;</a>
<a name="14"><span class="lineNum">      14 </span>            : #include &quot;einsums/Section.hpp&quot;</a>
<a name="15"><span class="lineNum">      15 </span>            : #include &quot;einsums/Tensor.hpp&quot;</a>
<a name="16"><span class="lineNum">      16 </span>            : #include &quot;einsums/Utilities.hpp&quot;</a>
<a name="17"><span class="lineNum">      17 </span>            : #include &quot;einsums/utility/ComplexTraits.hpp&quot;</a>
<a name="18"><span class="lineNum">      18 </span>            : #include &quot;einsums/utility/SmartPointerTraits.hpp&quot;</a>
<a name="19"><span class="lineNum">      19 </span>            : #include &quot;einsums/utility/TensorTraits.hpp&quot;</a>
<a name="20"><span class="lineNum">      20 </span>            : </a>
<a name="21"><span class="lineNum">      21 </span>            : #include &lt;algorithm&gt;</a>
<a name="22"><span class="lineNum">      22 </span>            : #include &lt;cmath&gt;</a>
<a name="23"><span class="lineNum">      23 </span>            : #include &lt;cstddef&gt;</a>
<a name="24"><span class="lineNum">      24 </span>            : #include &lt;limits&gt;</a>
<a name="25"><span class="lineNum">      25 </span>            : #include &lt;stdexcept&gt;</a>
<a name="26"><span class="lineNum">      26 </span>            : #include &lt;tuple&gt;</a>
<a name="27"><span class="lineNum">      27 </span>            : #include &lt;type_traits&gt;</a>
<a name="28"><span class="lineNum">      28 </span>            : </a>
<a name="29"><span class="lineNum">      29 </span>            : // For some stupid reason doxygen can't handle this macro here but it can in other files.</a>
<a name="30"><span class="lineNum">      30 </span>            : #if !defined(DOXYGEN_SHOULD_SKIP_THIS)</a>
<a name="31"><span class="lineNum">      31 </span>            : BEGIN_EINSUMS_NAMESPACE_HPP(einsums::linear_algebra)</a>
<a name="32"><span class="lineNum">      32 </span>            : #else</a>
<a name="33"><span class="lineNum">      33 </span>            : namespace einsums::linear_algebra {</a>
<a name="34"><span class="lineNum">      34 </span>            : #endif</a>
<a name="35"><span class="lineNum">      35 </span>            : </a>
<a name="36"><span class="lineNum">      36 </span>            : /**</a>
<a name="37"><span class="lineNum">      37 </span>            :  * @brief Computes the square sum of a tensor.</a>
<a name="38"><span class="lineNum">      38 </span>            :  *</a>
<a name="39"><span class="lineNum">      39 </span>            :  * returns the values scale_out and sumsq_out such that</a>
<a name="40"><span class="lineNum">      40 </span>            :  * \f[</a>
<a name="41"><span class="lineNum">      41 </span>            :  *   (scale_{out}^{2})*sumsq_{out} = a( 1 )^{2} +...+ a( n )^{2} + (scale_{in}^{2})*sumsq_{in},</a>
<a name="42"><span class="lineNum">      42 </span>            :  * \f]</a>
<a name="43"><span class="lineNum">      43 </span>            :  *</a>
<a name="44"><span class="lineNum">      44 </span>            :  * Under the hood the LAPACK routine `lassq` is used.</a>
<a name="45"><span class="lineNum">      45 </span>            :  *</a>
<a name="46"><span class="lineNum">      46 </span>            :  * @code</a>
<a name="47"><span class="lineNum">      47 </span>            :  * NEED TO ADD AN EXAMPLE</a>
<a name="48"><span class="lineNum">      48 </span>            :  * @endcode</a>
<a name="49"><span class="lineNum">      49 </span>            :  *</a>
<a name="50"><span class="lineNum">      50 </span>            :  * @tparam AType The type of the tensor</a>
<a name="51"><span class="lineNum">      51 </span>            :  * @tparam ADataType The underlying data type of the tensor</a>
<a name="52"><span class="lineNum">      52 </span>            :  * @tparam ARank The rank of the tensor</a>
<a name="53"><span class="lineNum">      53 </span>            :  * @param a The tensor to compute the sum of squares for</a>
<a name="54"><span class="lineNum">      54 </span>            :  * @param scale scale_in and scale_out for the equation provided</a>
<a name="55"><span class="lineNum">      55 </span>            :  * @param sumsq sumsq_in and sumsq_out for the equation provided</a>
<a name="56"><span class="lineNum">      56 </span>            :  */</a>
<a name="57"><span class="lineNum">      57 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, typename ADataType, size_t ARank&gt;</a>
<a name="58"><span class="lineNum">      58 </span>            :     requires CoreRankTensor&lt;AType&lt;ADataType, ARank&gt;, 1, ADataType&gt;</a>
<a name="59"><span class="lineNum">      59 </span><span class="lineCov">          4 : void sum_square(const AType&lt;ADataType, ARank&gt; &amp;a, RemoveComplexT&lt;ADataType&gt; *scale, RemoveComplexT&lt;ADataType&gt; *sumsq) {</span></a>
<a name="60"><span class="lineNum">      60 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;ADataType, ARank&gt;, ARank, ADataType&gt;) {</a>
<a name="61"><span class="lineNum">      61 </span>            :         *sumsq = 0;</a>
<a name="62"><span class="lineNum">      62 </span>            : </a>
<a name="63"><span class="lineNum">      63 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="64"><span class="lineNum">      64 </span>            :         for (int i = 0; i &lt; a.num_blocks(); i++) {</a>
<a name="65"><span class="lineNum">      65 </span>            :             if (a.block_dim(i) == 0) {</a>
<a name="66"><span class="lineNum">      66 </span>            :                 continue;</a>
<a name="67"><span class="lineNum">      67 </span>            :             }</a>
<a name="68"><span class="lineNum">      68 </span>            :             RemoveComplexT&lt;ADataType&gt; out;</a>
<a name="69"><span class="lineNum">      69 </span>            : </a>
<a name="70"><span class="lineNum">      70 </span>            :             sum_square(a.block(i), scale, &amp;out);</a>
<a name="71"><span class="lineNum">      71 </span>            : </a>
<a name="72"><span class="lineNum">      72 </span>            :             *sumsq += out;</a>
<a name="73"><span class="lineNum">      73 </span>            :         }</a>
<a name="74"><span class="lineNum">      74 </span>            :     } else {</a>
<a name="75"><span class="lineNum">      75 </span><span class="lineCov">         12 :         LabeledSection0();</span></a>
<a name="76"><span class="lineNum">      76 </span>            : </a>
<a name="77"><span class="lineNum">      77 </span><span class="lineCov">          4 :         int n    = a.dim(0);</span></a>
<a name="78"><span class="lineNum">      78 </span><span class="lineCov">          4 :         int incx = a.stride(0);</span></a>
<a name="79"><span class="lineNum">      79 </span><span class="lineCov">          4 :         blas::lassq(n, a.data(), incx, scale, sumsq);</span></a>
<a name="80"><span class="lineNum">      80 </span><span class="lineCov">          4 :     }</span></a>
<a name="81"><span class="lineNum">      81 </span><span class="lineCov">          4 : }</span></a>
<a name="82"><span class="lineNum">      82 </span>            : </a>
<a name="83"><span class="lineNum">      83 </span>            : /**</a>
<a name="84"><span class="lineNum">      84 </span>            :  * @brief General matrix multiplication.</a>
<a name="85"><span class="lineNum">      85 </span>            :  *</a>
<a name="86"><span class="lineNum">      86 </span>            :  * Takes two rank-2 tensors ( \p A and \p B ) performs the multiplication and stores the result in to another</a>
<a name="87"><span class="lineNum">      87 </span>            :  * rank-2 tensor that is passed in ( \p C ).</a>
<a name="88"><span class="lineNum">      88 </span>            :  *</a>
<a name="89"><span class="lineNum">      89 </span>            :  * In this equation, \p TransA is op(A) and \p TransB is op(B).</a>
<a name="90"><span class="lineNum">      90 </span>            :  * @f[</a>
<a name="91"><span class="lineNum">      91 </span>            :  * C = \alpha \;op(A) \;op(B) + \beta C</a>
<a name="92"><span class="lineNum">      92 </span>            :  * @f]</a>
<a name="93"><span class="lineNum">      93 </span>            :  *</a>
<a name="94"><span class="lineNum">      94 </span>            :  * @code</a>
<a name="95"><span class="lineNum">      95 </span>            :  * auto A = einsums::create_random_tensor(&quot;A&quot;, 3, 3);</a>
<a name="96"><span class="lineNum">      96 </span>            :  * auto B = einsums::create_random_tensor(&quot;B&quot;, 3, 3);</a>
<a name="97"><span class="lineNum">      97 </span>            :  * auto C = einsums::create_tensor(&quot;C&quot;, 3, 3);</a>
<a name="98"><span class="lineNum">      98 </span>            :  *</a>
<a name="99"><span class="lineNum">      99 </span>            :  * einsums::linear_algebra::gemm&lt;false, false&gt;(1.0, A, B, 0.0, &amp;C);</a>
<a name="100"><span class="lineNum">     100 </span>            :  * @endcode</a>
<a name="101"><span class="lineNum">     101 </span>            :  *</a>
<a name="102"><span class="lineNum">     102 </span>            :  * @tparam TransA Tranpose A? true or false</a>
<a name="103"><span class="lineNum">     103 </span>            :  * @tparam TransB Tranpose B? true or false</a>
<a name="104"><span class="lineNum">     104 </span>            :  * @param alpha Scaling factor for the product of A and B</a>
<a name="105"><span class="lineNum">     105 </span>            :  * @param A First input tensor</a>
<a name="106"><span class="lineNum">     106 </span>            :  * @param B Second input tensor</a>
<a name="107"><span class="lineNum">     107 </span>            :  * @param beta Scaling factor for the output tensor C</a>
<a name="108"><span class="lineNum">     108 </span>            :  * @param C Output tensor</a>
<a name="109"><span class="lineNum">     109 </span>            :  * @tparam T the underlying data type</a>
<a name="110"><span class="lineNum">     110 </span>            :  */</a>
<a name="111"><span class="lineNum">     111 </span>            : template &lt;bool TransA, bool TransB, template &lt;typename, size_t&gt; typename AType, template &lt;typename, size_t&gt; typename BType,</a>
<a name="112"><span class="lineNum">     112 </span>            :           template &lt;typename, size_t&gt; typename CType, size_t Rank, typename T, typename U&gt;</a>
<a name="113"><span class="lineNum">     113 </span>            :     requires requires {</a>
<a name="114"><span class="lineNum">     114 </span>            :         requires CoreRankTensor&lt;AType&lt;T, Rank&gt;, 2, T&gt;;</a>
<a name="115"><span class="lineNum">     115 </span>            :         requires CoreRankTensor&lt;BType&lt;T, Rank&gt;, 2, T&gt;;</a>
<a name="116"><span class="lineNum">     116 </span>            :         requires CoreRankTensor&lt;CType&lt;T, Rank&gt;, 2, T&gt;;</a>
<a name="117"><span class="lineNum">     117 </span>            :         requires !CoreRankBlockTensor&lt;CType&lt;T, Rank&gt;, 2, T&gt; ||</a>
<a name="118"><span class="lineNum">     118 </span>            :                      (CoreRankBlockTensor&lt;AType&lt;T, Rank&gt;, 2, T&gt; &amp;&amp; CoreRankBlockTensor&lt;BType&lt;T, Rank&gt;, 2, T&gt;);</a>
<a name="119"><span class="lineNum">     119 </span>            :         requires std::convertible_to&lt;U, T&gt;;</a>
<a name="120"><span class="lineNum">     120 </span>            :     }</a>
<a name="121"><span class="lineNum">     121 </span><span class="lineCov">        576 : void gemm(const U alpha, const AType&lt;T, Rank&gt; &amp;A, const BType&lt;T, Rank&gt; &amp;B, const U beta, CType&lt;T, Rank&gt; *C) {</span></a>
<a name="122"><span class="lineNum">     122 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp;</a>
<a name="123"><span class="lineNum">     123 </span>            :                   einsums::detail::IsIncoreRankBlockTensorV&lt;BType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp;</a>
<a name="124"><span class="lineNum">     124 </span>            :                   einsums::detail::IsIncoreRankBlockTensorV&lt;CType&lt;T, Rank&gt;, Rank, T&gt;) {</a>
<a name="125"><span class="lineNum">     125 </span>            : </a>
<a name="126"><span class="lineNum">     126 </span><span class="lineCov">          1 :         if (A.num_blocks() != B.num_blocks() || A.num_blocks() != C-&gt;num_blocks() || B.num_blocks() != C-&gt;num_blocks()) {</span></a>
<a name="127"><span class="lineNum">     127 </span><span class="lineNoCov">          0 :             throw std::runtime_error(&quot;gemm: Tensors need the same number of blocks.&quot;);</span></a>
<a name="128"><span class="lineNum">     128 </span>            :         }</a>
<a name="129"><span class="lineNum">     129 </span>            : </a>
<a name="130"><span class="lineNum">     130 </span><span class="lineCov">          1 :         EINSUMS_OMP_PARALLEL_FOR</span></a>
<a name="131"><span class="lineNum">     131 </span>            :         for (int i = 0; i &lt; A.num_blocks(); i++) {</a>
<a name="132"><span class="lineNum">     132 </span>            :             if (A.block_dim(i) == 0) {</a>
<a name="133"><span class="lineNum">     133 </span>            :                 continue;</a>
<a name="134"><span class="lineNum">     134 </span>            :             }</a>
<a name="135"><span class="lineNum">     135 </span>            :             gemm&lt;TransA, TransB&gt;(static_cast&lt;T&gt;(alpha), A.block(i), B.block(i), static_cast&lt;T&gt;(beta), &amp;(C-&gt;block(i)));</a>
<a name="136"><span class="lineNum">     136 </span>            :         }</a>
<a name="137"><span class="lineNum">     137 </span>            : </a>
<a name="138"><span class="lineNum">     138 </span><span class="lineCov">          1 :         return;</span></a>
<a name="139"><span class="lineNum">     139 </span>            :     } else if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp;</a>
<a name="140"><span class="lineNum">     140 </span>            :                          einsums::detail::IsIncoreRankBlockTensorV&lt;BType&lt;T, Rank&gt;, Rank, T&gt;) {</a>
<a name="141"><span class="lineNum">     141 </span>            :         if (A.num_blocks() != B.num_blocks()) {</a>
<a name="142"><span class="lineNum">     142 </span>            :             gemm&lt;TransA, TransB&gt;(static_cast&lt;T&gt;(alpha), (Tensor&lt;T, 2&gt;)A, (Tensor&lt;T, 2&gt;)B, static_cast&lt;T&gt;(beta), C);</a>
<a name="143"><span class="lineNum">     143 </span>            :         } else {</a>
<a name="144"><span class="lineNum">     144 </span>            :             EINSUMS_OMP_PARALLEL_FOR</a>
<a name="145"><span class="lineNum">     145 </span>            :             for (int i = 0; i &lt; A.num_blocks(); i++) {</a>
<a name="146"><span class="lineNum">     146 </span>            :                 if (A.block_dim(i) == 0) {</a>
<a name="147"><span class="lineNum">     147 </span>            :                     continue;</a>
<a name="148"><span class="lineNum">     148 </span>            :                 }</a>
<a name="149"><span class="lineNum">     149 </span>            :                 gemm&lt;TransA, TransB&gt;(static_cast&lt;T&gt;(alpha), A.block(i), B.block(i), static_cast&lt;T&gt;(beta),</a>
<a name="150"><span class="lineNum">     150 </span>            :                                      &amp;((*C)(A.block_range(i), A.block_range(i))));</a>
<a name="151"><span class="lineNum">     151 </span>            :             }</a>
<a name="152"><span class="lineNum">     152 </span>            :         }</a>
<a name="153"><span class="lineNum">     153 </span>            : </a>
<a name="154"><span class="lineNum">     154 </span>            :         return;</a>
<a name="155"><span class="lineNum">     155 </span>            :     } else {</a>
<a name="156"><span class="lineNum">     156 </span><span class="lineCov">       1725 :         LabeledSection0();</span></a>
<a name="157"><span class="lineNum">     157 </span>            : </a>
<a name="158"><span class="lineNum">     158 </span><span class="lineCov">        575 :         auto m = C-&gt;dim(0), n = C-&gt;dim(1), k = TransA ? A.dim(0) : A.dim(1);</span></a>
<a name="159"><span class="lineNum">     159 </span><span class="lineCov">        575 :         auto lda = A.stride(0), ldb = B.stride(0), ldc = C-&gt;stride(0);</span></a>
<a name="160"><span class="lineNum">     160 </span>            : </a>
<a name="161"><span class="lineNum">     161 </span><span class="lineCov">        575 :         blas::gemm(TransA ? 't' : 'n', TransB ? 't' : 'n', m, n, k, static_cast&lt;T&gt;(alpha), A.data(), lda, B.data(), ldb,</span></a>
<a name="162"><span class="lineNum">     162 </span>            :                    static_cast&lt;T&gt;(beta), C-&gt;data(), ldc);</a>
<a name="163"><span class="lineNum">     163 </span><span class="lineCov">        575 :     }</span></a>
<a name="164"><span class="lineNum">     164 </span><span class="lineCov">        575 : }</span></a>
<a name="165"><span class="lineNum">     165 </span>            : </a>
<a name="166"><span class="lineNum">     166 </span>            : /**</a>
<a name="167"><span class="lineNum">     167 </span>            :  * @brief General matrix multiplication. Returns new tensor.</a>
<a name="168"><span class="lineNum">     168 </span>            :  *</a>
<a name="169"><span class="lineNum">     169 </span>            :  * Takes two rank-2 tensors performs the multiplication and returns the result</a>
<a name="170"><span class="lineNum">     170 </span>            :  *</a>
<a name="171"><span class="lineNum">     171 </span>            :  * @code</a>
<a name="172"><span class="lineNum">     172 </span>            :  * auto A = einsums::create_random_tensor(&quot;A&quot;, 3, 3);</a>
<a name="173"><span class="lineNum">     173 </span>            :  * auto B = einsums::create_random_tensor(&quot;B&quot;, 3, 3);</a>
<a name="174"><span class="lineNum">     174 </span>            :  *</a>
<a name="175"><span class="lineNum">     175 </span>            :  * auto C = einsums::linear_algebra::gemm&lt;false, false&gt;(1.0, A, B);</a>
<a name="176"><span class="lineNum">     176 </span>            :  * @endcode</a>
<a name="177"><span class="lineNum">     177 </span>            :  *</a>
<a name="178"><span class="lineNum">     178 </span>            :  * @tparam TransA Tranpose A?</a>
<a name="179"><span class="lineNum">     179 </span>            :  * @tparam TransB Tranpose B?</a>
<a name="180"><span class="lineNum">     180 </span>            :  * @param alpha Scaling factor for the product of A and B</a>
<a name="181"><span class="lineNum">     181 </span>            :  * @param A First input tensor</a>
<a name="182"><span class="lineNum">     182 </span>            :  * @param B Second input tensor</a>
<a name="183"><span class="lineNum">     183 </span>            :  * @returns resulting tensor</a>
<a name="184"><span class="lineNum">     184 </span>            :  * @tparam T the underlying data type</a>
<a name="185"><span class="lineNum">     185 </span>            :  */</a>
<a name="186"><span class="lineNum">     186 </span>            : template &lt;bool TransA, bool TransB, template &lt;typename, size_t&gt; typename AType, template &lt;typename, size_t&gt; typename BType, size_t Rank,</a>
<a name="187"><span class="lineNum">     187 </span>            :           typename T, typename U&gt;</a>
<a name="188"><span class="lineNum">     188 </span>            :     requires requires {</a>
<a name="189"><span class="lineNum">     189 </span>            :         requires CoreRankTensor&lt;AType&lt;T, Rank&gt;, 2, T&gt;;</a>
<a name="190"><span class="lineNum">     190 </span>            :         requires CoreRankTensor&lt;BType&lt;T, Rank&gt;, 2, T&gt;;</a>
<a name="191"><span class="lineNum">     191 </span>            :         requires std::convertible_to&lt;U, T&gt;;</a>
<a name="192"><span class="lineNum">     192 </span>            :     }</a>
<a name="193"><span class="lineNum">     193 </span><span class="lineCov">         22 : auto gemm(const U alpha, const AType&lt;T, Rank&gt; &amp;A, const BType&lt;T, Rank&gt; &amp;B) -&gt; Tensor&lt;T, 2&gt; {</span></a>
<a name="194"><span class="lineNum">     194 </span><span class="lineCov">         66 :     LabeledSection0();</span></a>
<a name="195"><span class="lineNum">     195 </span>            : </a>
<a name="196"><span class="lineNum">     196 </span><span class="lineCov">         30 :     Tensor&lt;T, 2&gt; C{&quot;gemm result&quot;, TransA ? A.dim(1) : A.dim(0), TransB ? B.dim(0) : B.dim(1)};</span></a>
<a name="197"><span class="lineNum">     197 </span>            : </a>
<a name="198"><span class="lineNum">     198 </span><span class="lineCov">         22 :     gemm&lt;TransA, TransB&gt;(static_cast&lt;T&gt;(alpha), A, B, static_cast&lt;T&gt;(0.0), &amp;C);</span></a>
<a name="199"><span class="lineNum">     199 </span>            : </a>
<a name="200"><span class="lineNum">     200 </span><span class="lineCov">         22 :     return C;</span></a>
<a name="201"><span class="lineNum">     201 </span><span class="lineCov">         22 : }</span></a>
<a name="202"><span class="lineNum">     202 </span>            : </a>
<a name="203"><span class="lineNum">     203 </span>            : /**</a>
<a name="204"><span class="lineNum">     204 </span>            :  * @brief General matrix-vector multiplication.</a>
<a name="205"><span class="lineNum">     205 </span>            :  *</a>
<a name="206"><span class="lineNum">     206 </span>            :  * This function performs one of the matrix-vector operations</a>
<a name="207"><span class="lineNum">     207 </span>            :  * \f[</a>
<a name="208"><span class="lineNum">     208 </span>            :  *    y := alpha*A*z + beta*y\mathrm{,\ or\ }y := alpha*A^{T}*z + beta*y,</a>
<a name="209"><span class="lineNum">     209 </span>            :  * \f]</a>
<a name="210"><span class="lineNum">     210 </span>            :  * where alpha and beta are scalars, z and y are vectors and A is an</a>
<a name="211"><span class="lineNum">     211 </span>            :  * \f$m\f$ by \f$n\f$ matrix.</a>
<a name="212"><span class="lineNum">     212 </span>            :  *</a>
<a name="213"><span class="lineNum">     213 </span>            :  * @code</a>
<a name="214"><span class="lineNum">     214 </span>            :  * NEED TO ADD AN EXAMPLE</a>
<a name="215"><span class="lineNum">     215 </span>            :  * @endcode</a>
<a name="216"><span class="lineNum">     216 </span>            :  *</a>
<a name="217"><span class="lineNum">     217 </span>            :  * @tparam TransA Transpose matrix A? true or false</a>
<a name="218"><span class="lineNum">     218 </span>            :  * @tparam AType The type of the matrix A</a>
<a name="219"><span class="lineNum">     219 </span>            :  * @tparam XType The type of the vector z</a>
<a name="220"><span class="lineNum">     220 </span>            :  * @tparam YType The type of the vector y</a>
<a name="221"><span class="lineNum">     221 </span>            :  * @tparam ARank The rank of the matrix A</a>
<a name="222"><span class="lineNum">     222 </span>            :  * @tparam XYRank  The rank of the vectors z and y</a>
<a name="223"><span class="lineNum">     223 </span>            :  * @tparam T The underlying data type</a>
<a name="224"><span class="lineNum">     224 </span>            :  * @param alpha Scaling factor for the product of A and z</a>
<a name="225"><span class="lineNum">     225 </span>            :  * @param A Matrix A</a>
<a name="226"><span class="lineNum">     226 </span>            :  * @param z Vector z</a>
<a name="227"><span class="lineNum">     227 </span>            :  * @param beta Scaling factor for the output vector y</a>
<a name="228"><span class="lineNum">     228 </span>            :  * @param y Output vector y</a>
<a name="229"><span class="lineNum">     229 </span>            :  */</a>
<a name="230"><span class="lineNum">     230 </span>            : template &lt;bool TransA, template &lt;typename, size_t&gt; typename AType, template &lt;typename, size_t&gt; typename XType,</a>
<a name="231"><span class="lineNum">     231 </span>            :           template &lt;typename, size_t&gt; typename YType, size_t ARank, size_t XYRank, typename T, typename U&gt;</a>
<a name="232"><span class="lineNum">     232 </span>            :     requires requires {</a>
<a name="233"><span class="lineNum">     233 </span>            :         requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;;</a>
<a name="234"><span class="lineNum">     234 </span>            :         requires CoreRankTensor&lt;XType&lt;T, XYRank&gt;, 1, T&gt;;</a>
<a name="235"><span class="lineNum">     235 </span>            :         requires CoreRankTensor&lt;YType&lt;T, XYRank&gt;, 1, T&gt;;</a>
<a name="236"><span class="lineNum">     236 </span>            :         requires std::convertible_to&lt;U, T&gt;; // Make sure the alpha and beta can be converted to T</a>
<a name="237"><span class="lineNum">     237 </span>            :     }</a>
<a name="238"><span class="lineNum">     238 </span><span class="lineCov">         15 : void gemv(const U alpha, const AType&lt;T, ARank&gt; &amp;A, const XType&lt;T, XYRank&gt; &amp;z, const U beta, YType&lt;T, XYRank&gt; *y) {</span></a>
<a name="239"><span class="lineNum">     239 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt;) {</a>
<a name="240"><span class="lineNum">     240 </span>            : </a>
<a name="241"><span class="lineNum">     241 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="242"><span class="lineNum">     242 </span>            :         for (int i = 0; i &lt; A.num_blocks(); i++) {</a>
<a name="243"><span class="lineNum">     243 </span>            :             if (A.block_dim(i) == 0) {</a>
<a name="244"><span class="lineNum">     244 </span>            :                 continue;</a>
<a name="245"><span class="lineNum">     245 </span>            :             }</a>
<a name="246"><span class="lineNum">     246 </span>            :             gemv(static_cast&lt;T&gt;(alpha), A.block(i), x(A.block_range(i)), static_cast&lt;T&gt;(beta), &amp;((*y)(A.block_range(i))));</a>
<a name="247"><span class="lineNum">     247 </span>            :         }</a>
<a name="248"><span class="lineNum">     248 </span>            : </a>
<a name="249"><span class="lineNum">     249 </span>            :     } else {</a>
<a name="250"><span class="lineNum">     250 </span><span class="lineCov">         60 :         LabeledSection1(fmt::format(&quot;&lt;TransA={}&gt;&quot;, TransA));</span></a>
<a name="251"><span class="lineNum">     251 </span><span class="lineCov">         15 :         auto m = A.dim(0), n = A.dim(1);</span></a>
<a name="252"><span class="lineNum">     252 </span><span class="lineCov">         15 :         auto lda  = A.stride(0);</span></a>
<a name="253"><span class="lineNum">     253 </span><span class="lineCov">         15 :         auto incx = z.stride(0);</span></a>
<a name="254"><span class="lineNum">     254 </span><span class="lineCov">         15 :         auto incy = y-&gt;stride(0);</span></a>
<a name="255"><span class="lineNum">     255 </span>            : </a>
<a name="256"><span class="lineNum">     256 </span><span class="lineCov">         15 :         blas::gemv(TransA ? 't' : 'n', m, n, static_cast&lt;T&gt;(alpha), A.data(), lda, z.data(), incx, static_cast&lt;T&gt;(beta), y-&gt;data(), incy);</span></a>
<a name="257"><span class="lineNum">     257 </span><span class="lineCov">         15 :     }</span></a>
<a name="258"><span class="lineNum">     258 </span><span class="lineCov">         15 : }</span></a>
<a name="259"><span class="lineNum">     259 </span>            : </a>
<a name="260"><span class="lineNum">     260 </span>            : /**</a>
<a name="261"><span class="lineNum">     261 </span>            :  * Computes all eigenvalues and, optionally, eigenvectors of a real symmetric matrix.</a>
<a name="262"><span class="lineNum">     262 </span>            :  *</a>
<a name="263"><span class="lineNum">     263 </span>            :  * This routines assumes the upper triangle of A is stored. The lower triangle is not referenced.</a>
<a name="264"><span class="lineNum">     264 </span>            :  *</a>
<a name="265"><span class="lineNum">     265 </span>            :  * @code</a>
<a name="266"><span class="lineNum">     266 </span>            :  * // Create tensors A and b.</a>
<a name="267"><span class="lineNum">     267 </span>            :  * auto A = einsums::create_tensor(&quot;A&quot;, 3, 3);</a>
<a name="268"><span class="lineNum">     268 </span>            :  * auto b = einsums::create_tensor(&quot;b&quot;, 3);</a>
<a name="269"><span class="lineNum">     269 </span>            :  *</a>
<a name="270"><span class="lineNum">     270 </span>            :  * // Fill A with the symmetric data.</a>
<a name="271"><span class="lineNum">     271 </span>            :  * A.vector_data() = einsums::VectorData{1.0, 2.0, 3.0, 2.0, 4.0, 5.0, 3.0, 5.0, 6.0};</a>
<a name="272"><span class="lineNum">     272 </span>            :  *</a>
<a name="273"><span class="lineNum">     273 </span>            :  * // On exit, A is destroyed and replaced with the eigenvectors.</a>
<a name="274"><span class="lineNum">     274 </span>            :  * // b is replaced with the eigenvalues in ascending order.</a>
<a name="275"><span class="lineNum">     275 </span>            :  * einsums::linear_algebra::syev(&amp;A, &amp;b);</a>
<a name="276"><span class="lineNum">     276 </span>            :  * @endcode</a>
<a name="277"><span class="lineNum">     277 </span>            :  *</a>
<a name="278"><span class="lineNum">     278 </span>            :  * @tparam AType The type of the tensor A</a>
<a name="279"><span class="lineNum">     279 </span>            :  * @tparam ARank The rank of the tensor A (required to be 2)</a>
<a name="280"><span class="lineNum">     280 </span>            :  * @tparam WType The type of the tensor W</a>
<a name="281"><span class="lineNum">     281 </span>            :  * @tparam WRank The rank of the tensor W (required to be 1)</a>
<a name="282"><span class="lineNum">     282 </span>            :  * @tparam T The underlying data type (required to be real)</a>
<a name="283"><span class="lineNum">     283 </span>            :  * @tparam ComputeEigenvectors If true, eigenvalues and eigenvectors are computed. If false, only eigenvalues are computed. Defaults to</a>
<a name="284"><span class="lineNum">     284 </span>            :  * true.</a>
<a name="285"><span class="lineNum">     285 </span>            :  * @param A</a>
<a name="286"><span class="lineNum">     286 </span>            :  *   On entry, the symmetric matrix A in the leading N-by-N upper triangular part of A.</a>
<a name="287"><span class="lineNum">     287 </span>            :  *   On exit, if eigenvectors are requested, the orthonormal eigenvectors of A.</a>
<a name="288"><span class="lineNum">     288 </span>            :  *   Any data previously stored in A is destroyed.</a>
<a name="289"><span class="lineNum">     289 </span>            :  * @param W On exit, the eigenvalues in ascending order.</a>
<a name="290"><span class="lineNum">     290 </span>            :  */</a>
<a name="291"><span class="lineNum">     291 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, template &lt;typename, size_t&gt; typename WType, size_t WRank, typename T,</a>
<a name="292"><span class="lineNum">     292 </span>            :           bool ComputeEigenvectors = true&gt;</a>
<a name="293"><span class="lineNum">     293 </span>            :     requires requires {</a>
<a name="294"><span class="lineNum">     294 </span>            :         requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;;</a>
<a name="295"><span class="lineNum">     295 </span>            :         requires CoreRankTensor&lt;WType&lt;T, WRank&gt;, 1, T&gt;;</a>
<a name="296"><span class="lineNum">     296 </span>            :         requires !Complex&lt;T&gt;;</a>
<a name="297"><span class="lineNum">     297 </span>            :     }</a>
<a name="298"><span class="lineNum">     298 </span><span class="lineCov">         18 : void syev(AType&lt;T, ARank&gt; *A, WType&lt;T, WRank&gt; *W) {</span></a>
<a name="299"><span class="lineNum">     299 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt;) {</a>
<a name="300"><span class="lineNum">     300 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="301"><span class="lineNum">     301 </span>            :         for (int i = 0; i &lt; A-&gt;num_blocks(); i++) {</a>
<a name="302"><span class="lineNum">     302 </span>            :             if (A-&gt;block_dim(i) == 0) {</a>
<a name="303"><span class="lineNum">     303 </span>            :                 continue;</a>
<a name="304"><span class="lineNum">     304 </span>            :             }</a>
<a name="305"><span class="lineNum">     305 </span>            :             if (A-&gt;block_range(i)[1] &lt; 0) {</a>
<a name="306"><span class="lineNum">     306 </span>            :                 printf(&quot;Hi&quot;);</a>
<a name="307"><span class="lineNum">     307 </span>            :             }</a>
<a name="308"><span class="lineNum">     308 </span>            :             auto out_block = (*W)(A-&gt;block_range(i));</a>
<a name="309"><span class="lineNum">     309 </span>            :             syev(&amp;(A-&gt;block(i)), &amp;out_block);</a>
<a name="310"><span class="lineNum">     310 </span>            :         }</a>
<a name="311"><span class="lineNum">     311 </span>            :     } else {</a>
<a name="312"><span class="lineNum">     312 </span>            : </a>
<a name="313"><span class="lineNum">     313 </span><span class="lineCov">         72 :         LabeledSection1(fmt::format(&quot;&lt;ComputeEigenvectors={}&gt;&quot;, ComputeEigenvectors));</span></a>
<a name="314"><span class="lineNum">     314 </span>            : </a>
<a name="315"><span class="lineNum">     315 </span><span class="lineCov">         18 :         assert(A-&gt;dim(0) == A-&gt;dim(1));</span></a>
<a name="316"><span class="lineNum">     316 </span>            : </a>
<a name="317"><span class="lineNum">     317 </span><span class="lineCov">         18 :         auto           n     = A-&gt;dim(0);</span></a>
<a name="318"><span class="lineNum">     318 </span><span class="lineCov">         18 :         auto           lda   = A-&gt;stride(0);</span></a>
<a name="319"><span class="lineNum">     319 </span><span class="lineCov">         18 :         int            lwork = 3 * n;</span></a>
<a name="320"><span class="lineNum">     320 </span><span class="lineCov">         18 :         std::vector&lt;T&gt; work(lwork);</span></a>
<a name="321"><span class="lineNum">     321 </span>            : </a>
<a name="322"><span class="lineNum">     322 </span><span class="lineCov">         18 :         blas::syev(ComputeEigenvectors ? 'v' : 'n', 'u', n, A-&gt;data(), lda, W-&gt;data(), work.data(), lwork);</span></a>
<a name="323"><span class="lineNum">     323 </span><span class="lineCov">         18 :     }</span></a>
<a name="324"><span class="lineNum">     324 </span><span class="lineCov">         18 : }</span></a>
<a name="325"><span class="lineNum">     325 </span>            : </a>
<a name="326"><span class="lineNum">     326 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, template &lt;Complex, size_t&gt; typename WType, size_t WRank, typename T,</a>
<a name="327"><span class="lineNum">     327 </span>            :           bool ComputeLeftRightEigenvectors = true&gt;</a>
<a name="328"><span class="lineNum">     328 </span>            :     requires requires {</a>
<a name="329"><span class="lineNum">     329 </span>            :         requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;;</a>
<a name="330"><span class="lineNum">     330 </span>            :         requires CoreRankTensor&lt;WType&lt;AddComplexT&lt;T&gt;, WRank&gt;, 1, AddComplexT&lt;T&gt;&gt;;</a>
<a name="331"><span class="lineNum">     331 </span>            :     }</a>
<a name="332"><span class="lineNum">     332 </span><span class="lineCov">          4 : void geev(AType&lt;T, ARank&gt; *A, WType&lt;AddComplexT&lt;T&gt;, WRank&gt; *W, AType&lt;T, ARank&gt; *lvecs, AType&lt;T, ARank&gt; *rvecs) {</span></a>
<a name="333"><span class="lineNum">     333 </span><span class="lineCov">         16 :     LabeledSection1(fmt::format(&quot;&lt;ComputeLeftRightEigenvectors={}&gt;&quot;, ComputeLeftRightEigenvectors));</span></a>
<a name="334"><span class="lineNum">     334 </span>            : </a>
<a name="335"><span class="lineNum">     335 </span><span class="lineCov">          4 :     assert(A-&gt;dim(0) == A-&gt;dim(1));</span></a>
<a name="336"><span class="lineNum">     336 </span><span class="lineCov">          4 :     assert(W-&gt;dim(0) == A-&gt;dim(0));</span></a>
<a name="337"><span class="lineNum">     337 </span><span class="lineCov">          4 :     assert(A-&gt;dim(0) == lvecs-&gt;dim(0));</span></a>
<a name="338"><span class="lineNum">     338 </span><span class="lineCov">          4 :     assert(A-&gt;dim(1) == lvecs-&gt;dim(1));</span></a>
<a name="339"><span class="lineNum">     339 </span><span class="lineCov">          4 :     assert(A-&gt;dim(0) == rvecs-&gt;dim(0));</span></a>
<a name="340"><span class="lineNum">     340 </span><span class="lineCov">          4 :     assert(A-&gt;dim(1) == rvecs-&gt;dim(1));</span></a>
<a name="341"><span class="lineNum">     341 </span>            : </a>
<a name="342"><span class="lineNum">     342 </span><span class="lineCov">          8 :     blas::geev(ComputeLeftRightEigenvectors ? 'v' : 'n', ComputeLeftRightEigenvectors ? 'v' : 'n', A-&gt;dim(0), A-&gt;data(), A-&gt;stride(0),</span></a>
<a name="343"><span class="lineNum">     343 </span><span class="lineCov">          4 :                W-&gt;data(), lvecs-&gt;data(), lvecs-&gt;stride(0), rvecs-&gt;data(), rvecs-&gt;stride(0));</span></a>
<a name="344"><span class="lineNum">     344 </span><span class="lineCov">          4 : }</span></a>
<a name="345"><span class="lineNum">     345 </span>            : </a>
<a name="346"><span class="lineNum">     346 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, template &lt;typename, size_t&gt; typename WType, size_t WRank, typename T,</a>
<a name="347"><span class="lineNum">     347 </span>            :           bool ComputeEigenvectors = true&gt;</a>
<a name="348"><span class="lineNum">     348 </span>            :     requires requires {</a>
<a name="349"><span class="lineNum">     349 </span>            :         requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;;</a>
<a name="350"><span class="lineNum">     350 </span>            :         requires CoreRankTensor&lt;WType&lt;T, WRank&gt;, 1, T&gt;;</a>
<a name="351"><span class="lineNum">     351 </span>            :         requires Complex&lt;T&gt;;</a>
<a name="352"><span class="lineNum">     352 </span>            :     }</a>
<a name="353"><span class="lineNum">     353 </span><span class="lineCov">          2 : void heev(AType&lt;T, ARank&gt; *A, WType&lt;RemoveComplexT&lt;T&gt;, WRank&gt; *W) {</span></a>
<a name="354"><span class="lineNum">     354 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt;) {</a>
<a name="355"><span class="lineNum">     355 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="356"><span class="lineNum">     356 </span>            :         for (int i = 0; i &lt; A-&gt;num_blocks(); i++) {</a>
<a name="357"><span class="lineNum">     357 </span>            :             if (A-&gt;block_dim(i) == 0) {</a>
<a name="358"><span class="lineNum">     358 </span>            :                 continue;</a>
<a name="359"><span class="lineNum">     359 </span>            :             }</a>
<a name="360"><span class="lineNum">     360 </span>            :             heev(&amp;(A-&gt;block(i)), &amp;((*W)(A-&gt;block_range(i))));</a>
<a name="361"><span class="lineNum">     361 </span>            :         }</a>
<a name="362"><span class="lineNum">     362 </span>            :     } else {</a>
<a name="363"><span class="lineNum">     363 </span>            : </a>
<a name="364"><span class="lineNum">     364 </span><span class="lineCov">          8 :         LabeledSection1(fmt::format(&quot;&lt;ComputeEigenvectors={}&gt;&quot;, ComputeEigenvectors));</span></a>
<a name="365"><span class="lineNum">     365 </span><span class="lineCov">          2 :         assert(A-&gt;dim(0) == A-&gt;dim(1));</span></a>
<a name="366"><span class="lineNum">     366 </span>            : </a>
<a name="367"><span class="lineNum">     367 </span><span class="lineCov">          2 :         auto                           n     = A-&gt;dim(0);</span></a>
<a name="368"><span class="lineNum">     368 </span><span class="lineCov">          2 :         auto                           lda   = A-&gt;stride(0);</span></a>
<a name="369"><span class="lineNum">     369 </span><span class="lineCov">          2 :         int                            lwork = 2 * n;</span></a>
<a name="370"><span class="lineNum">     370 </span><span class="lineCov">          2 :         std::vector&lt;T&gt;                 work(lwork);</span></a>
<a name="371"><span class="lineNum">     371 </span><span class="lineCov">          2 :         std::vector&lt;RemoveComplexT&lt;T&gt;&gt; rwork(3 * n);</span></a>
<a name="372"><span class="lineNum">     372 </span>            : </a>
<a name="373"><span class="lineNum">     373 </span><span class="lineCov">          2 :         blas::heev(ComputeEigenvectors ? 'v' : 'n', 'u', n, A-&gt;data(), lda, W-&gt;data(), work.data(), lwork, rwork.data());</span></a>
<a name="374"><span class="lineNum">     374 </span><span class="lineCov">          2 :     }</span></a>
<a name="375"><span class="lineNum">     375 </span><span class="lineCov">          2 : }</span></a>
<a name="376"><span class="lineNum">     376 </span>            : </a>
<a name="377"><span class="lineNum">     377 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, template &lt;typename, size_t&gt; typename BType, size_t BRank, typename T&gt;</a>
<a name="378"><span class="lineNum">     378 </span>            :     requires requires {</a>
<a name="379"><span class="lineNum">     379 </span>            :         requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;;</a>
<a name="380"><span class="lineNum">     380 </span>            :         requires CoreRankTensor&lt;BType&lt;T, BRank&gt;, 2, T&gt;;</a>
<a name="381"><span class="lineNum">     381 </span>            :     }</a>
<a name="382"><span class="lineNum">     382 </span><span class="lineCov">         94 : auto gesv(AType&lt;T, ARank&gt; *A, BType&lt;T, BRank&gt; *B) -&gt; int {</span></a>
<a name="383"><span class="lineNum">     383 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt; &amp;&amp;</a>
<a name="384"><span class="lineNum">     384 </span>            :                   einsums::detail::IsIncoreRankBlockTensorV&lt;BType&lt;T, BRank&gt;, BRank, T&gt;) {</a>
<a name="385"><span class="lineNum">     385 </span>            : </a>
<a name="386"><span class="lineNum">     386 </span>            :         if (A-&gt;num_blocks() != B-&gt;num_blocks()) {</a>
<a name="387"><span class="lineNum">     387 </span>            :             throw std::runtime_error(&quot;gesv: Tensors need the same number of blocks.&quot;);</a>
<a name="388"><span class="lineNum">     388 </span>            :         }</a>
<a name="389"><span class="lineNum">     389 </span>            : </a>
<a name="390"><span class="lineNum">     390 </span>            :         int info_out = 0;</a>
<a name="391"><span class="lineNum">     391 </span>            : </a>
<a name="392"><span class="lineNum">     392 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="393"><span class="lineNum">     393 </span>            :         for (int i = 0; i &lt; A-&gt;num_blocks(); i++) {</a>
<a name="394"><span class="lineNum">     394 </span>            :             if (A-&gt;block_dim(i) == 0) {</a>
<a name="395"><span class="lineNum">     395 </span>            :                 continue;</a>
<a name="396"><span class="lineNum">     396 </span>            :             }</a>
<a name="397"><span class="lineNum">     397 </span>            :             int info = gesv(&amp;(A-&gt;block(i)), &amp;(B-&gt;block(i)));</a>
<a name="398"><span class="lineNum">     398 </span>            : </a>
<a name="399"><span class="lineNum">     399 </span>            :             info_out |= info;</a>
<a name="400"><span class="lineNum">     400 </span>            : </a>
<a name="401"><span class="lineNum">     401 </span>            :             if (info != 0) {</a>
<a name="402"><span class="lineNum">     402 </span>            :                 println(&quot;gesv: Got non-zero return: %d&quot;, info);</a>
<a name="403"><span class="lineNum">     403 </span>            :             }</a>
<a name="404"><span class="lineNum">     404 </span>            :         }</a>
<a name="405"><span class="lineNum">     405 </span>            : </a>
<a name="406"><span class="lineNum">     406 </span>            :         return info_out;</a>
<a name="407"><span class="lineNum">     407 </span>            : </a>
<a name="408"><span class="lineNum">     408 </span>            :     } else if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt;) {</a>
<a name="409"><span class="lineNum">     409 </span>            :         int info_out = 0;</a>
<a name="410"><span class="lineNum">     410 </span>            : </a>
<a name="411"><span class="lineNum">     411 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="412"><span class="lineNum">     412 </span>            :         for (int i = 0; i &lt; A-&gt;num_blocks(); i++) {</a>
<a name="413"><span class="lineNum">     413 </span>            : </a>
<a name="414"><span class="lineNum">     414 </span>            :             if (A-&gt;block_dim(i) == 0) {</a>
<a name="415"><span class="lineNum">     415 </span>            :                 continue;</a>
<a name="416"><span class="lineNum">     416 </span>            :             }</a>
<a name="417"><span class="lineNum">     417 </span>            :             int info = gesv(&amp;(A-&gt;block(i)), &amp;((*B)(AllT(), A-&gt;block_range(i))));</a>
<a name="418"><span class="lineNum">     418 </span>            : </a>
<a name="419"><span class="lineNum">     419 </span>            :             info_out |= info;</a>
<a name="420"><span class="lineNum">     420 </span>            : </a>
<a name="421"><span class="lineNum">     421 </span>            :             if (info != 0) {</a>
<a name="422"><span class="lineNum">     422 </span>            :                 println(&quot;gesv: Got non-zero return: %d&quot;, info);</a>
<a name="423"><span class="lineNum">     423 </span>            :             }</a>
<a name="424"><span class="lineNum">     424 </span>            :         }</a>
<a name="425"><span class="lineNum">     425 </span>            : </a>
<a name="426"><span class="lineNum">     426 </span>            :         return info_out;</a>
<a name="427"><span class="lineNum">     427 </span>            :     }</a>
<a name="428"><span class="lineNum">     428 </span>            : </a>
<a name="429"><span class="lineNum">     429 </span><span class="lineCov">        282 :     LabeledSection0();</span></a>
<a name="430"><span class="lineNum">     430 </span>            : </a>
<a name="431"><span class="lineNum">     431 </span><span class="lineCov">         94 :     auto n   = A-&gt;dim(0);</span></a>
<a name="432"><span class="lineNum">     432 </span><span class="lineCov">         94 :     auto lda = A-&gt;dim(0);</span></a>
<a name="433"><span class="lineNum">     433 </span><span class="lineCov">         94 :     auto ldb = B-&gt;dim(1);</span></a>
<a name="434"><span class="lineNum">     434 </span>            : </a>
<a name="435"><span class="lineNum">     435 </span><span class="lineCov">         94 :     auto nrhs = B-&gt;dim(0);</span></a>
<a name="436"><span class="lineNum">     436 </span>            : </a>
<a name="437"><span class="lineNum">     437 </span><span class="lineCov">         94 :     int                   lwork = n;</span></a>
<a name="438"><span class="lineNum">     438 </span><span class="lineCov">         94 :     std::vector&lt;blas_int&gt; ipiv(lwork);</span></a>
<a name="439"><span class="lineNum">     439 </span>            : </a>
<a name="440"><span class="lineNum">     440 </span><span class="lineCov">         94 :     int info = blas::gesv(n, nrhs, A-&gt;data(), lda, ipiv.data(), B-&gt;data(), ldb);</span></a>
<a name="441"><span class="lineNum">     441 </span><span class="lineCov">        188 :     return info;</span></a>
<a name="442"><span class="lineNum">     442 </span><span class="lineCov">         94 : }</span></a>
<a name="443"><span class="lineNum">     443 </span>            : </a>
<a name="444"><span class="lineNum">     444 </span>            : #if !defined(DOXYGEN_SHOULD_SKIP_THIS)</a>
<a name="445"><span class="lineNum">     445 </span>            : /**</a>
<a name="446"><span class="lineNum">     446 </span>            :  * Computes all eigenvalues and, optionally, eigenvectors of a real symmetric matrix.</a>
<a name="447"><span class="lineNum">     447 </span>            :  *</a>
<a name="448"><span class="lineNum">     448 </span>            :  * This routines assumes the upper triangle of A is stored. The lower triangle is not referenced.</a>
<a name="449"><span class="lineNum">     449 </span>            :  *</a>
<a name="450"><span class="lineNum">     450 </span>            :  * @code</a>
<a name="451"><span class="lineNum">     451 </span>            :  * // Create tensors A and b.</a>
<a name="452"><span class="lineNum">     452 </span>            :  * auto A = einsums::create_tensor(&quot;A&quot;, 3, 3);</a>
<a name="453"><span class="lineNum">     453 </span>            :  *</a>
<a name="454"><span class="lineNum">     454 </span>            :  * // Fill A with the symmetric data.</a>
<a name="455"><span class="lineNum">     455 </span>            :  * A.vector_data() = einsums::VectorData{1.0, 2.0, 3.0, 2.0, 4.0, 5.0, 3.0, 5.0, 6.0};</a>
<a name="456"><span class="lineNum">     456 </span>            :  *</a>
<a name="457"><span class="lineNum">     457 </span>            :  * // On exit, A is not destroyed. The eigenvectors and eigenvalues are returned in a std::tuple.</a>
<a name="458"><span class="lineNum">     458 </span>            :  * auto [evecs, evals ] = einsums::linear_algebra::syev(A);</a>
<a name="459"><span class="lineNum">     459 </span>            :  * @endcode</a>
<a name="460"><span class="lineNum">     460 </span>            :  *</a>
<a name="461"><span class="lineNum">     461 </span>            :  * @tparam AType The type of the tensor A</a>
<a name="462"><span class="lineNum">     462 </span>            :  * @tparam ARank The rank of the tensor A (required to be 2)</a>
<a name="463"><span class="lineNum">     463 </span>            :  * @tparam T The underlying data type (required to be real)</a>
<a name="464"><span class="lineNum">     464 </span>            :  * @tparam ComputeEigenvectors If true, eigenvalues and eigenvectors are computed. If false, only eigenvalues are computed. Defaults to</a>
<a name="465"><span class="lineNum">     465 </span>            :  * true.</a>
<a name="466"><span class="lineNum">     466 </span>            :  * @param A The symmetric matrix A in the leading N-by-N upper triangular part of A.</a>
<a name="467"><span class="lineNum">     467 </span>            :  * @return std::tuple&lt;Tensor&lt;T, 2&gt;, Tensor&lt;T, 1&gt;&gt; The eigenvectors and eigenvalues.</a>
<a name="468"><span class="lineNum">     468 </span>            :  */</a>
<a name="469"><span class="lineNum">     469 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, typename T, bool ComputeEigenvectors = true&gt;</a>
<a name="470"><span class="lineNum">     470 </span>            :     requires CoreRankBlockTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="471"><span class="lineNum">     471 </span>            : auto syev(const AType&lt;T, ARank&gt; &amp;A) -&gt; std::tuple&lt;BlockTensor&lt;T, 2&gt;, Tensor&lt;T, 1&gt;&gt; {</a>
<a name="472"><span class="lineNum">     472 </span>            :     LabeledSection0();</a>
<a name="473"><span class="lineNum">     473 </span>            : </a>
<a name="474"><span class="lineNum">     474 </span>            :     BlockTensor&lt;T, 2&gt; a = A;</a>
<a name="475"><span class="lineNum">     475 </span>            :     Tensor&lt;T, 1&gt;      w{&quot;eigenvalues&quot;, A.dim(0)};</a>
<a name="476"><span class="lineNum">     476 </span>            : </a>
<a name="477"><span class="lineNum">     477 </span>            :     syev&lt;ComputeEigenvectors&gt;(&amp;a, &amp;w);</a>
<a name="478"><span class="lineNum">     478 </span>            : </a>
<a name="479"><span class="lineNum">     479 </span>            :     return std::make_tuple(a, w);</a>
<a name="480"><span class="lineNum">     480 </span>            : }</a>
<a name="481"><span class="lineNum">     481 </span>            : #endif</a>
<a name="482"><span class="lineNum">     482 </span>            : </a>
<a name="483"><span class="lineNum">     483 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, typename T, bool ComputeEigenvectors = true&gt;</a>
<a name="484"><span class="lineNum">     484 </span>            :     requires requires {</a>
<a name="485"><span class="lineNum">     485 </span>            :         requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;;</a>
<a name="486"><span class="lineNum">     486 </span>            :         requires !CoreRankBlockTensor&lt;AType&lt;T, ARank&gt;, ARank, T&gt;;</a>
<a name="487"><span class="lineNum">     487 </span>            :     }</a>
<a name="488"><span class="lineNum">     488 </span>            : auto syev(const AType&lt;T, ARank&gt; &amp;A) -&gt; std::tuple&lt;Tensor&lt;T, 2&gt;, Tensor&lt;T, 1&gt;&gt; {</a>
<a name="489"><span class="lineNum">     489 </span>            :     LabeledSection0();</a>
<a name="490"><span class="lineNum">     490 </span>            : </a>
<a name="491"><span class="lineNum">     491 </span>            :     assert(A.dim(0) == A.dim(1));</a>
<a name="492"><span class="lineNum">     492 </span>            : </a>
<a name="493"><span class="lineNum">     493 </span>            :     Tensor&lt;T, 2&gt; a = A;</a>
<a name="494"><span class="lineNum">     494 </span>            :     Tensor&lt;T, 1&gt; w{&quot;eigenvalues&quot;, A.dim(0)};</a>
<a name="495"><span class="lineNum">     495 </span>            : </a>
<a name="496"><span class="lineNum">     496 </span>            :     blas::syev&lt;ComputeEigenvectors&gt;(&amp;a, &amp;w);</a>
<a name="497"><span class="lineNum">     497 </span>            : </a>
<a name="498"><span class="lineNum">     498 </span>            :     return std::make_tuple(a, w);</a>
<a name="499"><span class="lineNum">     499 </span>            : }</a>
<a name="500"><span class="lineNum">     500 </span>            : </a>
<a name="501"><span class="lineNum">     501 </span>            : /**</a>
<a name="502"><span class="lineNum">     502 </span>            :  * Scales a tensor by a scalar.</a>
<a name="503"><span class="lineNum">     503 </span>            :  *</a>
<a name="504"><span class="lineNum">     504 </span>            :  * @code</a>
<a name="505"><span class="lineNum">     505 </span>            :  * auto A = einsums::create_ones_tensor(&quot;A&quot;, 3, 3);</a>
<a name="506"><span class="lineNum">     506 </span>            :  *</a>
<a name="507"><span class="lineNum">     507 </span>            :  * // A is filled with 1.0</a>
<a name="508"><span class="lineNum">     508 </span>            :  * einsums::linear_algebra::scale(2.0, &amp;A);</a>
<a name="509"><span class="lineNum">     509 </span>            :  * // A is now filled with 2.0</a>
<a name="510"><span class="lineNum">     510 </span>            :  * @endcode</a>
<a name="511"><span class="lineNum">     511 </span>            :  *</a>
<a name="512"><span class="lineNum">     512 </span>            :  * @tparam AType The type of the tensor</a>
<a name="513"><span class="lineNum">     513 </span>            :  * @tparam ARank The rank of the tensor</a>
<a name="514"><span class="lineNum">     514 </span>            :  * @tparam T The underlying data type</a>
<a name="515"><span class="lineNum">     515 </span>            :  * @param scale The scalar to scale the tensor by</a>
<a name="516"><span class="lineNum">     516 </span>            :  * @param A The tensor to scale</a>
<a name="517"><span class="lineNum">     517 </span>            :  */</a>
<a name="518"><span class="lineNum">     518 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, typename T&gt;</a>
<a name="519"><span class="lineNum">     519 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, ARank, T&gt;</a>
<a name="520"><span class="lineNum">     520 </span><span class="lineCov">         34 : void scale(T scale, AType&lt;T, ARank&gt; *A) {</span></a>
<a name="521"><span class="lineNum">     521 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt;) {</a>
<a name="522"><span class="lineNum">     522 </span>            : </a>
<a name="523"><span class="lineNum">     523 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="524"><span class="lineNum">     524 </span>            :         for (int i = 0; i &lt; A-&gt;num_blocks(); i++) {</a>
<a name="525"><span class="lineNum">     525 </span>            :             if (A-&gt;block_dim(i) == 0) {</a>
<a name="526"><span class="lineNum">     526 </span>            :                 continue;</a>
<a name="527"><span class="lineNum">     527 </span>            :             }</a>
<a name="528"><span class="lineNum">     528 </span>            :             scale(scale, &amp;(A-&gt;block(i)));</a>
<a name="529"><span class="lineNum">     529 </span>            :         }</a>
<a name="530"><span class="lineNum">     530 </span>            :     } else {</a>
<a name="531"><span class="lineNum">     531 </span>            : </a>
<a name="532"><span class="lineNum">     532 </span><span class="lineCov">        102 :         LabeledSection0();</span></a>
<a name="533"><span class="lineNum">     533 </span>            : </a>
<a name="534"><span class="lineNum">     534 </span><span class="lineCov">         34 :         blas::scal(A-&gt;dim(0) * A-&gt;stride(0), scale, A-&gt;data(), 1);</span></a>
<a name="535"><span class="lineNum">     535 </span><span class="lineCov">         34 :     }</span></a>
<a name="536"><span class="lineNum">     536 </span><span class="lineCov">         34 : }</span></a>
<a name="537"><span class="lineNum">     537 </span>            : </a>
<a name="538"><span class="lineNum">     538 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, typename T&gt;</a>
<a name="539"><span class="lineNum">     539 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="540"><span class="lineNum">     540 </span><span class="lineCov">          2 : void scale_row(size_t row, T scale, AType&lt;T, ARank&gt; *A) {</span></a>
<a name="541"><span class="lineNum">     541 </span><span class="lineCov">          6 :     LabeledSection0();</span></a>
<a name="542"><span class="lineNum">     542 </span>            : </a>
<a name="543"><span class="lineNum">     543 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt;) {</a>
<a name="544"><span class="lineNum">     544 </span>            :         blas::scal(A-&gt;block_dim(A-&gt;block_of(row), 1), scale, A-&gt;data(row, 0ul), A-&gt;block(A-&gt;block_of(row)).stride(1));</a>
<a name="545"><span class="lineNum">     545 </span>            :     } else {</a>
<a name="546"><span class="lineNum">     546 </span><span class="lineCov">          2 :         blas::scal(A-&gt;dim(1), scale, A-&gt;data(row, 0ul), A-&gt;stride(1));</span></a>
<a name="547"><span class="lineNum">     547 </span>            :     }</a>
<a name="548"><span class="lineNum">     548 </span><span class="lineCov">          2 : }</span></a>
<a name="549"><span class="lineNum">     549 </span>            : </a>
<a name="550"><span class="lineNum">     550 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, typename T&gt;</a>
<a name="551"><span class="lineNum">     551 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="552"><span class="lineNum">     552 </span><span class="lineCov">         14 : void scale_column(size_t col, T scale, AType&lt;T, ARank&gt; *A) {</span></a>
<a name="553"><span class="lineNum">     553 </span><span class="lineCov">         42 :     LabeledSection0();</span></a>
<a name="554"><span class="lineNum">     554 </span>            : </a>
<a name="555"><span class="lineNum">     555 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt;) {</a>
<a name="556"><span class="lineNum">     556 </span>            :         blas::scal(A-&gt;block_dim(A-&gt;block_of(col), 1), scale, A-&gt;data(0ul, col), A-&gt;block(A-&gt;block_of(col)).stride(0));</a>
<a name="557"><span class="lineNum">     557 </span>            :     } else {</a>
<a name="558"><span class="lineNum">     558 </span><span class="lineCov">         14 :         blas::scal(A-&gt;dim(0), scale, A-&gt;data(0ul, col), A-&gt;stride(0));</span></a>
<a name="559"><span class="lineNum">     559 </span>            :     }</a>
<a name="560"><span class="lineNum">     560 </span><span class="lineCov">         14 : }</span></a>
<a name="561"><span class="lineNum">     561 </span>            : </a>
<a name="562"><span class="lineNum">     562 </span>            : /**</a>
<a name="563"><span class="lineNum">     563 </span>            :  * @brief Computes the matrix power of a to alpha.  Return a new tensor, does not destroy a.</a>
<a name="564"><span class="lineNum">     564 </span>            :  *</a>
<a name="565"><span class="lineNum">     565 </span>            :  * @tparam AType</a>
<a name="566"><span class="lineNum">     566 </span>            :  * @param a Matrix to take power of</a>
<a name="567"><span class="lineNum">     567 </span>            :  * @param alpha The power to take</a>
<a name="568"><span class="lineNum">     568 </span>            :  * @param cutoff Values below cutoff are considered zero.</a>
<a name="569"><span class="lineNum">     569 </span>            :  *</a>
<a name="570"><span class="lineNum">     570 </span>            :  * @return std::enable_if_t&lt;std::is_base_of_v&lt;Detail::TensorBase&lt;double, 2&gt;, AType&gt;, AType&gt;</a>
<a name="571"><span class="lineNum">     571 </span>            :  *</a>
<a name="572"><span class="lineNum">     572 </span>            :  * TODO This function needs to have a test case implemented.</a>
<a name="573"><span class="lineNum">     573 </span>            :  */</a>
<a name="574"><span class="lineNum">     574 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, typename T&gt;</a>
<a name="575"><span class="lineNum">     575 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="576"><span class="lineNum">     576 </span>            : auto pow(const AType&lt;T, ARank&gt; &amp;a, T alpha, T cutoff = std::numeric_limits&lt;T&gt;::epsilon()) -&gt; AType&lt;T, ARank&gt; {</a>
<a name="577"><span class="lineNum">     577 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, ARank&gt;, ARank, T&gt;) {</a>
<a name="578"><span class="lineNum">     578 </span>            :         auto out = AType&lt;T, ARank&gt;(a); // Copy a so that this has the same signature.</a>
<a name="579"><span class="lineNum">     579 </span>            :         out.set_name(&quot;pow result&quot;);</a>
<a name="580"><span class="lineNum">     580 </span>            : </a>
<a name="581"><span class="lineNum">     581 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="582"><span class="lineNum">     582 </span>            :         for (int i = 0; i &lt; a.num_blocks(); i++) {</a>
<a name="583"><span class="lineNum">     583 </span>            :             if (a.block_dim(i) == 0) {</a>
<a name="584"><span class="lineNum">     584 </span>            :                 continue;</a>
<a name="585"><span class="lineNum">     585 </span>            :             }</a>
<a name="586"><span class="lineNum">     586 </span>            :             out.block(i) = pow(a.block(i), alpha, cutoff);</a>
<a name="587"><span class="lineNum">     587 </span>            :         }</a>
<a name="588"><span class="lineNum">     588 </span>            : </a>
<a name="589"><span class="lineNum">     589 </span>            :         return out;</a>
<a name="590"><span class="lineNum">     590 </span>            :     } else {</a>
<a name="591"><span class="lineNum">     591 </span>            :         LabeledSection0();</a>
<a name="592"><span class="lineNum">     592 </span>            : </a>
<a name="593"><span class="lineNum">     593 </span>            :         assert(a.dim(0) == a.dim(1));</a>
<a name="594"><span class="lineNum">     594 </span>            : </a>
<a name="595"><span class="lineNum">     595 </span>            :         size_t       n  = a.dim(0);</a>
<a name="596"><span class="lineNum">     596 </span>            :         Tensor&lt;T, 2&gt; a1 = a;</a>
<a name="597"><span class="lineNum">     597 </span>            :         Tensor&lt;T, 2&gt; result{&quot;pow result&quot;, a.dim(0), a.dim(1)};</a>
<a name="598"><span class="lineNum">     598 </span>            :         Tensor&lt;T, 1&gt; e{&quot;e&quot;, n};</a>
<a name="599"><span class="lineNum">     599 </span>            : </a>
<a name="600"><span class="lineNum">     600 </span>            :         // Diagonalize</a>
<a name="601"><span class="lineNum">     601 </span>            :         syev(&amp;a1, &amp;e);</a>
<a name="602"><span class="lineNum">     602 </span>            : </a>
<a name="603"><span class="lineNum">     603 </span>            :         Tensor&lt;T, 2&gt; a2 = a1;</a>
<a name="604"><span class="lineNum">     604 </span>            : </a>
<a name="605"><span class="lineNum">     605 </span>            :         // Determine the largest magnitude of the eigenvalues to use as a scaling factor for the cutoff.</a>
<a name="606"><span class="lineNum">     606 </span>            :         double max_e = std::fabs(e(n - 1)) &gt; std::fabs(e(0)) ? std::fabs(e(n - 1)) : std::fabs(e(0));</a>
<a name="607"><span class="lineNum">     607 </span>            : </a>
<a name="608"><span class="lineNum">     608 </span>            :         for (size_t i = 0; i &lt; n; i++) {</a>
<a name="609"><span class="lineNum">     609 </span>            :             if (alpha &lt; 0.0 &amp;&amp; std::fabs(e(i)) &lt; cutoff * max_e) {</a>
<a name="610"><span class="lineNum">     610 </span>            :                 e(i) = 0.0;</a>
<a name="611"><span class="lineNum">     611 </span>            :             } else {</a>
<a name="612"><span class="lineNum">     612 </span>            :                 e(i) = std::pow(e(i), alpha);</a>
<a name="613"><span class="lineNum">     613 </span>            :                 if (!std::isfinite(e(i))) {</a>
<a name="614"><span class="lineNum">     614 </span>            :                     e(i) = 0.0;</a>
<a name="615"><span class="lineNum">     615 </span>            :                 }</a>
<a name="616"><span class="lineNum">     616 </span>            :             }</a>
<a name="617"><span class="lineNum">     617 </span>            : </a>
<a name="618"><span class="lineNum">     618 </span>            :             scale_row(i, e(i), &amp;a2);</a>
<a name="619"><span class="lineNum">     619 </span>            :         }</a>
<a name="620"><span class="lineNum">     620 </span>            : </a>
<a name="621"><span class="lineNum">     621 </span>            :         gemm&lt;true, false&gt;(1.0, a2, a1, 0.0, &amp;result);</a>
<a name="622"><span class="lineNum">     622 </span>            : </a>
<a name="623"><span class="lineNum">     623 </span>            :         return result;</a>
<a name="624"><span class="lineNum">     624 </span>            :     }</a>
<a name="625"><span class="lineNum">     625 </span>            : }</a>
<a name="626"><span class="lineNum">     626 </span>            : </a>
<a name="627"><span class="lineNum">     627 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, template &lt;typename, size_t&gt; typename BType, typename T&gt;</a>
<a name="628"><span class="lineNum">     628 </span>            :     requires requires {</a>
<a name="629"><span class="lineNum">     629 </span>            :         requires CoreRankTensor&lt;AType&lt;T, 1&gt;, 1, T&gt;;</a>
<a name="630"><span class="lineNum">     630 </span>            :         requires CoreRankTensor&lt;BType&lt;T, 1&gt;, 1, T&gt;;</a>
<a name="631"><span class="lineNum">     631 </span>            :     }</a>
<a name="632"><span class="lineNum">     632 </span><span class="lineCov">         48 : auto dot(const AType&lt;T, 1&gt; &amp;A, const BType&lt;T, 1&gt; &amp;B) -&gt; T {</span></a>
<a name="633"><span class="lineNum">     633 </span><span class="lineCov">        144 :     LabeledSection0();</span></a>
<a name="634"><span class="lineNum">     634 </span>            : </a>
<a name="635"><span class="lineNum">     635 </span><span class="lineCov">         48 :     assert(A.dim(0) == B.dim(0));</span></a>
<a name="636"><span class="lineNum">     636 </span>            : </a>
<a name="637"><span class="lineNum">     637 </span><span class="lineCov">         48 :     auto result = blas::dot(A.dim(0), A.data(), A.stride(0), B.data(), B.stride(0));</span></a>
<a name="638"><span class="lineNum">     638 </span><span class="lineCov">         48 :     return result;</span></a>
<a name="639"><span class="lineNum">     639 </span><span class="lineCov">         48 : }</span></a>
<a name="640"><span class="lineNum">     640 </span>            : </a>
<a name="641"><span class="lineNum">     641 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, template &lt;typename, size_t&gt; typename BType,</a>
<a name="642"><span class="lineNum">     642 </span>            :           typename T, size_t Rank&gt;</a>
<a name="643"><span class="lineNum">     643 </span>            :     requires requires {</a>
<a name="644"><span class="lineNum">     644 </span>            :         requires CoreRankTensor&lt;AType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="645"><span class="lineNum">     645 </span>            :         requires CoreRankTensor&lt;BType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="646"><span class="lineNum">     646 </span>            :     }</a>
<a name="647"><span class="lineNum">     647 </span><span class="lineCov">         36 : auto dot(const AType&lt;T, Rank&gt; &amp;A, const BType&lt;T, Rank&gt; &amp;B) -&gt; T {</span></a>
<a name="648"><span class="lineNum">     648 </span>            : </a>
<a name="649"><span class="lineNum">     649 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp;</a>
<a name="650"><span class="lineNum">     650 </span>            :                   einsums::detail::IsIncoreRankBlockTensorV&lt;BType&lt;T, Rank&gt;, Rank, T&gt;) {</a>
<a name="651"><span class="lineNum">     651 </span>            :         if (A.num_blocks() != B.num_blocks()) {</a>
<a name="652"><span class="lineNum">     652 </span>            :             return dot((einsums::Tensor&lt;T, Rank&gt;)A, (einsums::Tensor&lt;T, Rank&gt;)B);</a>
<a name="653"><span class="lineNum">     653 </span>            :         }</a>
<a name="654"><span class="lineNum">     654 </span>            : </a>
<a name="655"><span class="lineNum">     655 </span>            :         if (A.ranges() != B.ranges()) {</a>
<a name="656"><span class="lineNum">     656 </span>            :             return dot((einsums::Tensor&lt;T, Rank&gt;)A, (einsums::Tensor&lt;T, Rank&gt;)B);</a>
<a name="657"><span class="lineNum">     657 </span>            :         }</a>
<a name="658"><span class="lineNum">     658 </span>            : </a>
<a name="659"><span class="lineNum">     659 </span>            :         T out{0};</a>
<a name="660"><span class="lineNum">     660 </span>            : </a>
<a name="661"><span class="lineNum">     661 </span>            : #pragma omp parallel for reduction(+ : out)</a>
<a name="662"><span class="lineNum">     662 </span>            :         for (int i = 0; i &lt; A.num_blocks(); i++) {</a>
<a name="663"><span class="lineNum">     663 </span>            :             if (A.block_dim(i) == 0) {</a>
<a name="664"><span class="lineNum">     664 </span>            :                 continue;</a>
<a name="665"><span class="lineNum">     665 </span>            :             }</a>
<a name="666"><span class="lineNum">     666 </span>            :             out += dot(A.block(i), B.block(i));</a>
<a name="667"><span class="lineNum">     667 </span>            :         }</a>
<a name="668"><span class="lineNum">     668 </span>            : </a>
<a name="669"><span class="lineNum">     669 </span>            :         return out;</a>
<a name="670"><span class="lineNum">     670 </span>            : </a>
<a name="671"><span class="lineNum">     671 </span>            :     } else {</a>
<a name="672"><span class="lineNum">     672 </span><span class="lineCov">         72 :         LabeledSection0();</span></a>
<a name="673"><span class="lineNum">     673 </span>            : </a>
<a name="674"><span class="lineNum">     674 </span><span class="lineCov">         36 :         Dim&lt;1&gt; dim{1};</span></a>
<a name="675"><span class="lineNum">     675 </span>            : </a>
<a name="676"><span class="lineNum">     676 </span><span class="lineCov">        144 :         for (size_t i = 0; i &lt; Rank; i++) {</span></a>
<a name="677"><span class="lineNum">     677 </span><span class="lineCov">        108 :             assert(A.dim(i) == B.dim(i));</span></a>
<a name="678"><span class="lineNum">     678 </span><span class="lineCov">        108 :             dim[0] *= A.dim(i);</span></a>
<a name="679"><span class="lineNum">     679 </span>            :         }</a>
<a name="680"><span class="lineNum">     680 </span>            : </a>
<a name="681"><span class="lineNum">     681 </span><span class="lineCov">         90 :         return dot(TensorView&lt;T, 1&gt;(const_cast&lt;AType&lt;T, Rank&gt; &amp;&gt;(A), dim), TensorView&lt;T, 1&gt;(const_cast&lt;BType&lt;T, Rank&gt; &amp;&gt;(B), dim));</span></a>
<a name="682"><span class="lineNum">     682 </span><span class="lineCov">         36 :     }</span></a>
<a name="683"><span class="lineNum">     683 </span>            : }</a>
<a name="684"><span class="lineNum">     684 </span>            : </a>
<a name="685"><span class="lineNum">     685 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, template &lt;typename, size_t&gt; typename BType,</a>
<a name="686"><span class="lineNum">     686 </span>            :           template &lt;typename, size_t&gt; typename CType, typename T, size_t Rank&gt;</a>
<a name="687"><span class="lineNum">     687 </span>            :     requires requires {</a>
<a name="688"><span class="lineNum">     688 </span>            :         requires CoreRankTensor&lt;AType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="689"><span class="lineNum">     689 </span>            :         requires CoreRankTensor&lt;BType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="690"><span class="lineNum">     690 </span>            :         requires CoreRankTensor&lt;CType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="691"><span class="lineNum">     691 </span>            :     }</a>
<a name="692"><span class="lineNum">     692 </span>            : auto dot(const AType&lt;T, Rank&gt; &amp;A, const BType&lt;T, Rank&gt; &amp;B, const CType&lt;T, Rank&gt; &amp;C) -&gt; T {</a>
<a name="693"><span class="lineNum">     693 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;AType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp;</a>
<a name="694"><span class="lineNum">     694 </span>            :                   einsums::detail::IsIncoreRankBlockTensorV&lt;BType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp;</a>
<a name="695"><span class="lineNum">     695 </span>            :                   einsums::detail::IsIncoreRankBlockTensorV&lt;CType&lt;T, Rank&gt;, Rank, T&gt;) {</a>
<a name="696"><span class="lineNum">     696 </span>            :         if (A.num_blocks() != B.num_blocks() || A.num_blocks() != C.num_blocks() || B.num_blocks() != C.num_blocks()) {</a>
<a name="697"><span class="lineNum">     697 </span>            :             return dot((Tensor&lt;T, Rank&gt;)A, (Tensor&lt;T, Rank&gt;)B, (Tensor&lt;T, Rank&gt;)C);</a>
<a name="698"><span class="lineNum">     698 </span>            :         }</a>
<a name="699"><span class="lineNum">     699 </span>            : </a>
<a name="700"><span class="lineNum">     700 </span>            :         if (A.ranges() != B.ranges() || A.ranges() != C.ranges() || B.ranges() != C.ranges()) {</a>
<a name="701"><span class="lineNum">     701 </span>            :             return dot((Tensor&lt;T, Rank&gt;)A, (Tensor&lt;T, Rank&gt;)B, (Tensor&lt;T, Rank&gt;)C);</a>
<a name="702"><span class="lineNum">     702 </span>            :         }</a>
<a name="703"><span class="lineNum">     703 </span>            : </a>
<a name="704"><span class="lineNum">     704 </span>            :         T out{0};</a>
<a name="705"><span class="lineNum">     705 </span>            : </a>
<a name="706"><span class="lineNum">     706 </span>            : #pragma omp parallel for reduction(+ : out)</a>
<a name="707"><span class="lineNum">     707 </span>            :         for (int i = 0; i &lt; A.num_blocks(); i++) {</a>
<a name="708"><span class="lineNum">     708 </span>            :             if (A.block_dim(i) == 0) {</a>
<a name="709"><span class="lineNum">     709 </span>            :                 continue;</a>
<a name="710"><span class="lineNum">     710 </span>            :             }</a>
<a name="711"><span class="lineNum">     711 </span>            :             out += dot(A.block(i), B.block(i), C.block(i));</a>
<a name="712"><span class="lineNum">     712 </span>            :         }</a>
<a name="713"><span class="lineNum">     713 </span>            : </a>
<a name="714"><span class="lineNum">     714 </span>            :         return out;</a>
<a name="715"><span class="lineNum">     715 </span>            : </a>
<a name="716"><span class="lineNum">     716 </span>            :     } else {</a>
<a name="717"><span class="lineNum">     717 </span>            : </a>
<a name="718"><span class="lineNum">     718 </span>            :         LabeledSection0();</a>
<a name="719"><span class="lineNum">     719 </span>            : </a>
<a name="720"><span class="lineNum">     720 </span>            :         Dim&lt;1&gt; dim{1};</a>
<a name="721"><span class="lineNum">     721 </span>            : </a>
<a name="722"><span class="lineNum">     722 </span>            :         for (size_t i = 0; i &lt; Rank; i++) {</a>
<a name="723"><span class="lineNum">     723 </span>            :             assert(A.dim(i) == B.dim(i) &amp;&amp; A.dim(i) == C.dim(i));</a>
<a name="724"><span class="lineNum">     724 </span>            :             dim[0] *= A.dim(i);</a>
<a name="725"><span class="lineNum">     725 </span>            :         }</a>
<a name="726"><span class="lineNum">     726 </span>            : </a>
<a name="727"><span class="lineNum">     727 </span>            :         auto vA = TensorView&lt;T, 1&gt;(const_cast&lt;AType&lt;T, Rank&gt; &amp;&gt;(A), dim);</a>
<a name="728"><span class="lineNum">     728 </span>            :         auto vB = TensorView&lt;T, 1&gt;(const_cast&lt;BType&lt;T, Rank&gt; &amp;&gt;(B), dim);</a>
<a name="729"><span class="lineNum">     729 </span>            :         auto vC = TensorView&lt;T, 1&gt;(const_cast&lt;CType&lt;T, Rank&gt; &amp;&gt;(C), dim);</a>
<a name="730"><span class="lineNum">     730 </span>            : </a>
<a name="731"><span class="lineNum">     731 </span>            :         T result{0};</a>
<a name="732"><span class="lineNum">     732 </span>            : #pragma omp parallel for reduction(+ : result)</a>
<a name="733"><span class="lineNum">     733 </span>            :         for (size_t i = 0; i &lt; dim[0]; i++) {</a>
<a name="734"><span class="lineNum">     734 </span>            :             result += vA(i) * vB(i) * vC(i);</a>
<a name="735"><span class="lineNum">     735 </span>            :         }</a>
<a name="736"><span class="lineNum">     736 </span>            :         return result;</a>
<a name="737"><span class="lineNum">     737 </span>            :     }</a>
<a name="738"><span class="lineNum">     738 </span>            : }</a>
<a name="739"><span class="lineNum">     739 </span>            : </a>
<a name="740"><span class="lineNum">     740 </span>            : template &lt;template &lt;typename, size_t&gt; typename XType, template &lt;typename, size_t&gt; typename YType, typename T, size_t Rank&gt;</a>
<a name="741"><span class="lineNum">     741 </span>            :     requires requires {</a>
<a name="742"><span class="lineNum">     742 </span>            :         requires CoreRankTensor&lt;XType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="743"><span class="lineNum">     743 </span>            :         requires CoreRankTensor&lt;YType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="744"><span class="lineNum">     744 </span>            :         requires !CoreRankBlockTensor&lt;YType&lt;T, Rank&gt;, Rank, T&gt; ||</a>
<a name="745"><span class="lineNum">     745 </span>            :                      (CoreRankBlockTensor&lt;XType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp; CoreRankBlockTensor&lt;YType&lt;T, Rank&gt;, Rank, T&gt;);</a>
<a name="746"><span class="lineNum">     746 </span>            :     }</a>
<a name="747"><span class="lineNum">     747 </span><span class="lineCov">          1 : void axpy(T alpha, const XType&lt;T, Rank&gt; &amp;X, YType&lt;T, Rank&gt; *Y) {</span></a>
<a name="748"><span class="lineNum">     748 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;XType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp;</a>
<a name="749"><span class="lineNum">     749 </span>            :                   einsums::detail::IsIncoreRankBlockTensorV&lt;YType&lt;T, Rank&gt;, Rank, T&gt;) {</a>
<a name="750"><span class="lineNum">     750 </span>            :         if (X.num_blocks() != Y-&gt;num_blocks()) {</a>
<a name="751"><span class="lineNum">     751 </span>            :             throw std::runtime_error(&quot;axpy: Tensors need to have the same number of blocks.&quot;);</a>
<a name="752"><span class="lineNum">     752 </span>            :         }</a>
<a name="753"><span class="lineNum">     753 </span>            : </a>
<a name="754"><span class="lineNum">     754 </span>            :         if (X.ranges() != Y-&gt;ranges()) {</a>
<a name="755"><span class="lineNum">     755 </span>            :             throw std::runtime_error(&quot;axpy: Tensor blocks need to be compatible.&quot;);</a>
<a name="756"><span class="lineNum">     756 </span>            :         }</a>
<a name="757"><span class="lineNum">     757 </span>            : </a>
<a name="758"><span class="lineNum">     758 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="759"><span class="lineNum">     759 </span>            :         for (int i = 0; i &lt; X.num_blocks(); i++) {</a>
<a name="760"><span class="lineNum">     760 </span>            :             if (X.block_dim() == 0) {</a>
<a name="761"><span class="lineNum">     761 </span>            :                 continue;</a>
<a name="762"><span class="lineNum">     762 </span>            :             }</a>
<a name="763"><span class="lineNum">     763 </span>            : </a>
<a name="764"><span class="lineNum">     764 </span>            :             axpy(alpha, X[i], &amp;(Y-&gt;block(i)));</a>
<a name="765"><span class="lineNum">     765 </span>            :         }</a>
<a name="766"><span class="lineNum">     766 </span>            :     } else if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;XType&lt;T, Rank&gt;, Rank, T&gt;) {</a>
<a name="767"><span class="lineNum">     767 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="768"><span class="lineNum">     768 </span>            :         for (int i = 0; i &lt; X.num_blocks(); i++) {</a>
<a name="769"><span class="lineNum">     769 </span>            :             if (X.block_dim() == 0) {</a>
<a name="770"><span class="lineNum">     770 </span>            :                 continue;</a>
<a name="771"><span class="lineNum">     771 </span>            :             }</a>
<a name="772"><span class="lineNum">     772 </span>            : </a>
<a name="773"><span class="lineNum">     773 </span>            :             std::array&lt;einsums::Range, Rank&gt; slice;</a>
<a name="774"><span class="lineNum">     774 </span>            : </a>
<a name="775"><span class="lineNum">     775 </span>            :             slice.fill(X.block_range());</a>
<a name="776"><span class="lineNum">     776 </span>            : </a>
<a name="777"><span class="lineNum">     777 </span>            :             auto Y_block = std::apply(*Y, slice);</a>
<a name="778"><span class="lineNum">     778 </span>            : </a>
<a name="779"><span class="lineNum">     779 </span>            :             axpy(alpha, X[i], &amp;Y_block);</a>
<a name="780"><span class="lineNum">     780 </span>            :         }</a>
<a name="781"><span class="lineNum">     781 </span>            :     } else {</a>
<a name="782"><span class="lineNum">     782 </span>            : </a>
<a name="783"><span class="lineNum">     783 </span><span class="lineCov">          3 :         LabeledSection0();</span></a>
<a name="784"><span class="lineNum">     784 </span>            : </a>
<a name="785"><span class="lineNum">     785 </span><span class="lineCov">          1 :         blas::axpy(X.dim(0) * X.stride(0), alpha, X.data(), 1, Y-&gt;data(), 1);</span></a>
<a name="786"><span class="lineNum">     786 </span><span class="lineCov">          1 :     }</span></a>
<a name="787"><span class="lineNum">     787 </span><span class="lineCov">          1 : }</span></a>
<a name="788"><span class="lineNum">     788 </span>            : </a>
<a name="789"><span class="lineNum">     789 </span>            : template &lt;template &lt;typename, size_t&gt; typename XType, template &lt;typename, size_t&gt; typename YType, typename T, size_t Rank&gt;</a>
<a name="790"><span class="lineNum">     790 </span>            :     requires requires {</a>
<a name="791"><span class="lineNum">     791 </span>            :         requires CoreRankTensor&lt;XType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="792"><span class="lineNum">     792 </span>            :         requires CoreRankTensor&lt;YType&lt;T, Rank&gt;, Rank, T&gt;;</a>
<a name="793"><span class="lineNum">     793 </span>            :         requires !CoreRankBlockTensor&lt;YType&lt;T, Rank&gt;, Rank, T&gt; ||</a>
<a name="794"><span class="lineNum">     794 </span>            :                      (CoreRankBlockTensor&lt;XType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp; CoreRankBlockTensor&lt;YType&lt;T, Rank&gt;, Rank, T&gt;);</a>
<a name="795"><span class="lineNum">     795 </span>            :     }</a>
<a name="796"><span class="lineNum">     796 </span><span class="lineCov">          3 : void axpby(T alpha, const XType&lt;T, Rank&gt; &amp;X, T beta, YType&lt;T, Rank&gt; *Y) {</span></a>
<a name="797"><span class="lineNum">     797 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;XType&lt;T, Rank&gt;, Rank, T&gt; &amp;&amp;</a>
<a name="798"><span class="lineNum">     798 </span>            :                   einsums::detail::IsIncoreRankBlockTensorV&lt;YType&lt;T, Rank&gt;, Rank, T&gt;) {</a>
<a name="799"><span class="lineNum">     799 </span>            :         if (X.num_blocks() != Y-&gt;num_blocks()) {</a>
<a name="800"><span class="lineNum">     800 </span>            :             throw std::runtime_error(&quot;axpby: Tensors need to have the same number of blocks.&quot;);</a>
<a name="801"><span class="lineNum">     801 </span>            :         }</a>
<a name="802"><span class="lineNum">     802 </span>            : </a>
<a name="803"><span class="lineNum">     803 </span>            :         if (X.ranges() != Y-&gt;ranges()) {</a>
<a name="804"><span class="lineNum">     804 </span>            :             throw std::runtime_error(&quot;axpby: Tensor blocks need to be compatible.&quot;);</a>
<a name="805"><span class="lineNum">     805 </span>            :         }</a>
<a name="806"><span class="lineNum">     806 </span>            : </a>
<a name="807"><span class="lineNum">     807 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="808"><span class="lineNum">     808 </span>            :         for (int i = 0; i &lt; X.num_blocks(); i++) {</a>
<a name="809"><span class="lineNum">     809 </span>            :             if (X.block_dim() == 0) {</a>
<a name="810"><span class="lineNum">     810 </span>            :                 continue;</a>
<a name="811"><span class="lineNum">     811 </span>            :             }</a>
<a name="812"><span class="lineNum">     812 </span>            :             axpby(alpha, X[i], beta, Y-&gt;block(i));</a>
<a name="813"><span class="lineNum">     813 </span>            :         }</a>
<a name="814"><span class="lineNum">     814 </span>            :     } else if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;XType&lt;T, Rank&gt;, Rank, T&gt;) {</a>
<a name="815"><span class="lineNum">     815 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="816"><span class="lineNum">     816 </span>            :         for (int i = 0; i &lt; X.num_blocks(); i++) {</a>
<a name="817"><span class="lineNum">     817 </span>            :             if (X.block_dim() == 0) {</a>
<a name="818"><span class="lineNum">     818 </span>            :                 continue;</a>
<a name="819"><span class="lineNum">     819 </span>            :             }</a>
<a name="820"><span class="lineNum">     820 </span>            : </a>
<a name="821"><span class="lineNum">     821 </span>            :             std::array&lt;einsums::Range, Rank&gt; slice;</a>
<a name="822"><span class="lineNum">     822 </span>            : </a>
<a name="823"><span class="lineNum">     823 </span>            :             slice.fill(X.block_range());</a>
<a name="824"><span class="lineNum">     824 </span>            : </a>
<a name="825"><span class="lineNum">     825 </span>            :             auto Y_block = std::apply(*Y, slice);</a>
<a name="826"><span class="lineNum">     826 </span>            : </a>
<a name="827"><span class="lineNum">     827 </span>            :             axpby(alpha, X[i], beta, &amp;Y_block);</a>
<a name="828"><span class="lineNum">     828 </span>            :         }</a>
<a name="829"><span class="lineNum">     829 </span>            :     } else {</a>
<a name="830"><span class="lineNum">     830 </span>            : </a>
<a name="831"><span class="lineNum">     831 </span><span class="lineCov">          9 :         LabeledSection0();</span></a>
<a name="832"><span class="lineNum">     832 </span>            : </a>
<a name="833"><span class="lineNum">     833 </span><span class="lineCov">          3 :         blas::axpby(X.dim(0) * X.stride(0), alpha, X.data(), 1, beta, Y-&gt;data(), 1);</span></a>
<a name="834"><span class="lineNum">     834 </span><span class="lineCov">          3 :     }</span></a>
<a name="835"><span class="lineNum">     835 </span><span class="lineCov">          3 : }</span></a>
<a name="836"><span class="lineNum">     836 </span>            : </a>
<a name="837"><span class="lineNum">     837 </span>            : template &lt;template &lt;typename, size_t&gt; typename XYType, size_t XYRank, template &lt;typename, size_t&gt; typename AType, typename T, size_t ARank&gt;</a>
<a name="838"><span class="lineNum">     838 </span>            :     requires requires {</a>
<a name="839"><span class="lineNum">     839 </span>            :         requires CoreRankTensor&lt;XYType&lt;T, XYRank&gt;, 1, T&gt;;</a>
<a name="840"><span class="lineNum">     840 </span>            :         requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;;</a>
<a name="841"><span class="lineNum">     841 </span>            :     }</a>
<a name="842"><span class="lineNum">     842 </span><span class="lineCov">         34 : void ger(T alpha, const XYType&lt;T, XYRank&gt; &amp;X, const XYType&lt;T, XYRank&gt; &amp;Y, AType&lt;T, ARank&gt; *A) {</span></a>
<a name="843"><span class="lineNum">     843 </span><span class="lineCov">        102 :     LabeledSection0();</span></a>
<a name="844"><span class="lineNum">     844 </span>            : </a>
<a name="845"><span class="lineNum">     845 </span><span class="lineCov">         34 :     blas::ger(X.dim(0), Y.dim(0), alpha, X.data(), X.stride(0), Y.data(), Y.stride(0), A-&gt;data(), A-&gt;stride(0));</span></a>
<a name="846"><span class="lineNum">     846 </span><span class="lineCov">         34 : }</span></a>
<a name="847"><span class="lineNum">     847 </span>            : </a>
<a name="848"><span class="lineNum">     848 </span>            : /**</a>
<a name="849"><span class="lineNum">     849 </span>            :  * @brief Computes the LU factorization of a general m-by-n matrix.</a>
<a name="850"><span class="lineNum">     850 </span>            :  *</a>
<a name="851"><span class="lineNum">     851 </span>            :  * The routine computes the LU factorization of a general m-by-n matrix A as</a>
<a name="852"><span class="lineNum">     852 </span>            :  * \f[</a>
<a name="853"><span class="lineNum">     853 </span>            :  * A = P*L*U</a>
<a name="854"><span class="lineNum">     854 </span>            :  * \f]</a>
<a name="855"><span class="lineNum">     855 </span>            :  * where P is a permutation matrix, L is lower triangular with unit diagonal elements and U is upper triangular. The routine uses</a>
<a name="856"><span class="lineNum">     856 </span>            :  * partial pivoting, with row interchanges.</a>
<a name="857"><span class="lineNum">     857 </span>            :  *</a>
<a name="858"><span class="lineNum">     858 </span>            :  * @tparam TensorType</a>
<a name="859"><span class="lineNum">     859 </span>            :  * @tparam T</a>
<a name="860"><span class="lineNum">     860 </span>            :  * @tparam TensorRank</a>
<a name="861"><span class="lineNum">     861 </span>            :  * @param A</a>
<a name="862"><span class="lineNum">     862 </span>            :  * @param pivot</a>
<a name="863"><span class="lineNum">     863 </span>            :  * @return</a>
<a name="864"><span class="lineNum">     864 </span>            :  */</a>
<a name="865"><span class="lineNum">     865 </span>            : template &lt;template &lt;typename, size_t&gt; typename TensorType, typename T, size_t TensorRank&gt;</a>
<a name="866"><span class="lineNum">     866 </span>            :     requires CoreRankTensor&lt;TensorType&lt;T, TensorRank&gt;, 2, T&gt;</a>
<a name="867"><span class="lineNum">     867 </span><span class="lineCov">          3 : auto getrf(TensorType&lt;T, TensorRank&gt; *A, std::vector&lt;blas_int&gt; *pivot) -&gt; int {</span></a>
<a name="868"><span class="lineNum">     868 </span><span class="lineCov">          9 :     LabeledSection0();</span></a>
<a name="869"><span class="lineNum">     869 </span>            : </a>
<a name="870"><span class="lineNum">     870 </span><span class="lineCov">          3 :     if (pivot-&gt;size() &lt; std::min(A-&gt;dim(0), A-&gt;dim(1))) {</span></a>
<a name="871"><span class="lineNum">     871 </span><span class="lineNoCov">          0 :         println(&quot;getrf: resizing pivot vector from {} to {}&quot;, pivot-&gt;size(), std::min(A-&gt;dim(0), A-&gt;dim(1)));</span></a>
<a name="872"><span class="lineNum">     872 </span><span class="lineNoCov">          0 :         pivot-&gt;resize(std::min(A-&gt;dim(0), A-&gt;dim(1)));</span></a>
<a name="873"><span class="lineNum">     873 </span>            :     }</a>
<a name="874"><span class="lineNum">     874 </span><span class="lineCov">          3 :     int result = blas::getrf(A-&gt;dim(0), A-&gt;dim(1), A-&gt;data(), A-&gt;stride(0), pivot-&gt;data());</span></a>
<a name="875"><span class="lineNum">     875 </span>            : </a>
<a name="876"><span class="lineNum">     876 </span><span class="lineCov">          3 :     if (result &lt; 0) {</span></a>
<a name="877"><span class="lineNum">     877 </span><span class="lineNoCov">          0 :         println_warn(&quot;getrf: argument {} has an invalid value&quot;, -result);</span></a>
<a name="878"><span class="lineNum">     878 </span><span class="lineNoCov">          0 :         abort();</span></a>
<a name="879"><span class="lineNum">     879 </span>            :     }</a>
<a name="880"><span class="lineNum">     880 </span>            : </a>
<a name="881"><span class="lineNum">     881 </span><span class="lineCov">          3 :     return result;</span></a>
<a name="882"><span class="lineNum">     882 </span><span class="lineCov">          3 : }</span></a>
<a name="883"><span class="lineNum">     883 </span>            : </a>
<a name="884"><span class="lineNum">     884 </span>            : /**</a>
<a name="885"><span class="lineNum">     885 </span>            :  * @brief Computes the inverse of a matrix using the LU factorization computed by getrf.</a>
<a name="886"><span class="lineNum">     886 </span>            :  *</a>
<a name="887"><span class="lineNum">     887 </span>            :  * The routine computes the inverse \f$inv(A)\f$ of a general matrix \f$A\f$. Before calling this routine, call getrf to factorize \f$A\f$.</a>
<a name="888"><span class="lineNum">     888 </span>            :  *</a>
<a name="889"><span class="lineNum">     889 </span>            :  * @tparam TensorType The type of the tensor</a>
<a name="890"><span class="lineNum">     890 </span>            :  * @tparam T The underlying data type</a>
<a name="891"><span class="lineNum">     891 </span>            :  * @tparam TensorRank The rank of the tensor</a>
<a name="892"><span class="lineNum">     892 </span>            :  * @param A The matrix to invert</a>
<a name="893"><span class="lineNum">     893 </span>            :  * @param pivot The pivot vector from getrf</a>
<a name="894"><span class="lineNum">     894 </span>            :  * @return int If 0, the execution is successful.</a>
<a name="895"><span class="lineNum">     895 </span>            :  */</a>
<a name="896"><span class="lineNum">     896 </span>            : template &lt;template &lt;typename, size_t&gt; typename TensorType, typename T, size_t TensorRank&gt;</a>
<a name="897"><span class="lineNum">     897 </span>            :     requires CoreRankTensor&lt;TensorType&lt;T, TensorRank&gt;, 2, T&gt;</a>
<a name="898"><span class="lineNum">     898 </span><span class="lineCov">          3 : auto getri(TensorType&lt;T, TensorRank&gt; *A, const std::vector&lt;blas_int&gt; &amp;pivot) -&gt; int {</span></a>
<a name="899"><span class="lineNum">     899 </span><span class="lineCov">          9 :     LabeledSection0();</span></a>
<a name="900"><span class="lineNum">     900 </span>            : </a>
<a name="901"><span class="lineNum">     901 </span><span class="lineCov">          3 :     int result = blas::getri(A-&gt;dim(0), A-&gt;data(), A-&gt;stride(0), pivot.data());</span></a>
<a name="902"><span class="lineNum">     902 </span>            : </a>
<a name="903"><span class="lineNum">     903 </span><span class="lineCov">          3 :     if (result &lt; 0) {</span></a>
<a name="904"><span class="lineNum">     904 </span><span class="lineNoCov">          0 :         println_warn(&quot;getri: argument {} has an invalid value&quot;, -result);</span></a>
<a name="905"><span class="lineNum">     905 </span>            :     }</a>
<a name="906"><span class="lineNum">     906 </span><span class="lineCov">          3 :     return result;</span></a>
<a name="907"><span class="lineNum">     907 </span><span class="lineCov">          3 : }</span></a>
<a name="908"><span class="lineNum">     908 </span>            : </a>
<a name="909"><span class="lineNum">     909 </span>            : /**</a>
<a name="910"><span class="lineNum">     910 </span>            :  * @brief Inverts a matrix.</a>
<a name="911"><span class="lineNum">     911 </span>            :  *</a>
<a name="912"><span class="lineNum">     912 </span>            :  * Utilizes the LAPACK routines getrf and getri to invert a matrix.</a>
<a name="913"><span class="lineNum">     913 </span>            :  *</a>
<a name="914"><span class="lineNum">     914 </span>            :  * @tparam TensorType The type of the tensor</a>
<a name="915"><span class="lineNum">     915 </span>            :  * @tparam T The underlying data type</a>
<a name="916"><span class="lineNum">     916 </span>            :  * @tparam TensorRank The rank of the tensor</a>
<a name="917"><span class="lineNum">     917 </span>            :  * @param A Matrix to invert. On exit, the inverse of A.</a>
<a name="918"><span class="lineNum">     918 </span>            :  */</a>
<a name="919"><span class="lineNum">     919 </span>            : template &lt;template &lt;typename, size_t&gt; typename TensorType, typename T, size_t TensorRank&gt;</a>
<a name="920"><span class="lineNum">     920 </span>            :     requires CoreRankTensor&lt;TensorType&lt;T, TensorRank&gt;, 2, T&gt;</a>
<a name="921"><span class="lineNum">     921 </span><span class="lineCov">          1 : void invert(TensorType&lt;T, TensorRank&gt; *A) {</span></a>
<a name="922"><span class="lineNum">     922 </span>            :     if constexpr (einsums::detail::IsIncoreRankBlockTensorV&lt;TensorType&lt;T, TensorRank&gt;, TensorRank, T&gt;) {</a>
<a name="923"><span class="lineNum">     923 </span>            :         EINSUMS_OMP_PARALLEL_FOR</a>
<a name="924"><span class="lineNum">     924 </span>            :         for (int i = 0; i &lt; A-&gt;num_blocks; i++) {</a>
<a name="925"><span class="lineNum">     925 </span>            :             einsums::linear_algebra::invert(&amp;(A-&gt;block(i)));</a>
<a name="926"><span class="lineNum">     926 </span>            :         }</a>
<a name="927"><span class="lineNum">     927 </span>            :     } else {</a>
<a name="928"><span class="lineNum">     928 </span><span class="lineCov">          3 :         LabeledSection0();</span></a>
<a name="929"><span class="lineNum">     929 </span>            : </a>
<a name="930"><span class="lineNum">     930 </span><span class="lineCov">          1 :         std::vector&lt;blas_int&gt; pivot(A-&gt;dim(0));</span></a>
<a name="931"><span class="lineNum">     931 </span><span class="lineCov">          1 :         int                   result = getrf(A, &amp;pivot);</span></a>
<a name="932"><span class="lineNum">     932 </span><span class="lineCov">          1 :         if (result &gt; 0) {</span></a>
<a name="933"><span class="lineNum">     933 </span><span class="lineNoCov">          0 :             println_abort(&quot;invert: getrf: the ({}, {}) element of the factor U or L is zero, and the inverse could not be computed&quot;, result,</span></a>
<a name="934"><span class="lineNum">     934 </span>            :                           result);</a>
<a name="935"><span class="lineNum">     935 </span>            :         }</a>
<a name="936"><span class="lineNum">     936 </span>            : </a>
<a name="937"><span class="lineNum">     937 </span><span class="lineCov">          1 :         result = getri(A, pivot);</span></a>
<a name="938"><span class="lineNum">     938 </span><span class="lineCov">          1 :         if (result &gt; 0) {</span></a>
<a name="939"><span class="lineNum">     939 </span><span class="lineNoCov">          0 :             println_abort(&quot;invert: getri: the ({}, {}) element of the factor U or L i zero, and the inverse could not be computed&quot;, result,</span></a>
<a name="940"><span class="lineNum">     940 </span>            :                           result);</a>
<a name="941"><span class="lineNum">     941 </span>            :         }</a>
<a name="942"><span class="lineNum">     942 </span><span class="lineCov">          1 :     }</span></a>
<a name="943"><span class="lineNum">     943 </span><span class="lineCov">          1 : }</span></a>
<a name="944"><span class="lineNum">     944 </span>            : </a>
<a name="945"><span class="lineNum">     945 </span>            : #if !defined(DOXYGEN_SHOULD_SKIP_THIS)</a>
<a name="946"><span class="lineNum">     946 </span>            : template &lt;SmartPointer SmartPtr&gt;</a>
<a name="947"><span class="lineNum">     947 </span>            : void invert(SmartPtr *A) {</a>
<a name="948"><span class="lineNum">     948 </span>            :     LabeledSection0();</a>
<a name="949"><span class="lineNum">     949 </span>            : </a>
<a name="950"><span class="lineNum">     950 </span>            :     return invert(A-&gt;get());</a>
<a name="951"><span class="lineNum">     951 </span>            : }</a>
<a name="952"><span class="lineNum">     952 </span>            : #endif</a>
<a name="953"><span class="lineNum">     953 </span>            : </a>
<a name="954"><span class="lineNum">     954 </span>            : /**</a>
<a name="955"><span class="lineNum">     955 </span>            :  * @brief Indicates the type of norm to compute.</a>
<a name="956"><span class="lineNum">     956 </span>            :  */</a>
<a name="957"><span class="lineNum">     957 </span>            : enum class Norm : char {</a>
<a name="958"><span class="lineNum">     958 </span>            :     MaxAbs    = 'M', /**&lt; \f$val = max(abs(Aij))\f$, largest absolute value of the matrix A. */</a>
<a name="959"><span class="lineNum">     959 </span>            :     One       = '1', /**&lt; \f$val = norm1(A)\f$, 1-norm of the matrix A (maximum column sum) */</a>
<a name="960"><span class="lineNum">     960 </span>            :     Infinity  = 'I', /**&lt; \f$val = normI(A)\f$, infinity norm of the matrix A (maximum row sum) */</a>
<a name="961"><span class="lineNum">     961 </span>            :     Frobenius = 'F', /**&lt; \f$val = normF(A)\f$, Frobenius norm of the matrix A (square root of sum of squares). */</a>
<a name="962"><span class="lineNum">     962 </span>            :     //    Two       = 'F'</a>
<a name="963"><span class="lineNum">     963 </span>            : };</a>
<a name="964"><span class="lineNum">     964 </span>            : </a>
<a name="965"><span class="lineNum">     965 </span>            : /**</a>
<a name="966"><span class="lineNum">     966 </span>            :  * @brief Computes the norm of a matrix.</a>
<a name="967"><span class="lineNum">     967 </span>            :  *</a>
<a name="968"><span class="lineNum">     968 </span>            :  * Returns the value of the one norm, or the Frobenius norm, or</a>
<a name="969"><span class="lineNum">     969 </span>            :  * the infinity norm, or the element of largest absolute value of a</a>
<a name="970"><span class="lineNum">     970 </span>            :  * real matrix A.</a>
<a name="971"><span class="lineNum">     971 </span>            :  *</a>
<a name="972"><span class="lineNum">     972 </span>            :  * @note</a>
<a name="973"><span class="lineNum">     973 </span>            :  * This function assumes that the matrix is stored in column-major order.</a>
<a name="974"><span class="lineNum">     974 </span>            :  *</a>
<a name="975"><span class="lineNum">     975 </span>            :  * @code</a>
<a name="976"><span class="lineNum">     976 </span>            :  * using namespace einsums;</a>
<a name="977"><span class="lineNum">     977 </span>            :  *</a>
<a name="978"><span class="lineNum">     978 </span>            :  * auto A = einsums::create_random_tensor(&quot;A&quot;, 3, 3);</a>
<a name="979"><span class="lineNum">     979 </span>            :  * auto norm = einsums::linear_algebra::norm(einsums::linear_algebra::Norm::One, A);</a>
<a name="980"><span class="lineNum">     980 </span>            :  * @endcode</a>
<a name="981"><span class="lineNum">     981 </span>            :  *</a>
<a name="982"><span class="lineNum">     982 </span>            :  * @tparam AType The type of the matrix</a>
<a name="983"><span class="lineNum">     983 </span>            :  * @tparam ADataType The underlying data type of the matrix</a>
<a name="984"><span class="lineNum">     984 </span>            :  * @tparam ARank The rank of the matrix</a>
<a name="985"><span class="lineNum">     985 </span>            :  * @param norm_type where Norm::One denotes the one norm of a matrix (maximum column sum),</a>
<a name="986"><span class="lineNum">     986 </span>            :  *   Norm::Infinity denotes the infinity norm of a matrix  (maximum row sum) and</a>
<a name="987"><span class="lineNum">     987 </span>            :  *   Norm::Frobenius denotes the Frobenius norm of a matrix (square root of sum of</a>
<a name="988"><span class="lineNum">     988 </span>            :  *   squares). Note that \f$ max(abs(A(i,j))) \f$ is not a consistent matrix norm.</a>
<a name="989"><span class="lineNum">     989 </span>            :  * @param a The matrix to compute the norm of</a>
<a name="990"><span class="lineNum">     990 </span>            :  * @return The norm of the matrix</a>
<a name="991"><span class="lineNum">     991 </span>            :  */</a>
<a name="992"><span class="lineNum">     992 </span>            : </a>
<a name="993"><span class="lineNum">     993 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, typename ADataType, size_t ARank&gt;</a>
<a name="994"><span class="lineNum">     994 </span>            :     requires CoreRankTensor&lt;AType&lt;ADataType, ARank&gt;, 2, ADataType&gt;</a>
<a name="995"><span class="lineNum">     995 </span><span class="lineCov">          2 : auto norm(Norm norm_type, const AType&lt;ADataType, ARank&gt; &amp;a) -&gt; RemoveComplexT&lt;ADataType&gt; {</span></a>
<a name="996"><span class="lineNum">     996 </span><span class="lineCov">          6 :     LabeledSection0();</span></a>
<a name="997"><span class="lineNum">     997 </span>            : </a>
<a name="998"><span class="lineNum">     998 </span><span class="lineCov">          2 :     std::vector&lt;RemoveComplexT&lt;ADataType&gt;&gt; work(4 * a.dim(0), 0.0);</span></a>
<a name="999"><span class="lineNum">     999 </span><span class="lineCov">          4 :     return blas::lange(static_cast&lt;char&gt;(norm_type), a.dim(0), a.dim(1), a.data(), a.stride(0), work.data());</span></a>
<a name="1000"><span class="lineNum">    1000 </span><span class="lineCov">          2 : }</span></a>
<a name="1001"><span class="lineNum">    1001 </span>            : </a>
<a name="1002"><span class="lineNum">    1002 </span>            : // Uses the original svd function found in lapack, gesvd, request all left and right vectors.</a>
<a name="1003"><span class="lineNum">    1003 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, typename T, size_t ARank&gt;</a>
<a name="1004"><span class="lineNum">    1004 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="1005"><span class="lineNum">    1005 </span><span class="lineCov">          2 : auto svd(const AType&lt;T, ARank&gt; &amp;_A) -&gt; std::tuple&lt;Tensor&lt;T, 2&gt;, Tensor&lt;RemoveComplexT&lt;T&gt;, 1&gt;, Tensor&lt;T, 2&gt;&gt; {</span></a>
<a name="1006"><span class="lineNum">    1006 </span><span class="lineCov">          4 :     LabeledSection0();</span></a>
<a name="1007"><span class="lineNum">    1007 </span>            : </a>
<a name="1008"><span class="lineNum">    1008 </span><span class="lineCov">          2 :     DisableOMPThreads const nothreads;</span></a>
<a name="1009"><span class="lineNum">    1009 </span>            : </a>
<a name="1010"><span class="lineNum">    1010 </span>            :     // Calling svd will destroy the original data. Make a copy of it.</a>
<a name="1011"><span class="lineNum">    1011 </span><span class="lineCov">          2 :     Tensor&lt;T, 2&gt; A = _A;</span></a>
<a name="1012"><span class="lineNum">    1012 </span>            : </a>
<a name="1013"><span class="lineNum">    1013 </span><span class="lineCov">          2 :     size_t m   = A.dim(0);</span></a>
<a name="1014"><span class="lineNum">    1014 </span><span class="lineCov">          2 :     size_t n   = A.dim(1);</span></a>
<a name="1015"><span class="lineNum">    1015 </span><span class="lineCov">          2 :     size_t lda = A.stride(0);</span></a>
<a name="1016"><span class="lineNum">    1016 </span>            : </a>
<a name="1017"><span class="lineNum">    1017 </span>            :     // Test if it absolutely necessary to zero out these tensors first.</a>
<a name="1018"><span class="lineNum">    1018 </span><span class="lineCov">          2 :     auto U = create_tensor&lt;T&gt;(&quot;U (stored columnwise)&quot;, m, m);</span></a>
<a name="1019"><span class="lineNum">    1019 </span><span class="lineCov">          2 :     U.zero();</span></a>
<a name="1020"><span class="lineNum">    1020 </span><span class="lineCov">          2 :     auto S = create_tensor&lt;RemoveComplexT&lt;T&gt;&gt;(&quot;S&quot;, std::min(m, n));</span></a>
<a name="1021"><span class="lineNum">    1021 </span><span class="lineCov">          2 :     S.zero();</span></a>
<a name="1022"><span class="lineNum">    1022 </span><span class="lineCov">          2 :     auto Vt = create_tensor&lt;T&gt;(&quot;Vt (stored rowwise)&quot;, n, n);</span></a>
<a name="1023"><span class="lineNum">    1023 </span><span class="lineCov">          2 :     Vt.zero();</span></a>
<a name="1024"><span class="lineNum">    1024 </span><span class="lineCov">          2 :     auto superb = create_tensor&lt;T&gt;(&quot;superb&quot;, std::min(m, n) - 2);</span></a>
<a name="1025"><span class="lineNum">    1025 </span><span class="lineCov">          2 :     superb.zero();</span></a>
<a name="1026"><span class="lineNum">    1026 </span>            : </a>
<a name="1027"><span class="lineNum">    1027 </span><span class="lineCov">          2 :     int info = blas::gesvd('A', 'A', m, n, A.data(), lda, S.data(), U.data(), m, Vt.data(), n, superb.data());</span></a>
<a name="1028"><span class="lineNum">    1028 </span>            : </a>
<a name="1029"><span class="lineNum">    1029 </span><span class="lineCov">          2 :     if (info != 0) {</span></a>
<a name="1030"><span class="lineNum">    1030 </span><span class="lineNoCov">          0 :         if (info &lt; 0) {</span></a>
<a name="1031"><span class="lineNum">    1031 </span><span class="lineNoCov">          0 :             println_abort(&quot;svd: Argument {} has an invalid parameter\n#2 (m) = {}, #3 (n) = {}, #5 (n) = {}, #8 (m) = {}&quot;, -info, m, n, n,</span></a>
<a name="1032"><span class="lineNum">    1032 </span>            :                           m);</a>
<a name="1033"><span class="lineNum">    1033 </span>            :         } else {</a>
<a name="1034"><span class="lineNum">    1034 </span><span class="lineNoCov">          0 :             println_abort(&quot;svd: error value {}&quot;, info);</span></a>
<a name="1035"><span class="lineNum">    1035 </span>            :         }</a>
<a name="1036"><span class="lineNum">    1036 </span>            :     }</a>
<a name="1037"><span class="lineNum">    1037 </span>            : </a>
<a name="1038"><span class="lineNum">    1038 </span><span class="lineCov">          2 :     return std::make_tuple(U, S, Vt);</span></a>
<a name="1039"><span class="lineNum">    1039 </span><span class="lineCov">          2 : }</span></a>
<a name="1040"><span class="lineNum">    1040 </span>            : </a>
<a name="1041"><span class="lineNum">    1041 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, typename T, size_t Rank&gt;</a>
<a name="1042"><span class="lineNum">    1042 </span>            :     requires CoreRankTensor&lt;AType&lt;T, Rank&gt;, 2, T&gt;</a>
<a name="1043"><span class="lineNum">    1043 </span>            : auto svd_nullspace(const AType&lt;T, Rank&gt; &amp;_A) -&gt; Tensor&lt;T, 2&gt; {</a>
<a name="1044"><span class="lineNum">    1044 </span>            :     LabeledSection0();</a>
<a name="1045"><span class="lineNum">    1045 </span>            : </a>
<a name="1046"><span class="lineNum">    1046 </span>            :     // Calling svd will destroy the original data. Make a copy of it.</a>
<a name="1047"><span class="lineNum">    1047 </span>            :     Tensor&lt;T, 2&gt; A = _A;</a>
<a name="1048"><span class="lineNum">    1048 </span>            : </a>
<a name="1049"><span class="lineNum">    1049 </span>            :     blas_int m   = A.dim(0);</a>
<a name="1050"><span class="lineNum">    1050 </span>            :     blas_int n   = A.dim(1);</a>
<a name="1051"><span class="lineNum">    1051 </span>            :     blas_int lda = A.stride(0);</a>
<a name="1052"><span class="lineNum">    1052 </span>            : </a>
<a name="1053"><span class="lineNum">    1053 </span>            :     auto U = create_tensor&lt;T&gt;(&quot;U&quot;, m, m);</a>
<a name="1054"><span class="lineNum">    1054 </span>            :     zero(U);</a>
<a name="1055"><span class="lineNum">    1055 </span>            :     auto S = create_tensor&lt;T&gt;(&quot;S&quot;, n);</a>
<a name="1056"><span class="lineNum">    1056 </span>            :     zero(S);</a>
<a name="1057"><span class="lineNum">    1057 </span>            :     auto V = create_tensor&lt;T&gt;(&quot;V&quot;, n, n);</a>
<a name="1058"><span class="lineNum">    1058 </span>            :     zero(V);</a>
<a name="1059"><span class="lineNum">    1059 </span>            :     auto superb = create_tensor&lt;T&gt;(&quot;superb&quot;, std::min(m, n) - 2);</a>
<a name="1060"><span class="lineNum">    1060 </span>            : </a>
<a name="1061"><span class="lineNum">    1061 </span>            :     int info = blas::gesvd('N', 'A', m, n, A.data(), lda, S.data(), U.data(), m, V.data(), n, superb.data());</a>
<a name="1062"><span class="lineNum">    1062 </span>            : </a>
<a name="1063"><span class="lineNum">    1063 </span>            :     if (info != 0) {</a>
<a name="1064"><span class="lineNum">    1064 </span>            :         if (info &lt; 0) {</a>
<a name="1065"><span class="lineNum">    1065 </span>            :             println_abort(&quot;svd: Argument {} has an invalid parameter\n#2 (m) = {}, #3 (n) = {}, #5 (n) = {}, #8 (m) = {}&quot;, -info, m, n, n,</a>
<a name="1066"><span class="lineNum">    1066 </span>            :                           m);</a>
<a name="1067"><span class="lineNum">    1067 </span>            :         } else {</a>
<a name="1068"><span class="lineNum">    1068 </span>            :             println_abort(&quot;svd: error value {}&quot;, info);</a>
<a name="1069"><span class="lineNum">    1069 </span>            :         }</a>
<a name="1070"><span class="lineNum">    1070 </span>            :     }</a>
<a name="1071"><span class="lineNum">    1071 </span>            : </a>
<a name="1072"><span class="lineNum">    1072 </span>            :     // Determine the rank of the nullspace matrix</a>
<a name="1073"><span class="lineNum">    1073 </span>            :     int rank = 0;</a>
<a name="1074"><span class="lineNum">    1074 </span>            :     for (int i = 0; i &lt; n; i++) {</a>
<a name="1075"><span class="lineNum">    1075 </span>            :         if (S(i) &gt; 1e-12) {</a>
<a name="1076"><span class="lineNum">    1076 </span>            :             rank++;</a>
<a name="1077"><span class="lineNum">    1077 </span>            :         }</a>
<a name="1078"><span class="lineNum">    1078 </span>            :     }</a>
<a name="1079"><span class="lineNum">    1079 </span>            : </a>
<a name="1080"><span class="lineNum">    1080 </span>            :     // println(&quot;rank {}&quot;, rank);</a>
<a name="1081"><span class="lineNum">    1081 </span>            :     auto Vview     = V(Range{rank, V.dim(0)}, All);</a>
<a name="1082"><span class="lineNum">    1082 </span>            :     auto nullspace = Tensor(V);</a>
<a name="1083"><span class="lineNum">    1083 </span>            : </a>
<a name="1084"><span class="lineNum">    1084 </span>            :     // Normalize nullspace. LAPACK does not guarentee them to be orthonormal</a>
<a name="1085"><span class="lineNum">    1085 </span>            :     for (int i = 0; i &lt; nullspace.dim(0); i++) {</a>
<a name="1086"><span class="lineNum">    1086 </span>            :         T sum{0};</a>
<a name="1087"><span class="lineNum">    1087 </span>            :         for (int j = 0; j &lt; nullspace.dim(1); j++) {</a>
<a name="1088"><span class="lineNum">    1088 </span>            :             sum += std::pow(nullspace(i, j), 2.0);</a>
<a name="1089"><span class="lineNum">    1089 </span>            :         }</a>
<a name="1090"><span class="lineNum">    1090 </span>            :         sum = std::sqrt(sum);</a>
<a name="1091"><span class="lineNum">    1091 </span>            :         scale_row(i, sum, &amp;nullspace);</a>
<a name="1092"><span class="lineNum">    1092 </span>            :     }</a>
<a name="1093"><span class="lineNum">    1093 </span>            : </a>
<a name="1094"><span class="lineNum">    1094 </span>            :     return nullspace;</a>
<a name="1095"><span class="lineNum">    1095 </span>            : }</a>
<a name="1096"><span class="lineNum">    1096 </span>            : </a>
<a name="1097"><span class="lineNum">    1097 </span>            : enum class Vectors : char { All = 'A', Some = 'S', Overwrite = 'O', None = 'N' };</a>
<a name="1098"><span class="lineNum">    1098 </span>            : </a>
<a name="1099"><span class="lineNum">    1099 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, typename T, size_t ARank&gt;</a>
<a name="1100"><span class="lineNum">    1100 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="1101"><span class="lineNum">    1101 </span><span class="lineCov">        132 : auto svd_dd(const AType&lt;T, ARank&gt; &amp;_A, Vectors job = Vectors::All) -&gt; std::tuple&lt;Tensor&lt;T, 2&gt;, Tensor&lt;RemoveComplexT&lt;T&gt;, 1&gt;, Tensor&lt;T, 2&gt;&gt; {</span></a>
<a name="1102"><span class="lineNum">    1102 </span><span class="lineCov">        264 :     LabeledSection0();</span></a>
<a name="1103"><span class="lineNum">    1103 </span>            : </a>
<a name="1104"><span class="lineNum">    1104 </span><span class="lineCov">        132 :     DisableOMPThreads const nothreads;</span></a>
<a name="1105"><span class="lineNum">    1105 </span>            : </a>
<a name="1106"><span class="lineNum">    1106 </span>            :     // Calling svd will destroy the original data. Make a copy of it.</a>
<a name="1107"><span class="lineNum">    1107 </span><span class="lineCov">        132 :     Tensor&lt;T, 2&gt; A = _A;</span></a>
<a name="1108"><span class="lineNum">    1108 </span>            : </a>
<a name="1109"><span class="lineNum">    1109 </span><span class="lineCov">        132 :     size_t m = A.dim(0);</span></a>
<a name="1110"><span class="lineNum">    1110 </span><span class="lineCov">        132 :     size_t n = A.dim(1);</span></a>
<a name="1111"><span class="lineNum">    1111 </span>            : </a>
<a name="1112"><span class="lineNum">    1112 </span>            :     // Test if it absolutely necessary to zero out these tensors first.</a>
<a name="1113"><span class="lineNum">    1113 </span><span class="lineCov">        132 :     auto U = create_tensor&lt;T&gt;(&quot;U (stored columnwise)&quot;, m, m);</span></a>
<a name="1114"><span class="lineNum">    1114 </span><span class="lineCov">        132 :     zero(U);</span></a>
<a name="1115"><span class="lineNum">    1115 </span><span class="lineCov">        132 :     auto S = create_tensor&lt;RemoveComplexT&lt;T&gt;&gt;(&quot;S&quot;, std::min(m, n));</span></a>
<a name="1116"><span class="lineNum">    1116 </span><span class="lineCov">        132 :     zero(S);</span></a>
<a name="1117"><span class="lineNum">    1117 </span><span class="lineCov">        132 :     auto Vt = create_tensor&lt;T&gt;(&quot;Vt (stored rowwise)&quot;, n, n);</span></a>
<a name="1118"><span class="lineNum">    1118 </span><span class="lineCov">        132 :     zero(Vt);</span></a>
<a name="1119"><span class="lineNum">    1119 </span>            : </a>
<a name="1120"><span class="lineNum">    1120 </span><span class="lineCov">        132 :     int info = blas::gesdd(static_cast&lt;char&gt;(job), static_cast&lt;int&gt;(m), static_cast&lt;int&gt;(n), A.data(), static_cast&lt;int&gt;(n), S.data(),</span></a>
<a name="1121"><span class="lineNum">    1121 </span>            :                            U.data(), static_cast&lt;int&gt;(m), Vt.data(), static_cast&lt;int&gt;(n));</a>
<a name="1122"><span class="lineNum">    1122 </span>            : </a>
<a name="1123"><span class="lineNum">    1123 </span><span class="lineCov">        132 :     if (info != 0) {</span></a>
<a name="1124"><span class="lineNum">    1124 </span><span class="lineNoCov">          0 :         if (info &lt; 0) {</span></a>
<a name="1125"><span class="lineNum">    1125 </span><span class="lineNoCov">          0 :             println_abort(&quot;svd_a: Argument {} has an invalid parameter\n#2 (m) = {}, #3 (n) = {}, #5 (n) = {}, #8 (m) = {}&quot;, -info, m, n, n,</span></a>
<a name="1126"><span class="lineNum">    1126 </span>            :                           m);</a>
<a name="1127"><span class="lineNum">    1127 </span>            :         } else {</a>
<a name="1128"><span class="lineNum">    1128 </span><span class="lineNoCov">          0 :             println_abort(&quot;svd_a: error value {}&quot;, info);</span></a>
<a name="1129"><span class="lineNum">    1129 </span>            :         }</a>
<a name="1130"><span class="lineNum">    1130 </span>            :     }</a>
<a name="1131"><span class="lineNum">    1131 </span>            : </a>
<a name="1132"><span class="lineNum">    1132 </span><span class="lineCov">        132 :     return std::make_tuple(U, S, Vt);</span></a>
<a name="1133"><span class="lineNum">    1133 </span><span class="lineCov">        132 : }</span></a>
<a name="1134"><span class="lineNum">    1134 </span>            : </a>
<a name="1135"><span class="lineNum">    1135 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, typename T, size_t ARank&gt;</a>
<a name="1136"><span class="lineNum">    1136 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="1137"><span class="lineNum">    1137 </span><span class="lineCov">          4 : auto truncated_svd(const AType&lt;T, ARank&gt; &amp;_A, size_t k) -&gt; std::tuple&lt;Tensor&lt;T, 2&gt;, Tensor&lt;RemoveComplexT&lt;T&gt;, 1&gt;, Tensor&lt;T, 2&gt;&gt; {</span></a>
<a name="1138"><span class="lineNum">    1138 </span><span class="lineCov">         12 :     LabeledSection0();</span></a>
<a name="1139"><span class="lineNum">    1139 </span>            : </a>
<a name="1140"><span class="lineNum">    1140 </span><span class="lineCov">          4 :     size_t m = _A.dim(0);</span></a>
<a name="1141"><span class="lineNum">    1141 </span><span class="lineCov">          4 :     size_t n = _A.dim(1);</span></a>
<a name="1142"><span class="lineNum">    1142 </span>            : </a>
<a name="1143"><span class="lineNum">    1143 </span>            :     // Omega Test Matrix</a>
<a name="1144"><span class="lineNum">    1144 </span><span class="lineCov">          4 :     auto omega = create_random_tensor&lt;T&gt;(&quot;omega&quot;, n, k + 5);</span></a>
<a name="1145"><span class="lineNum">    1145 </span>            : </a>
<a name="1146"><span class="lineNum">    1146 </span>            :     // Matrix Y = A * Omega</a>
<a name="1147"><span class="lineNum">    1147 </span><span class="lineCov">          6 :     Tensor&lt;T, 2&gt; Y(&quot;Y&quot;, m, k + 5);</span></a>
<a name="1148"><span class="lineNum">    1148 </span><span class="lineCov">          4 :     gemm&lt;false, false&gt;(T{1.0}, _A, omega, T{0.0}, &amp;Y);</span></a>
<a name="1149"><span class="lineNum">    1149 </span>            : </a>
<a name="1150"><span class="lineNum">    1150 </span><span class="lineCov">          8 :     Tensor&lt;T, 1&gt; tau(&quot;tau&quot;, std::min(m, k + 5));</span></a>
<a name="1151"><span class="lineNum">    1151 </span>            :     // Compute QR factorization of Y</a>
<a name="1152"><span class="lineNum">    1152 </span><span class="lineCov">          4 :     int info1 = blas::geqrf(m, k + 5, Y.data(), k + 5, tau.data());</span></a>
<a name="1153"><span class="lineNum">    1153 </span>            :     // Extract Matrix Q out of QR factorization</a>
<a name="1154"><span class="lineNum">    1154 </span>            :     if constexpr (!IsComplexV&lt;T&gt;) {</a>
<a name="1155"><span class="lineNum">    1155 </span><span class="lineCov">          2 :         int info2 = blas::orgqr(m, k + 5, tau.dim(0), Y.data(), k + 5, const_cast&lt;const T *&gt;(tau.data()));</span></a>
<a name="1156"><span class="lineNum">    1156 </span>            :     } else {</a>
<a name="1157"><span class="lineNum">    1157 </span><span class="lineCov">          2 :         int info2 = blas::ungqr(m, k + 5, tau.dim(0), Y.data(), k + 5, const_cast&lt;const T *&gt;(tau.data()));</span></a>
<a name="1158"><span class="lineNum">    1158 </span>            :     }</a>
<a name="1159"><span class="lineNum">    1159 </span>            : </a>
<a name="1160"><span class="lineNum">    1160 </span>            :     // Cast the matrix A into a smaller rank (B)</a>
<a name="1161"><span class="lineNum">    1161 </span><span class="lineCov">          6 :     Tensor&lt;T, 2&gt; B(&quot;B&quot;, k + 5, n);</span></a>
<a name="1162"><span class="lineNum">    1162 </span><span class="lineCov">          4 :     gemm&lt;true, false&gt;(T{1.0}, Y, _A, T{0.0}, &amp;B);</span></a>
<a name="1163"><span class="lineNum">    1163 </span>            : </a>
<a name="1164"><span class="lineNum">    1164 </span>            :     // Perform svd on B</a>
<a name="1165"><span class="lineNum">    1165 </span><span class="lineCov">          4 :     auto [Utilde, S, Vt] = svd_dd(B);</span></a>
<a name="1166"><span class="lineNum">    1166 </span>            : </a>
<a name="1167"><span class="lineNum">    1167 </span>            :     // Cast U back into full basis</a>
<a name="1168"><span class="lineNum">    1168 </span><span class="lineCov">          6 :     Tensor&lt;T, 2&gt; U(&quot;U&quot;, m, k + 5);</span></a>
<a name="1169"><span class="lineNum">    1169 </span><span class="lineCov">          4 :     gemm&lt;false, false&gt;(T{1.0}, Y, Utilde, T{0.0}, &amp;U);</span></a>
<a name="1170"><span class="lineNum">    1170 </span>            : </a>
<a name="1171"><span class="lineNum">    1171 </span><span class="lineCov">          4 :     return std::make_tuple(U, S, Vt);</span></a>
<a name="1172"><span class="lineNum">    1172 </span><span class="lineCov">          4 : }</span></a>
<a name="1173"><span class="lineNum">    1173 </span>            : </a>
<a name="1174"><span class="lineNum">    1174 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, typename T&gt;</a>
<a name="1175"><span class="lineNum">    1175 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="1176"><span class="lineNum">    1176 </span>            : auto truncated_syev(const AType&lt;T, ARank&gt; &amp;A, size_t k) -&gt; std::tuple&lt;Tensor&lt;T, 2&gt;, Tensor&lt;T, 1&gt;&gt; {</a>
<a name="1177"><span class="lineNum">    1177 </span>            :     LabeledSection0();</a>
<a name="1178"><span class="lineNum">    1178 </span>            : </a>
<a name="1179"><span class="lineNum">    1179 </span>            :     if (A.dim(0) != A.dim(1)) {</a>
<a name="1180"><span class="lineNum">    1180 </span>            :         println_abort(&quot;Non-square matrix used as input of truncated_syev!&quot;);</a>
<a name="1181"><span class="lineNum">    1181 </span>            :     }</a>
<a name="1182"><span class="lineNum">    1182 </span>            : </a>
<a name="1183"><span class="lineNum">    1183 </span>            :     size_t n = A.dim(0);</a>
<a name="1184"><span class="lineNum">    1184 </span>            : </a>
<a name="1185"><span class="lineNum">    1185 </span>            :     // Omega Test Matrix</a>
<a name="1186"><span class="lineNum">    1186 </span>            :     Tensor&lt;double, 2&gt; omega = create_random_tensor(&quot;omega&quot;, n, k + 5);</a>
<a name="1187"><span class="lineNum">    1187 </span>            : </a>
<a name="1188"><span class="lineNum">    1188 </span>            :     // Matrix Y = A * Omega</a>
<a name="1189"><span class="lineNum">    1189 </span>            :     Tensor&lt;double, 2&gt; Y(&quot;Y&quot;, n, k + 5);</a>
<a name="1190"><span class="lineNum">    1190 </span>            :     gemm&lt;false, false&gt;(1.0, A, omega, 0.0, &amp;Y);</a>
<a name="1191"><span class="lineNum">    1191 </span>            : </a>
<a name="1192"><span class="lineNum">    1192 </span>            :     Tensor&lt;double, 1&gt; tau(&quot;tau&quot;, std::min(n, k + 5));</a>
<a name="1193"><span class="lineNum">    1193 </span>            :     // Compute QR factorization of Y</a>
<a name="1194"><span class="lineNum">    1194 </span>            :     blas_int const info1 = blas::geqrf(n, k + 5, Y.data(), k + 5, tau.data());</a>
<a name="1195"><span class="lineNum">    1195 </span>            :     // Extract Matrix Q out of QR factorization</a>
<a name="1196"><span class="lineNum">    1196 </span>            :     blas_int const info2 = blas::orgqr(n, k + 5, tau.dim(0), Y.data(), k + 5, const_cast&lt;const double *&gt;(tau.data()));</a>
<a name="1197"><span class="lineNum">    1197 </span>            : </a>
<a name="1198"><span class="lineNum">    1198 </span>            :     Tensor&lt;double, 2&gt; &amp;Q1 = Y;</a>
<a name="1199"><span class="lineNum">    1199 </span>            : </a>
<a name="1200"><span class="lineNum">    1200 </span>            :     // Cast the matrix A into a smaller rank (B)</a>
<a name="1201"><span class="lineNum">    1201 </span>            :     // B = Q^T * A * Q</a>
<a name="1202"><span class="lineNum">    1202 </span>            :     Tensor&lt;double, 2&gt; Btemp(&quot;Btemp&quot;, k + 5, n);</a>
<a name="1203"><span class="lineNum">    1203 </span>            :     gemm&lt;true, false&gt;(1.0, Q1, A, 0.0, &amp;Btemp);</a>
<a name="1204"><span class="lineNum">    1204 </span>            :     Tensor&lt;double, 2&gt; B(&quot;B&quot;, k + 5, k + 5);</a>
<a name="1205"><span class="lineNum">    1205 </span>            :     gemm&lt;false, false&gt;(1.0, Btemp, Q1, 0.0, &amp;B);</a>
<a name="1206"><span class="lineNum">    1206 </span>            : </a>
<a name="1207"><span class="lineNum">    1207 </span>            :     // Create buffer for eigenvalues</a>
<a name="1208"><span class="lineNum">    1208 </span>            :     Tensor&lt;double, 1&gt; w(&quot;eigenvalues&quot;, k + 5);</a>
<a name="1209"><span class="lineNum">    1209 </span>            : </a>
<a name="1210"><span class="lineNum">    1210 </span>            :     // Diagonalize B</a>
<a name="1211"><span class="lineNum">    1211 </span>            :     syev(&amp;B, &amp;w);</a>
<a name="1212"><span class="lineNum">    1212 </span>            : </a>
<a name="1213"><span class="lineNum">    1213 </span>            :     // Cast U back into full basis (B is column-major so we need to transpose it)</a>
<a name="1214"><span class="lineNum">    1214 </span>            :     Tensor&lt;double, 2&gt; U(&quot;U&quot;, n, k + 5);</a>
<a name="1215"><span class="lineNum">    1215 </span>            :     gemm&lt;false, true&gt;(1.0, Q1, B, 0.0, &amp;U);</a>
<a name="1216"><span class="lineNum">    1216 </span>            : </a>
<a name="1217"><span class="lineNum">    1217 </span>            :     return std::make_tuple(U, w);</a>
<a name="1218"><span class="lineNum">    1218 </span>            : }</a>
<a name="1219"><span class="lineNum">    1219 </span>            : </a>
<a name="1220"><span class="lineNum">    1220 </span>            : template &lt;typename T&gt;</a>
<a name="1221"><span class="lineNum">    1221 </span>            : inline auto pseudoinverse(const Tensor&lt;T, 2&gt; &amp;A, double tol) -&gt; Tensor&lt;T, 2&gt; {</a>
<a name="1222"><span class="lineNum">    1222 </span>            :     LabeledSection0();</a>
<a name="1223"><span class="lineNum">    1223 </span>            : </a>
<a name="1224"><span class="lineNum">    1224 </span>            :     auto [U, S, Vh] = svd_a(A);</a>
<a name="1225"><span class="lineNum">    1225 </span>            : </a>
<a name="1226"><span class="lineNum">    1226 </span>            :     size_t new_dim;</a>
<a name="1227"><span class="lineNum">    1227 </span>            :     for (size_t v = 0; v &lt; S.dim(0); v++) {</a>
<a name="1228"><span class="lineNum">    1228 </span>            :         T val = S(v);</a>
<a name="1229"><span class="lineNum">    1229 </span>            :         if (val &gt; tol)</a>
<a name="1230"><span class="lineNum">    1230 </span>            :             scale_column(v, 1.0 / val, &amp;U);</a>
<a name="1231"><span class="lineNum">    1231 </span>            :         else {</a>
<a name="1232"><span class="lineNum">    1232 </span>            :             new_dim = v;</a>
<a name="1233"><span class="lineNum">    1233 </span>            :             break;</a>
<a name="1234"><span class="lineNum">    1234 </span>            :         }</a>
<a name="1235"><span class="lineNum">    1235 </span>            :     }</a>
<a name="1236"><span class="lineNum">    1236 </span>            : </a>
<a name="1237"><span class="lineNum">    1237 </span>            :     TensorView&lt;T, 2&gt; U_view = U(All, Range{0, new_dim});</a>
<a name="1238"><span class="lineNum">    1238 </span>            :     TensorView&lt;T, 2&gt; V_view = Vh(Range{0, new_dim}, All);</a>
<a name="1239"><span class="lineNum">    1239 </span>            : </a>
<a name="1240"><span class="lineNum">    1240 </span>            :     Tensor&lt;T, 2&gt; pinv(&quot;pinv&quot;, A.dim(0), A.dim(1));</a>
<a name="1241"><span class="lineNum">    1241 </span>            :     gemm&lt;false, false&gt;(1.0, U_view, V_view, 0.0, &amp;pinv);</a>
<a name="1242"><span class="lineNum">    1242 </span>            : </a>
<a name="1243"><span class="lineNum">    1243 </span>            :     return pinv;</a>
<a name="1244"><span class="lineNum">    1244 </span>            : }</a>
<a name="1245"><span class="lineNum">    1245 </span>            : </a>
<a name="1246"><span class="lineNum">    1246 </span>            : template &lt;typename T&gt;</a>
<a name="1247"><span class="lineNum">    1247 </span><span class="lineCov">          1 : inline auto solve_continuous_lyapunov(const Tensor&lt;T, 2&gt; &amp;A, const Tensor&lt;T, 2&gt; &amp;Q) -&gt; Tensor&lt;T, 2&gt; {</span></a>
<a name="1248"><span class="lineNum">    1248 </span><span class="lineCov">          3 :     LabeledSection0();</span></a>
<a name="1249"><span class="lineNum">    1249 </span>            : </a>
<a name="1250"><span class="lineNum">    1250 </span><span class="lineCov">          1 :     if (A.dim(0) != A.dim(1)) {</span></a>
<a name="1251"><span class="lineNum">    1251 </span><span class="lineNoCov">          0 :         println_abort(&quot;solve_continuous_lyapunov: Dimensions of A ({} x {}), do not match&quot;, A.dim(0), A.dim(1));</span></a>
<a name="1252"><span class="lineNum">    1252 </span>            :     }</a>
<a name="1253"><span class="lineNum">    1253 </span><span class="lineCov">          1 :     if (Q.dim(0) != Q.dim(1)) {</span></a>
<a name="1254"><span class="lineNum">    1254 </span><span class="lineNoCov">          0 :         println_abort(&quot;solve_continuous_lyapunov: Dimensions of Q ({} x {}), do not match&quot;, Q.dim(0), Q.dim(1));</span></a>
<a name="1255"><span class="lineNum">    1255 </span>            :     }</a>
<a name="1256"><span class="lineNum">    1256 </span><span class="lineCov">          1 :     if (A.dim(0) != Q.dim(0)) {</span></a>
<a name="1257"><span class="lineNum">    1257 </span><span class="lineNoCov">          0 :         println_abort(&quot;solve_continuous_lyapunov: Dimensions of A ({} x {}) and Q ({} x {}), do not match&quot;, A.dim(0), A.dim(1), Q.dim(0),</span></a>
<a name="1258"><span class="lineNum">    1258 </span>            :                       Q.dim(1));</a>
<a name="1259"><span class="lineNum">    1259 </span>            :     }</a>
<a name="1260"><span class="lineNum">    1260 </span>            : </a>
<a name="1261"><span class="lineNum">    1261 </span><span class="lineCov">          1 :     size_t n = A.dim(0);</span></a>
<a name="1262"><span class="lineNum">    1262 </span>            : </a>
<a name="1263"><span class="lineNum">    1263 </span>            :     /// TODO: Break this off into a separate schur function</a>
<a name="1264"><span class="lineNum">    1264 </span>            :     // Compute Schur Decomposition of A</a>
<a name="1265"><span class="lineNum">    1265 </span><span class="lineCov">          1 :     Tensor&lt;T, 2&gt;          R = A; // R is a copy of A</span></a>
<a name="1266"><span class="lineNum">    1266 </span><span class="lineCov">          2 :     Tensor&lt;T, 2&gt;          wr(&quot;Schur Real Buffer&quot;, n, n);</span></a>
<a name="1267"><span class="lineNum">    1267 </span><span class="lineCov">          2 :     Tensor&lt;T, 2&gt;          wi(&quot;Schur Imaginary Buffer&quot;, n, n);</span></a>
<a name="1268"><span class="lineNum">    1268 </span><span class="lineCov">          2 :     Tensor&lt;T, 2&gt;          U(&quot;Lyapunov U&quot;, n, n);</span></a>
<a name="1269"><span class="lineNum">    1269 </span><span class="lineCov">          1 :     std::vector&lt;blas_int&gt; sdim(1);</span></a>
<a name="1270"><span class="lineNum">    1270 </span><span class="lineCov">          1 :     blas::gees('V', n, R.data(), n, sdim.data(), wr.data(), wi.data(), U.data(), n);</span></a>
<a name="1271"><span class="lineNum">    1271 </span>            : </a>
<a name="1272"><span class="lineNum">    1272 </span>            :     // Compute F = U^T * Q * U</a>
<a name="1273"><span class="lineNum">    1273 </span><span class="lineCov">          1 :     Tensor&lt;T, 2&gt; Fbuff = gemm&lt;true, false&gt;(1.0, U, Q);</span></a>
<a name="1274"><span class="lineNum">    1274 </span><span class="lineCov">          1 :     Tensor&lt;T, 2&gt; F     = gemm&lt;false, false&gt;(1.0, Fbuff, U);</span></a>
<a name="1275"><span class="lineNum">    1275 </span>            : </a>
<a name="1276"><span class="lineNum">    1276 </span>            :     // Call the Sylvester Solve</a>
<a name="1277"><span class="lineNum">    1277 </span><span class="lineCov">          1 :     std::vector&lt;T&gt; scale(1);</span></a>
<a name="1278"><span class="lineNum">    1278 </span><span class="lineCov">          1 :     blas::trsyl('N', 'N', 1, n, n, const_cast&lt;const T *&gt;(R.data()), n, const_cast&lt;const T *&gt;(R.data()), n, F.data(), n, scale.data());</span></a>
<a name="1279"><span class="lineNum">    1279 </span>            : </a>
<a name="1280"><span class="lineNum">    1280 </span><span class="lineCov">          1 :     Tensor&lt;T, 2&gt; Xbuff = gemm&lt;false, false&gt;(scale[0], U, F);</span></a>
<a name="1281"><span class="lineNum">    1281 </span><span class="lineCov">          1 :     Tensor&lt;T, 2&gt; X     = gemm&lt;false, true&gt;(1.0, Xbuff, U);</span></a>
<a name="1282"><span class="lineNum">    1282 </span>            : </a>
<a name="1283"><span class="lineNum">    1283 </span><span class="lineCov">          1 :     return X;</span></a>
<a name="1284"><span class="lineNum">    1284 </span><span class="lineCov">          1 : }</span></a>
<a name="1285"><span class="lineNum">    1285 </span>            : </a>
<a name="1286"><span class="lineNum">    1286 </span><span class="lineCov">          1 : ALIAS_TEMPLATE_FUNCTION(solve_lyapunov, solve_continuous_lyapunov)</span></a>
<a name="1287"><span class="lineNum">    1287 </span>            : </a>
<a name="1288"><span class="lineNum">    1288 </span>            : template &lt;template &lt;typename, size_t&gt; typename AType, size_t ARank, typename T&gt;</a>
<a name="1289"><span class="lineNum">    1289 </span>            :     requires CoreRankTensor&lt;AType&lt;T, ARank&gt;, 2, T&gt;</a>
<a name="1290"><span class="lineNum">    1290 </span>            : auto qr(const AType&lt;T, ARank&gt; &amp;_A) -&gt; std::tuple&lt;Tensor&lt;T, 2&gt;, Tensor&lt;T, 1&gt;&gt; {</a>
<a name="1291"><span class="lineNum">    1291 </span>            :     LabeledSection0();</a>
<a name="1292"><span class="lineNum">    1292 </span>            : </a>
<a name="1293"><span class="lineNum">    1293 </span>            :     // Copy A because it will be overwritten by the QR call.</a>
<a name="1294"><span class="lineNum">    1294 </span>            :     Tensor&lt;T, 2&gt;   A = _A;</a>
<a name="1295"><span class="lineNum">    1295 </span>            :     const blas_int m = A.dim(0);</a>
<a name="1296"><span class="lineNum">    1296 </span>            :     const blas_int n = A.dim(1);</a>
<a name="1297"><span class="lineNum">    1297 </span>            : </a>
<a name="1298"><span class="lineNum">    1298 </span>            :     Tensor&lt;double, 1&gt; tau(&quot;tau&quot;, std::min(m, n));</a>
<a name="1299"><span class="lineNum">    1299 </span>            :     // Compute QR factorization of Y</a>
<a name="1300"><span class="lineNum">    1300 </span>            :     blas_int info = blas::geqrf(m, n, A.data(), n, tau.data());</a>
<a name="1301"><span class="lineNum">    1301 </span>            : </a>
<a name="1302"><span class="lineNum">    1302 </span>            :     if (info != 0) {</a>
<a name="1303"><span class="lineNum">    1303 </span>            :         println_abort(&quot;{} parameter to geqrf has an illegal value.&quot;, -info);</a>
<a name="1304"><span class="lineNum">    1304 </span>            :     }</a>
<a name="1305"><span class="lineNum">    1305 </span>            : </a>
<a name="1306"><span class="lineNum">    1306 </span>            :     // Extract Matrix Q out of QR factorization</a>
<a name="1307"><span class="lineNum">    1307 </span>            :     // blas_int info2 = blas::orgqr(m, n, tau.dim(0), A.data(), n, const_cast&lt;const double *&gt;(tau.data()));</a>
<a name="1308"><span class="lineNum">    1308 </span>            :     return {A, tau};</a>
<a name="1309"><span class="lineNum">    1309 </span>            : }</a>
<a name="1310"><span class="lineNum">    1310 </span>            : </a>
<a name="1311"><span class="lineNum">    1311 </span>            : template &lt;typename T&gt;</a>
<a name="1312"><span class="lineNum">    1312 </span>            : auto q(const Tensor&lt;T, 2&gt; &amp;qr, const Tensor&lt;T, 1&gt; &amp;tau) -&gt; Tensor&lt;T, 2&gt; {</a>
<a name="1313"><span class="lineNum">    1313 </span>            :     const blas_int m = qr.dim(1);</a>
<a name="1314"><span class="lineNum">    1314 </span>            :     const blas_int p = qr.dim(0);</a>
<a name="1315"><span class="lineNum">    1315 </span>            : </a>
<a name="1316"><span class="lineNum">    1316 </span>            :     Tensor&lt;T, 2&gt; Q = qr;</a>
<a name="1317"><span class="lineNum">    1317 </span>            : </a>
<a name="1318"><span class="lineNum">    1318 </span>            :     blas_int info = blas::orgqr(m, m, p, Q.data(), m, tau.data());</a>
<a name="1319"><span class="lineNum">    1319 </span>            :     if (info != 0) {</a>
<a name="1320"><span class="lineNum">    1320 </span>            :         println_abort(&quot;{} parameter to orgqr has an illegal value. {} {} {}&quot;, -info, m, m, p);</a>
<a name="1321"><span class="lineNum">    1321 </span>            :     }</a>
<a name="1322"><span class="lineNum">    1322 </span>            : </a>
<a name="1323"><span class="lineNum">    1323 </span>            :     return Q;</a>
<a name="1324"><span class="lineNum">    1324 </span>            : }</a>
<a name="1325"><span class="lineNum">    1325 </span>            : </a>
<a name="1326"><span class="lineNum">    1326 </span>            : END_EINSUMS_NAMESPACE_HPP(einsums::linear_algebra)</a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="https://github.com/linux-test-project/lcov" target="_parent">LCOV version 1.16</a></td></tr>
  </table>
  <br>

</body>
</html>
