<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - coverage.info - src/include/einsums/Decomposition.hpp</title>
  <link rel="stylesheet" type="text/css" href="../../../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../../../index.html">top level</a> - <a href="index.html">src/include/einsums</a> - Decomposition.hpp<span style="font-size: 80%;"> (source / <a href="Decomposition.hpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">coverage.info</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">212</td>
            <td class="headerCovTableEntry">215</td>
            <td class="headerCovTableEntryHi">98.6 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2024-06-05 18:10:07</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">125</td>
            <td class="headerCovTableEntry">125</td>
            <td class="headerCovTableEntryHi">100.0 %</td>
          </tr>
          <tr><td><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : //----------------------------------------------------------------------------------------------</a>
<a name="2"><span class="lineNum">       2 </span>            : // Copyright (c) The Einsums Developers. All rights reserved.</a>
<a name="3"><span class="lineNum">       3 </span>            : // Licensed under the MIT License. See LICENSE.txt in the project root for license information.</a>
<a name="4"><span class="lineNum">       4 </span>            : //----------------------------------------------------------------------------------------------</a>
<a name="5"><span class="lineNum">       5 </span>            : </a>
<a name="6"><span class="lineNum">       6 </span>            : #pragma once</a>
<a name="7"><span class="lineNum">       7 </span>            : </a>
<a name="8"><span class="lineNum">       8 </span>            : #include &quot;einsums/_Common.hpp&quot;</a>
<a name="9"><span class="lineNum">       9 </span>            : </a>
<a name="10"><span class="lineNum">      10 </span>            : #include &quot;einsums/LinearAlgebra.hpp&quot;</a>
<a name="11"><span class="lineNum">      11 </span>            : #include &quot;einsums/Section.hpp&quot;</a>
<a name="12"><span class="lineNum">      12 </span>            : #include &quot;einsums/Sort.hpp&quot;</a>
<a name="13"><span class="lineNum">      13 </span>            : #include &quot;einsums/Tensor.hpp&quot;</a>
<a name="14"><span class="lineNum">      14 </span>            : #include &quot;einsums/TensorAlgebra.hpp&quot;</a>
<a name="15"><span class="lineNum">      15 </span>            : #include &quot;einsums/Utilities.hpp&quot;</a>
<a name="16"><span class="lineNum">      16 </span>            : </a>
<a name="17"><span class="lineNum">      17 </span>            : #include &lt;cmath&gt;</a>
<a name="18"><span class="lineNum">      18 </span>            : #include &lt;functional&gt;</a>
<a name="19"><span class="lineNum">      19 </span>            : </a>
<a name="20"><span class="lineNum">      20 </span>            : BEGIN_EINSUMS_NAMESPACE_HPP(einsums::decomposition)</a>
<a name="21"><span class="lineNum">      21 </span>            : </a>
<a name="22"><span class="lineNum">      22 </span>            : template &lt;size_t TRank&gt;</a>
<a name="23"><span class="lineNum">      23 </span>            : auto validate_cp_rank(const Dim&lt;TRank&gt; shape, const std::string &amp;rounding = &quot;round&quot;) -&gt; size_t {</a>
<a name="24"><span class="lineNum">      24 </span>            :     LabeledSection1(fmt::format(&quot;&lt;TRank={}&gt;&quot;, TRank));</a>
<a name="25"><span class="lineNum">      25 </span>            : </a>
<a name="26"><span class="lineNum">      26 </span>            :     using rounding_func_t = double (*)(double);</a>
<a name="27"><span class="lineNum">      27 </span>            :     rounding_func_t rounding_func;</a>
<a name="28"><span class="lineNum">      28 </span>            : </a>
<a name="29"><span class="lineNum">      29 </span>            :     if (rounding == &quot;ceil&quot;) {</a>
<a name="30"><span class="lineNum">      30 </span>            :         rounding_func = ::ceil;</a>
<a name="31"><span class="lineNum">      31 </span>            :     } else if (rounding == &quot;floor&quot;) {</a>
<a name="32"><span class="lineNum">      32 </span>            :         rounding_func = ::floor;</a>
<a name="33"><span class="lineNum">      33 </span>            :     } else if (rounding == &quot;round&quot;) {</a>
<a name="34"><span class="lineNum">      34 </span>            :         rounding_func = ::round;</a>
<a name="35"><span class="lineNum">      35 </span>            :     } else {</a>
<a name="36"><span class="lineNum">      36 </span>            :         throw std::runtime_error(fmt::format(&quot;Rounding should of round, floow, or ceil, but got {}&quot;, rounding));</a>
<a name="37"><span class="lineNum">      37 </span>            :     }</a>
<a name="38"><span class="lineNum">      38 </span>            : </a>
<a name="39"><span class="lineNum">      39 </span>            :     double prod = std::accumulate(shape.begin(), shape.end(), 1, std::multiplies&lt;&gt;());</a>
<a name="40"><span class="lineNum">      40 </span>            :     double sum  = std::accumulate(shape.begin(), shape.end(), 0, std::plus&lt;&gt;());</a>
<a name="41"><span class="lineNum">      41 </span>            : </a>
<a name="42"><span class="lineNum">      42 </span>            :     return static_cast&lt;int&gt;(rounding_func(prod / sum));</a>
<a name="43"><span class="lineNum">      43 </span>            : }</a>
<a name="44"><span class="lineNum">      44 </span>            : </a>
<a name="45"><span class="lineNum">      45 </span>            : /**</a>
<a name="46"><span class="lineNum">      46 </span>            :  * Computes the 2-norm of a tensor</a>
<a name="47"><span class="lineNum">      47 </span>            :  */</a>
<a name="48"><span class="lineNum">      48 </span>            : template &lt;template &lt;typename, size_t&gt; typename TTensor, size_t TRank, typename TType = double&gt;</a>
<a name="49"><span class="lineNum">      49 </span><span class="lineCov">          4 : auto norm(const TTensor&lt;TType, TRank&gt; &amp;tensor) -&gt; TType {</span></a>
<a name="50"><span class="lineNum">      50 </span><span class="lineCov">          8 :     LabeledSection0();</span></a>
<a name="51"><span class="lineNum">      51 </span>            : </a>
<a name="52"><span class="lineNum">      52 </span><span class="lineCov">          4 :     TType val         = 0.0;</span></a>
<a name="53"><span class="lineNum">      53 </span><span class="lineCov">          4 :     auto  target_dims = get_dim_ranges&lt;TRank&gt;(tensor);</span></a>
<a name="54"><span class="lineNum">      54 </span>            : </a>
<a name="55"><span class="lineNum">      55 </span><span class="lineCov">        148 :     for (auto target_combination : std::apply(ranges::views::cartesian_product, target_dims)) {</span></a>
<a name="56"><span class="lineNum">      56 </span><span class="lineCov">        111 :         TType target = std::apply(tensor, target_combination);</span></a>
<a name="57"><span class="lineNum">      57 </span><span class="lineCov">        111 :         val += target * target;</span></a>
<a name="58"><span class="lineNum">      58 </span>            :     }</a>
<a name="59"><span class="lineNum">      59 </span>            : </a>
<a name="60"><span class="lineNum">      60 </span><span class="lineCov">          4 :     return std::sqrt(val);</span></a>
<a name="61"><span class="lineNum">      61 </span><span class="lineCov">          4 : }</span></a>
<a name="62"><span class="lineNum">      62 </span>            : </a>
<a name="63"><span class="lineNum">      63 </span>            : /**</a>
<a name="64"><span class="lineNum">      64 </span>            :  * Computes the RMSD between two tensors of arbitrary dimension</a>
<a name="65"><span class="lineNum">      65 </span>            :  */</a>
<a name="66"><span class="lineNum">      66 </span>            : template &lt;template &lt;typename, size_t&gt; typename TTensor, size_t TRank, typename TType = double&gt;</a>
<a name="67"><span class="lineNum">      67 </span><span class="lineCov">        177 : auto rmsd(const TTensor&lt;TType, TRank&gt; &amp;tensor1, const TTensor&lt;TType, TRank&gt; &amp;tensor2) -&gt; TType {</span></a>
<a name="68"><span class="lineNum">      68 </span><span class="lineCov">        354 :     LabeledSection0();</span></a>
<a name="69"><span class="lineNum">      69 </span>            : </a>
<a name="70"><span class="lineNum">      70 </span><span class="lineCov">        177 :     TType diff = 0.0;</span></a>
<a name="71"><span class="lineNum">      71 </span>            : </a>
<a name="72"><span class="lineNum">      72 </span><span class="lineCov">        177 :     size_t nelem = 1;</span></a>
<a name="73"><span class="lineNum">      73 </span><span class="lineCov">        354 :     for_sequence&lt;TRank&gt;([&amp;](auto i) { nelem *= tensor1.dim(i); });</span></a>
<a name="74"><span class="lineNum">      74 </span>            : </a>
<a name="75"><span class="lineNum">      75 </span><span class="lineCov">        177 :     auto target_dims = get_dim_ranges&lt;TRank&gt;(tensor1);</span></a>
<a name="76"><span class="lineNum">      76 </span><span class="lineCov">        177 :     auto view        = std::apply(ranges::views::cartesian_product, target_dims);</span></a>
<a name="77"><span class="lineNum">      77 </span>            : </a>
<a name="78"><span class="lineNum">      78 </span><span class="lineCov">        177 : #pragma omp parallel for reduction(+ : diff)</span></a>
<a name="79"><span class="lineNum">      79 </span>            :     for (auto it = view.begin(); it &lt; view.end(); it++) {</a>
<a name="80"><span class="lineNum">      80 </span>            :         auto  target_combination = *it;</a>
<a name="81"><span class="lineNum">      81 </span>            :         TType target1            = std::apply(tensor1, target_combination);</a>
<a name="82"><span class="lineNum">      82 </span>            :         TType target2            = std::apply(tensor2, target_combination);</a>
<a name="83"><span class="lineNum">      83 </span>            :         diff += (target1 - target2) * (target1 - target2);</a>
<a name="84"><span class="lineNum">      84 </span>            :     }</a>
<a name="85"><span class="lineNum">      85 </span>            : </a>
<a name="86"><span class="lineNum">      86 </span><span class="lineCov">        177 :     return std::sqrt(diff / nelem);</span></a>
<a name="87"><span class="lineNum">      87 </span><span class="lineCov">        177 : }</span></a>
<a name="88"><span class="lineNum">      88 </span>            : </a>
<a name="89"><span class="lineNum">      89 </span>            : /**</a>
<a name="90"><span class="lineNum">      90 </span>            :  * &quot;Weight&quot; a tensor for weighted CANDECOMP/PARAFAC decompositions (returns a copy) by input weights</a>
<a name="91"><span class="lineNum">      91 </span>            :  */</a>
<a name="92"><span class="lineNum">      92 </span>            : template &lt;template &lt;typename, size_t&gt; typename TTensor, size_t TRank, typename TType = double&gt;</a>
<a name="93"><span class="lineNum">      93 </span>            : auto weight_tensor(const TTensor&lt;TType, TRank&gt; &amp;tensor, const TTensor&lt;TType, 1&gt; &amp;weights) -&gt; Tensor&lt;TType, TRank&gt; {</a>
<a name="94"><span class="lineNum">      94 </span>            :     LabeledSection0();</a>
<a name="95"><span class="lineNum">      95 </span>            : </a>
<a name="96"><span class="lineNum">      96 </span>            :     if (tensor.dim(0) != weights.dim(0)) {</a>
<a name="97"><span class="lineNum">      97 </span>            :         println_abort(&quot;The first dimension of the tensor and the dimension of the weight DO NOT match&quot;);</a>
<a name="98"><span class="lineNum">      98 </span>            :     }</a>
<a name="99"><span class="lineNum">      99 </span>            : </a>
<a name="100"><span class="lineNum">     100 </span>            :     Tensor&lt;TType, TRank&gt; weighted_tensor(tensor.dims());</a>
<a name="101"><span class="lineNum">     101 </span>            :     auto                 target_dims = get_dim_ranges&lt;TRank&gt;(tensor);</a>
<a name="102"><span class="lineNum">     102 </span>            : </a>
<a name="103"><span class="lineNum">     103 </span>            :     auto view = std::apply(ranges::views::cartesian_product, target_dims);</a>
<a name="104"><span class="lineNum">     104 </span>            : </a>
<a name="105"><span class="lineNum">     105 </span>            : #pragma omp parallel for</a>
<a name="106"><span class="lineNum">     106 </span>            :     for (auto it = view.begin(); it &lt; view.end(); it++) {</a>
<a name="107"><span class="lineNum">     107 </span>            :         auto         target_combination = *it;</a>
<a name="108"><span class="lineNum">     108 </span>            :         const TType &amp;source             = std::apply(tensor, target_combination);</a>
<a name="109"><span class="lineNum">     109 </span>            :         TType       &amp;target             = std::apply(weighted_tensor, target_combination);</a>
<a name="110"><span class="lineNum">     110 </span>            :         const TType &amp;scale              = weights(std::get&lt;0&gt;(target_combination));</a>
<a name="111"><span class="lineNum">     111 </span>            : </a>
<a name="112"><span class="lineNum">     112 </span>            :         target = scale * source;</a>
<a name="113"><span class="lineNum">     113 </span>            :     }</a>
<a name="114"><span class="lineNum">     114 </span>            : </a>
<a name="115"><span class="lineNum">     115 </span>            :     return weighted_tensor;</a>
<a name="116"><span class="lineNum">     116 </span>            : }</a>
<a name="117"><span class="lineNum">     117 </span>            : </a>
<a name="118"><span class="lineNum">     118 </span>            : /**</a>
<a name="119"><span class="lineNum">     119 </span>            :  * Reconstructs a tensor given a CANDECOMP/PARAFAC decomposition</a>
<a name="120"><span class="lineNum">     120 </span>            :  *</a>
<a name="121"><span class="lineNum">     121 </span>            :  *   factors = The decomposed CANDECOMP matrices (dimension: [dim[i], rank])</a>
<a name="122"><span class="lineNum">     122 </span>            :  */</a>
<a name="123"><span class="lineNum">     123 </span>            : template &lt;size_t TRank, typename TType&gt;</a>
<a name="124"><span class="lineNum">     124 </span><span class="lineCov">         32 : auto parafac_reconstruct(const std::vector&lt;Tensor&lt;TType, 2&gt;&gt; &amp;factors) -&gt; Tensor&lt;TType, TRank&gt; {</span></a>
<a name="125"><span class="lineNum">     125 </span><span class="lineCov">         64 :     LabeledSection0();</span></a>
<a name="126"><span class="lineNum">     126 </span>            : </a>
<a name="127"><span class="lineNum">     127 </span><span class="lineCov">         32 :     size_t     rank = 0;</span></a>
<a name="128"><span class="lineNum">     128 </span><span class="lineCov">         32 :     Dim&lt;TRank&gt; dims;</span></a>
<a name="129"><span class="lineNum">     129 </span>            : </a>
<a name="130"><span class="lineNum">     130 </span><span class="lineCov">         32 :     size_t i = 0;</span></a>
<a name="131"><span class="lineNum">     131 </span><span class="lineCov">        137 :     for (const auto &amp;factor : factors) {</span></a>
<a name="132"><span class="lineNum">     132 </span><span class="lineCov">        105 :         dims[i] = factor.dim(0);</span></a>
<a name="133"><span class="lineNum">     133 </span><span class="lineCov">        105 :         if (!rank)</span></a>
<a name="134"><span class="lineNum">     134 </span><span class="lineCov">         32 :             rank = factor.dim(1);</span></a>
<a name="135"><span class="lineNum">     135 </span><span class="lineCov">        105 :         i++;</span></a>
<a name="136"><span class="lineNum">     136 </span>            :     }</a>
<a name="137"><span class="lineNum">     137 </span>            : </a>
<a name="138"><span class="lineNum">     138 </span><span class="lineCov">         32 :     Tensor&lt;TType, TRank&gt; new_tensor(dims);</span></a>
<a name="139"><span class="lineNum">     139 </span><span class="lineCov">         32 :     new_tensor.zero();</span></a>
<a name="140"><span class="lineNum">     140 </span>            : </a>
<a name="141"><span class="lineNum">     141 </span><span class="lineCov">         32 :     auto indices = get_dim_ranges&lt;TRank&gt;(new_tensor);</span></a>
<a name="142"><span class="lineNum">     142 </span><span class="lineCov">         32 :     auto view    = std::apply(ranges::views::cartesian_product, indices);</span></a>
<a name="143"><span class="lineNum">     143 </span>            : </a>
<a name="144"><span class="lineNum">     144 </span><span class="lineCov">         32 : #pragma omp parallel for</span></a>
<a name="145"><span class="lineNum">     145 </span>            :     for (auto it = view.begin(); it &lt; view.end(); it++) {</a>
<a name="146"><span class="lineNum">     146 </span>            :         auto   idx_combo = *it;</a>
<a name="147"><span class="lineNum">     147 </span>            :         TType &amp;target    = std::apply(new_tensor, idx_combo);</a>
<a name="148"><span class="lineNum">     148 </span>            :         for (size_t r = 0; r &lt; rank; r++) {</a>
<a name="149"><span class="lineNum">     149 </span>            :             double temp = 1.0;</a>
<a name="150"><span class="lineNum">     150 </span><span class="lineCov">      12456 :             for_sequence&lt;TRank&gt;([&amp;](auto n) { temp *= factors[n](std::get&lt;n&gt;(idx_combo), r); });</span></a>
<a name="151"><span class="lineNum">     151 </span>            :             target += temp;</a>
<a name="152"><span class="lineNum">     152 </span>            :         }</a>
<a name="153"><span class="lineNum">     153 </span>            :     }</a>
<a name="154"><span class="lineNum">     154 </span>            : </a>
<a name="155"><span class="lineNum">     155 </span><span class="lineCov">         32 :     return new_tensor;</span></a>
<a name="156"><span class="lineNum">     156 </span><span class="lineCov">         32 : }</span></a>
<a name="157"><span class="lineNum">     157 </span>            : </a>
<a name="158"><span class="lineNum">     158 </span>            : template &lt;size_t TRank, typename TType&gt;</a>
<a name="159"><span class="lineNum">     159 </span><span class="lineCov">          4 : auto initialize_cp(std::vector&lt;Tensor&lt;TType, 2&gt;&gt; &amp;folds, size_t rank) -&gt; std::vector&lt;Tensor&lt;TType, 2&gt;&gt; {</span></a>
<a name="160"><span class="lineNum">     160 </span><span class="lineCov">          8 :     LabeledSection0();</span></a>
<a name="161"><span class="lineNum">     161 </span>            : </a>
<a name="162"><span class="lineNum">     162 </span>            :     using namespace einsums::tensor_algebra;</a>
<a name="163"><span class="lineNum">     163 </span>            :     using namespace einsums::tensor_algebra::index;</a>
<a name="164"><span class="lineNum">     164 </span>            : </a>
<a name="165"><span class="lineNum">     165 </span><span class="lineCov">          4 :     std::vector&lt;Tensor&lt;TType, 2&gt;&gt; factors;</span></a>
<a name="166"><span class="lineNum">     166 </span>            : </a>
<a name="167"><span class="lineNum">     167 </span>            :     // Perform compile-time looping.</a>
<a name="168"><span class="lineNum">     168 </span><span class="lineCov">         36 :     for_sequence&lt;TRank&gt;([&amp;](auto i) {</span></a>
<a name="169"><span class="lineNum">     169 </span><span class="lineCov">         13 :         size_t m = folds[i].dim(0);</span></a>
<a name="170"><span class="lineNum">     170 </span>            : </a>
<a name="171"><span class="lineNum">     171 </span>            :         // Multiply the fold by its transpose</a>
<a name="172"><span class="lineNum">     172 </span><span class="lineCov">         26 :         Tensor&lt;double, 2&gt; fold_squared(&quot;fold squared&quot;, m, m);</span></a>
<a name="173"><span class="lineNum">     173 </span><span class="lineCov">         13 :         einsum(0.0, Indices{index::M, index::N}, &amp;fold_squared, 1.0, Indices{index::M, index::p}, folds[i], Indices{index::N, index::p},</span></a>
<a name="174"><span class="lineNum">     174 </span><span class="lineCov">         13 :                folds[i]);</span></a>
<a name="175"><span class="lineNum">     175 </span>            : </a>
<a name="176"><span class="lineNum">     176 </span><span class="lineCov">         13 :         Tensor&lt;double, 1&gt; S(&quot;eigenvalues&quot;, m);</span></a>
<a name="177"><span class="lineNum">     177 </span>            : </a>
<a name="178"><span class="lineNum">     178 </span>            :         // Diagonalize fold squared (akin to SVD)</a>
<a name="179"><span class="lineNum">     179 </span><span class="lineCov">         13 :         linear_algebra::syev(&amp;fold_squared, &amp;S);</span></a>
<a name="180"><span class="lineNum">     180 </span>            : </a>
<a name="181"><span class="lineNum">     181 </span>            :         // Reorder into row major form</a>
<a name="182"><span class="lineNum">     182 </span><span class="lineCov">         26 :         Tensor&lt;double, 2&gt; U(&quot;Left Singular Vectors&quot;, m, m);</span></a>
<a name="183"><span class="lineNum">     183 </span><span class="lineCov">         13 :         sort(Indices{index::M, index::N}, &amp;U, Indices{index::N, index::M}, fold_squared);</span></a>
<a name="184"><span class="lineNum">     184 </span>            : </a>
<a name="185"><span class="lineNum">     185 </span>            :         // If (i == 0), Scale U by the singular values</a>
<a name="186"><span class="lineNum">     186 </span>            :         if (i == 0) {</a>
<a name="187"><span class="lineNum">     187 </span><span class="lineCov">         16 :             for (size_t v = 0; v &lt; S.dim(0); v++) {</span></a>
<a name="188"><span class="lineNum">     188 </span><span class="lineCov">         12 :                 double const scaling_factor = std::sqrt(S(v));</span></a>
<a name="189"><span class="lineNum">     189 </span><span class="lineCov">         12 :                 if (std::abs(scaling_factor) &gt; 1.0e-14)</span></a>
<a name="190"><span class="lineNum">     190 </span><span class="lineCov">         12 :                     linear_algebra::scale_column(v, scaling_factor, &amp;U);</span></a>
<a name="191"><span class="lineNum">     191 </span>            :             }</a>
<a name="192"><span class="lineNum">     192 </span>            :         }</a>
<a name="193"><span class="lineNum">     193 </span>            : </a>
<a name="194"><span class="lineNum">     194 </span>            :         // println(&quot;After scaling&quot;);</a>
<a name="195"><span class="lineNum">     195 </span>            :         // println(U);</a>
<a name="196"><span class="lineNum">     196 </span>            : </a>
<a name="197"><span class="lineNum">     197 </span><span class="lineCov">         13 :         if (folds[i].dim(0) &lt; rank) {</span></a>
<a name="198"><span class="lineNum">     198 </span>            :             // println_warn(&quot;dimension {} size {} is less than the requested decomposition rank {}&quot;, i, folds[i].dim(0), rank);</a>
<a name="199"><span class="lineNum">     199 </span>            :             // TODO: Need to padd U up to rank</a>
<a name="200"><span class="lineNum">     200 </span><span class="lineCov">          3 :             Tensor&lt;TType, 2&gt; Unew  = create_random_tensor(&quot;Padded SVD Left Vectors&quot;, folds[i].dim(0), rank);</span></a>
<a name="201"><span class="lineNum">     201 </span><span class="lineCov">          3 :             Unew(All, Range{0, m}) = U(All, All);</span></a>
<a name="202"><span class="lineNum">     202 </span>            : </a>
<a name="203"><span class="lineNum">     203 </span>            :             // Need to save the factors</a>
<a name="204"><span class="lineNum">     204 </span><span class="lineCov">          3 :             factors.push_back(Unew);</span></a>
<a name="205"><span class="lineNum">     205 </span><span class="lineCov">          3 :         } else {</span></a>
<a name="206"><span class="lineNum">     206 </span>            :             // Need to save the factors</a>
<a name="207"><span class="lineNum">     207 </span><span class="lineCov">         10 :             factors.emplace_back(Tensor{U(All, Range{m - rank, m})});</span></a>
<a name="208"><span class="lineNum">     208 </span>            :         }</a>
<a name="209"><span class="lineNum">     209 </span>            : </a>
<a name="210"><span class="lineNum">     210 </span>            :         // println(&quot;latest factor added&quot;);</a>
<a name="211"><span class="lineNum">     211 </span>            :         // println(factors[factors.size() - 1]);</a>
<a name="212"><span class="lineNum">     212 </span>            :         // Tensor&lt;TType, 2&gt; Unew = create_random_tensor(&quot;Padded SVD Left Vectors&quot;, folds[i].dim(0), rank);</a>
<a name="213"><span class="lineNum">     213 </span>            :         // factors.emplace_back(Unew);</a>
<a name="214"><span class="lineNum">     214 </span><span class="lineCov">         13 :     });</span></a>
<a name="215"><span class="lineNum">     215 </span>            : </a>
<a name="216"><span class="lineNum">     216 </span><span class="lineCov">          4 :     return factors;</span></a>
<a name="217"><span class="lineNum">     217 </span><span class="lineCov">          4 : }</span></a>
<a name="218"><span class="lineNum">     218 </span>            : </a>
<a name="219"><span class="lineNum">     219 </span>            : /**</a>
<a name="220"><span class="lineNum">     220 </span>            :  * CANDECOMP/PARAFAC decomposition via alternating least squares (ALS).</a>
<a name="221"><span class="lineNum">     221 </span>            :  * Computes a rank-`rank` decomposition of `tensor` such that:</a>
<a name="222"><span class="lineNum">     222 </span>            :  *</a>
<a name="223"><span class="lineNum">     223 </span>            :  *   tensor = [|weights; factor[0], ..., factors[-1] |].</a>
<a name="224"><span class="lineNum">     224 </span>            :  */</a>
<a name="225"><span class="lineNum">     225 </span>            : template &lt;template &lt;typename, size_t&gt; typename TTensor, size_t TRank, typename TType = double&gt;</a>
<a name="226"><span class="lineNum">     226 </span><span class="lineCov">          4 : auto parafac(const TTensor&lt;TType, TRank&gt; &amp;tensor, size_t rank, int n_iter_max = 100,</span></a>
<a name="227"><span class="lineNum">     227 </span>            :              double tolerance = 1.e-8) -&gt; std::vector&lt;Tensor&lt;TType, 2&gt;&gt; {</a>
<a name="228"><span class="lineNum">     228 </span><span class="lineCov">          8 :     LabeledSection0();</span></a>
<a name="229"><span class="lineNum">     229 </span>            : </a>
<a name="230"><span class="lineNum">     230 </span>            :     using namespace einsums::tensor_algebra;</a>
<a name="231"><span class="lineNum">     231 </span>            :     using namespace einsums::tensor_algebra::index;</a>
<a name="232"><span class="lineNum">     232 </span>            :     using vector = std::vector&lt;TType, AlignedAllocator&lt;TType, 64&gt;&gt;;</a>
<a name="233"><span class="lineNum">     233 </span>            : </a>
<a name="234"><span class="lineNum">     234 </span>            :     // Compute set of unfolded matrices</a>
<a name="235"><span class="lineNum">     235 </span><span class="lineCov">          4 :     std::vector&lt;Tensor&lt;TType, 2&gt;&gt; unfolded_matrices;</span></a>
<a name="236"><span class="lineNum">     236 </span><span class="lineCov">         30 :     for_sequence&lt;TRank&gt;([&amp;](auto i) { unfolded_matrices.push_back(tensor_algebra::unfold&lt;i&gt;(tensor)); });</span></a>
<a name="237"><span class="lineNum">     237 </span>            : </a>
<a name="238"><span class="lineNum">     238 </span>            :     // Perform SVD guess for parafac decomposition procedure</a>
<a name="239"><span class="lineNum">     239 </span><span class="lineCov">          4 :     std::vector&lt;Tensor&lt;TType, 2&gt;&gt; factors = initialize_cp&lt;TRank&gt;(unfolded_matrices, rank);</span></a>
<a name="240"><span class="lineNum">     240 </span>            : </a>
<a name="241"><span class="lineNum">     241 </span><span class="lineCov">          4 :     double tensor_norm = norm(tensor);</span></a>
<a name="242"><span class="lineNum">     242 </span><span class="lineCov">          4 :     size_t nelem       = 1;</span></a>
<a name="243"><span class="lineNum">     243 </span><span class="lineCov">          8 :     for_sequence&lt;TRank&gt;([&amp;](auto i) { nelem *= tensor.dim(i); });</span></a>
<a name="244"><span class="lineNum">     244 </span><span class="lineCov">          4 :     tensor_norm /= std::sqrt((double)nelem);</span></a>
<a name="245"><span class="lineNum">     245 </span>            : </a>
<a name="246"><span class="lineNum">     246 </span><span class="lineCov">          4 :     int    iter       = 0;</span></a>
<a name="247"><span class="lineNum">     247 </span><span class="lineCov">          4 :     bool   converged  = false;</span></a>
<a name="248"><span class="lineNum">     248 </span><span class="lineCov">          4 :     double prev_error = 0.0;</span></a>
<a name="249"><span class="lineNum">     249 </span><span class="lineCov">         28 :     while (iter &lt; n_iter_max) {</span></a>
<a name="250"><span class="lineNum">     250 </span><span class="lineCov">        180 :         for_sequence&lt;TRank&gt;([&amp;](auto n_ind) {</span></a>
<a name="251"><span class="lineNum">     251 </span>            :             // Form V and Khatri-Rao product intermediates</a>
<a name="252"><span class="lineNum">     252 </span><span class="lineCov">         92 :             Tensor&lt;TType, 2&gt; V;</span></a>
<a name="253"><span class="lineNum">     253 </span><span class="lineCov">         92 :             Tensor&lt;TType, 2&gt; KR;</span></a>
<a name="254"><span class="lineNum">     254 </span><span class="lineCov">         92 :             bool             first = true;</span></a>
<a name="255"><span class="lineNum">     255 </span>            : </a>
<a name="256"><span class="lineNum">     256 </span><span class="lineCov">        712 :             for_sequence&lt;TRank&gt;([&amp;](auto m_ind) {</span></a>
<a name="257"><span class="lineNum">     257 </span>            :                 if (m_ind != n_ind) {</a>
<a name="258"><span class="lineNum">     258 </span><span class="lineCov">        432 :                     Tensor&lt;TType, 2&gt; A_tA{&quot;V&quot;, rank, rank};</span></a>
<a name="259"><span class="lineNum">     259 </span>            :                     // A_tA = A^T[j] @ A[j]</a>
<a name="260"><span class="lineNum">     260 </span>            :                     // println(&quot;iter {}, mind {}&quot;, iter, m_ind);</a>
<a name="261"><span class="lineNum">     261 </span>            :                     // println(factors[m_ind]);</a>
<a name="262"><span class="lineNum">     262 </span><span class="lineCov">        216 :                     einsum(0.0, Indices{r, s}, &amp;A_tA, 1.0, Indices{I, r}, factors[m_ind], Indices{I, s}, factors[m_ind]);</span></a>
<a name="263"><span class="lineNum">     263 </span>            : </a>
<a name="264"><span class="lineNum">     264 </span><span class="lineCov">        216 :                     if (first) {</span></a>
<a name="265"><span class="lineNum">     265 </span><span class="lineCov">         92 :                         V     = A_tA;</span></a>
<a name="266"><span class="lineNum">     266 </span><span class="lineCov">         92 :                         KR    = factors[m_ind];</span></a>
<a name="267"><span class="lineNum">     267 </span><span class="lineCov">         92 :                         first = false;</span></a>
<a name="268"><span class="lineNum">     268 </span>            :                     } else {</a>
<a name="269"><span class="lineNum">     269 </span>            :                         // Uses a Hamamard Contraction to build V</a>
<a name="270"><span class="lineNum">     270 </span><span class="lineCov">        124 :                         Tensor&lt;TType, 2&gt; Vcopy = V;</span></a>
<a name="271"><span class="lineNum">     271 </span><span class="lineCov">        124 :                         einsum(0.0, Indices{r, s}, &amp;V, 1.0, Indices{r, s}, Vcopy, Indices{r, s}, A_tA);</span></a>
<a name="272"><span class="lineNum">     272 </span>            : </a>
<a name="273"><span class="lineNum">     273 </span>            :                         // Perform a Khatri-Rao contraction</a>
<a name="274"><span class="lineNum">     274 </span><span class="lineCov">        124 :                         KR = tensor_algebra::khatri_rao(Indices{I, r}, KR, Indices{M, r}, factors[m_ind]);</span></a>
<a name="275"><span class="lineNum">     275 </span><span class="lineCov">        124 :                     }</span></a>
<a name="276"><span class="lineNum">     276 </span><span class="lineCov">        216 :                 }</span></a>
<a name="277"><span class="lineNum">     277 </span>            :             });</a>
<a name="278"><span class="lineNum">     278 </span>            : </a>
<a name="279"><span class="lineNum">     279 </span>            :             // Update factors[n_ind]</a>
<a name="280"><span class="lineNum">     280 </span><span class="lineCov">         92 :             size_t ndim = tensor.dim(n_ind);</span></a>
<a name="281"><span class="lineNum">     281 </span>            : </a>
<a name="282"><span class="lineNum">     282 </span>            :             // Step 1: Matrix Multiplication</a>
<a name="283"><span class="lineNum">     283 </span><span class="lineCov">         92 :             einsum(0.0, Indices{I, r}, &amp;factors[n_ind], 1.0, Indices{I, K}, unfolded_matrices[n_ind], Indices{K, r}, KR);</span></a>
<a name="284"><span class="lineNum">     284 </span>            : </a>
<a name="285"><span class="lineNum">     285 </span>            :             // Step 2: Linear Solve (instead of inversion, for numerical stability, column-major ordering)</a>
<a name="286"><span class="lineNum">     286 </span><span class="lineCov">         92 :             linear_algebra::gesv(&amp;V, &amp;factors[n_ind]);</span></a>
<a name="287"><span class="lineNum">     287 </span><span class="lineCov">         92 :         });</span></a>
<a name="288"><span class="lineNum">     288 </span>            : </a>
<a name="289"><span class="lineNum">     289 </span>            :         // Check for convergence</a>
<a name="290"><span class="lineNum">     290 </span>            :         // Reconstruct Tensor based on the factors</a>
<a name="291"><span class="lineNum">     291 </span><span class="lineCov">         28 :         Tensor&lt;TType, TRank&gt; rec_tensor = parafac_reconstruct&lt;TRank&gt;(factors);</span></a>
<a name="292"><span class="lineNum">     292 </span>            : </a>
<a name="293"><span class="lineNum">     293 </span><span class="lineCov">         28 :         double const unnormalized_error = rmsd(rec_tensor, tensor);</span></a>
<a name="294"><span class="lineNum">     294 </span><span class="lineCov">         28 :         double const curr_error         = unnormalized_error / tensor_norm;</span></a>
<a name="295"><span class="lineNum">     295 </span><span class="lineCov">         28 :         double const delta              = std::abs(curr_error - prev_error);</span></a>
<a name="296"><span class="lineNum">     296 </span>            : </a>
<a name="297"><span class="lineNum">     297 </span>            :         // printf(&quot;    @CP Iteration %d, ERROR: %8.8f, DELTA: %8.8f\n&quot;, iter, curr_error, delta);</a>
<a name="298"><span class="lineNum">     298 </span>            : </a>
<a name="299"><span class="lineNum">     299 </span><span class="lineCov">         28 :         if (iter &gt;= 2 &amp;&amp; delta &lt; tolerance) {</span></a>
<a name="300"><span class="lineNum">     300 </span><span class="lineCov">          4 :             converged = true;</span></a>
<a name="301"><span class="lineNum">     301 </span>            :             break;</a>
<a name="302"><span class="lineNum">     302 </span>            :         }</a>
<a name="303"><span class="lineNum">     303 </span>            : </a>
<a name="304"><span class="lineNum">     304 </span><span class="lineCov">         24 :         prev_error = curr_error;</span></a>
<a name="305"><span class="lineNum">     305 </span><span class="lineCov">         24 :         iter += 1;</span></a>
<a name="306"><span class="lineNum">     306 </span>            :     }</a>
<a name="307"><span class="lineNum">     307 </span>            :     if (!converged) {</a>
<a name="308"><span class="lineNum">     308 </span><span class="lineNoCov">          0 :         println_warn(&quot;CP decomposition failed to converge in {} iterations&quot;, n_iter_max);</span></a>
<a name="309"><span class="lineNum">     309 </span>            :     }</a>
<a name="310"><span class="lineNum">     310 </span>            : </a>
<a name="311"><span class="lineNum">     311 </span>            :     // Return **non-normalized** factors</a>
<a name="312"><span class="lineNum">     312 </span><span class="lineCov">          4 :     return factors;</span></a>
<a name="313"><span class="lineNum">     313 </span><span class="lineCov">          4 : }</span></a>
<a name="314"><span class="lineNum">     314 </span>            : </a>
<a name="315"><span class="lineNum">     315 </span>            : /**</a>
<a name="316"><span class="lineNum">     316 </span>            :  * Weighted CANDECOMP/PARAFAC decomposition via alternating least squares (ALS).</a>
<a name="317"><span class="lineNum">     317 </span>            :  * Computes a rank-`rank` decomposition of `tensor` such that:</a>
<a name="318"><span class="lineNum">     318 </span>            :  *</a>
<a name="319"><span class="lineNum">     319 </span>            :  *   tensor = [| factor[0], ..., factors[-1] |].</a>
<a name="320"><span class="lineNum">     320 </span>            :  *   weights = The weights to multiply the tensor by</a>
<a name="321"><span class="lineNum">     321 </span>            :  */</a>
<a name="322"><span class="lineNum">     322 </span>            : template &lt;template &lt;typename, size_t&gt; typename TTensor, size_t TRank, typename TType = double&gt;</a>
<a name="323"><span class="lineNum">     323 </span>            : auto weighted_parafac(const TTensor&lt;TType, TRank&gt; &amp;tensor, const TTensor&lt;TType, 1&gt; &amp;weights, size_t rank, int n_iter_max = 100,</a>
<a name="324"><span class="lineNum">     324 </span>            :                       double tolerance = 1.e-8) -&gt; std::vector&lt;Tensor&lt;TType, 2&gt;&gt; {</a>
<a name="325"><span class="lineNum">     325 </span>            :     LabeledSection0();</a>
<a name="326"><span class="lineNum">     326 </span>            : </a>
<a name="327"><span class="lineNum">     327 </span>            :     using namespace einsums::tensor_algebra;</a>
<a name="328"><span class="lineNum">     328 </span>            :     using namespace einsums::tensor_algebra::index;</a>
<a name="329"><span class="lineNum">     329 </span>            :     using vector = std::vector&lt;TType, AlignedAllocator&lt;TType, 64&gt;&gt;;</a>
<a name="330"><span class="lineNum">     330 </span>            : </a>
<a name="331"><span class="lineNum">     331 </span>            :     // Compute set of unfolded matrices (unweighted)</a>
<a name="332"><span class="lineNum">     332 </span>            :     std::vector&lt;Tensor&lt;TType, 2&gt;&gt; unfolded_matrices;</a>
<a name="333"><span class="lineNum">     333 </span>            :     for_sequence&lt;TRank&gt;([&amp;](auto i) { unfolded_matrices.push_back(tensor_algebra::unfold&lt;i&gt;(tensor)); });</a>
<a name="334"><span class="lineNum">     334 </span>            : </a>
<a name="335"><span class="lineNum">     335 </span>            :     // Perform SVD guess for parafac decomposition procedure</a>
<a name="336"><span class="lineNum">     336 </span>            :     std::vector&lt;Tensor&lt;TType, 2&gt;&gt; factors = initialize_cp&lt;TRank&gt;(unfolded_matrices, rank);</a>
<a name="337"><span class="lineNum">     337 </span>            : </a>
<a name="338"><span class="lineNum">     338 </span>            :     { // Define new scope (for memory optimization)</a>
<a name="339"><span class="lineNum">     339 </span>            :         // Create the weighted tensor</a>
<a name="340"><span class="lineNum">     340 </span>            :         Tensor&lt;TType, 1&gt; square_weights(&quot;square_weights&quot;, weights.dim(0));</a>
<a name="341"><span class="lineNum">     341 </span>            :         einsum(0.0, Indices{index::P}, &amp;square_weights, 1.0, Indices{index::P}, weights, Indices{index::P}, weights);</a>
<a name="342"><span class="lineNum">     342 </span>            :         Tensor&lt;TType, TRank&gt; weighted_tensor = weight_tensor(tensor, square_weights);</a>
<a name="343"><span class="lineNum">     343 </span>            :         for_sequence&lt;TRank&gt;([&amp;](auto i) {</a>
<a name="344"><span class="lineNum">     344 </span>            :             if (i != 0)</a>
<a name="345"><span class="lineNum">     345 </span>            :                 unfolded_matrices[i] = tensor_algebra::unfold&lt;i&gt;(weighted_tensor);</a>
<a name="346"><span class="lineNum">     346 </span>            :         });</a>
<a name="347"><span class="lineNum">     347 </span>            :     }</a>
<a name="348"><span class="lineNum">     348 </span>            : </a>
<a name="349"><span class="lineNum">     349 </span>            :     double tensor_norm = norm(tensor);</a>
<a name="350"><span class="lineNum">     350 </span>            :     size_t nelem       = 1;</a>
<a name="351"><span class="lineNum">     351 </span>            :     for_sequence&lt;TRank&gt;([&amp;](auto i) { nelem *= tensor.dim(i); });</a>
<a name="352"><span class="lineNum">     352 </span>            :     tensor_norm /= std::sqrt((double)nelem);</a>
<a name="353"><span class="lineNum">     353 </span>            : </a>
<a name="354"><span class="lineNum">     354 </span>            :     int    iter       = 0;</a>
<a name="355"><span class="lineNum">     355 </span>            :     bool   converged  = false;</a>
<a name="356"><span class="lineNum">     356 </span>            :     double prev_error = 0.0;</a>
<a name="357"><span class="lineNum">     357 </span>            :     while (iter &lt; n_iter_max) {</a>
<a name="358"><span class="lineNum">     358 </span>            :         size_t n = 0; // NOLINT</a>
<a name="359"><span class="lineNum">     359 </span>            :         for_sequence&lt;TRank&gt;([&amp;](auto n_ind) {</a>
<a name="360"><span class="lineNum">     360 </span>            :             // Form V and Khatri-Rao product intermediates</a>
<a name="361"><span class="lineNum">     361 </span>            :             Tensor&lt;TType, 2&gt; V;</a>
<a name="362"><span class="lineNum">     362 </span>            :             Tensor&lt;TType, 2&gt; KR;</a>
<a name="363"><span class="lineNum">     363 </span>            :             bool             first = true;</a>
<a name="364"><span class="lineNum">     364 </span>            : </a>
<a name="365"><span class="lineNum">     365 </span>            :             size_t m = 0; // NOLINT</a>
<a name="366"><span class="lineNum">     366 </span>            :             for_sequence&lt;TRank&gt;([&amp;](auto m_ind) {</a>
<a name="367"><span class="lineNum">     367 </span>            :                 if (m_ind != n_ind) {</a>
<a name="368"><span class="lineNum">     368 </span>            :                     Tensor&lt;TType, 2&gt; A_tA{&quot;V&quot;, rank, rank};</a>
<a name="369"><span class="lineNum">     369 </span>            :                     // A_tA = A^T[j] @ A[j]</a>
<a name="370"><span class="lineNum">     370 </span>            :                     if (m == 0) {</a>
<a name="371"><span class="lineNum">     371 </span>            :                         Tensor&lt;TType, 2&gt; weighted_factor = weight_tensor(factors[m_ind], weights);</a>
<a name="372"><span class="lineNum">     372 </span>            :                         einsum(0.0, Indices{r, s}, &amp;A_tA, 1.0, Indices{I, r}, weighted_factor, Indices{I, s}, weighted_factor);</a>
<a name="373"><span class="lineNum">     373 </span>            :                     } else {</a>
<a name="374"><span class="lineNum">     374 </span>            :                         einsum(0.0, Indices{r, s}, &amp;A_tA, 1.0, Indices{I, r}, factors[m_ind], Indices{I, s}, factors[m_ind]);</a>
<a name="375"><span class="lineNum">     375 </span>            :                     }</a>
<a name="376"><span class="lineNum">     376 </span>            : </a>
<a name="377"><span class="lineNum">     377 </span>            :                     if (first) {</a>
<a name="378"><span class="lineNum">     378 </span>            :                         V     = A_tA;</a>
<a name="379"><span class="lineNum">     379 </span>            :                         KR    = factors[m_ind];</a>
<a name="380"><span class="lineNum">     380 </span>            :                         first = false;</a>
<a name="381"><span class="lineNum">     381 </span>            :                     } else {</a>
<a name="382"><span class="lineNum">     382 </span>            :                         // Uses a Hamamard Contraction to build V</a>
<a name="383"><span class="lineNum">     383 </span>            :                         Tensor&lt;TType, 2&gt; Vcopy = V;</a>
<a name="384"><span class="lineNum">     384 </span>            :                         einsum(0.0, Indices{r, s}, &amp;V, 1.0, Indices{r, s}, Vcopy, Indices{r, s}, A_tA);</a>
<a name="385"><span class="lineNum">     385 </span>            : </a>
<a name="386"><span class="lineNum">     386 </span>            :                         // Perform a Khatri-Rao contraction</a>
<a name="387"><span class="lineNum">     387 </span>            :                         KR = tensor_algebra::khatri_rao(Indices{I, r}, KR, Indices{M, r}, factors[m_ind]);</a>
<a name="388"><span class="lineNum">     388 </span>            :                     }</a>
<a name="389"><span class="lineNum">     389 </span>            :                 }</a>
<a name="390"><span class="lineNum">     390 </span>            :                 m += 1;</a>
<a name="391"><span class="lineNum">     391 </span>            :             });</a>
<a name="392"><span class="lineNum">     392 </span>            : </a>
<a name="393"><span class="lineNum">     393 </span>            :             // Update factors[n_ind]</a>
<a name="394"><span class="lineNum">     394 </span>            :             size_t ndim = tensor.dim(n_ind);</a>
<a name="395"><span class="lineNum">     395 </span>            : </a>
<a name="396"><span class="lineNum">     396 </span>            :             // Step 1: Matrix Multiplication</a>
<a name="397"><span class="lineNum">     397 </span>            :             einsum(0.0, Indices{I, r}, &amp;factors[n_ind], 1.0, Indices{I, K}, unfolded_matrices[n_ind], Indices{K, r}, KR);</a>
<a name="398"><span class="lineNum">     398 </span>            : </a>
<a name="399"><span class="lineNum">     399 </span>            :             // Step 2: Linear Solve (instead of inversion, for numerical stability, column-major ordering)</a>
<a name="400"><span class="lineNum">     400 </span>            :             linear_algebra::gesv(&amp;V, &amp;factors[n_ind]);</a>
<a name="401"><span class="lineNum">     401 </span>            : </a>
<a name="402"><span class="lineNum">     402 </span>            :             n += 1;</a>
<a name="403"><span class="lineNum">     403 </span>            :         });</a>
<a name="404"><span class="lineNum">     404 </span>            : </a>
<a name="405"><span class="lineNum">     405 </span>            :         // Check for convergence</a>
<a name="406"><span class="lineNum">     406 </span>            :         // Reconstruct Tensor based on the factors</a>
<a name="407"><span class="lineNum">     407 </span>            :         Tensor&lt;TType, TRank&gt; rec_tensor = parafac_reconstruct&lt;TRank&gt;(factors);</a>
<a name="408"><span class="lineNum">     408 </span>            : </a>
<a name="409"><span class="lineNum">     409 </span>            :         double const unnormalized_error = rmsd(rec_tensor, tensor);</a>
<a name="410"><span class="lineNum">     410 </span>            :         double const curr_error         = unnormalized_error / tensor_norm;</a>
<a name="411"><span class="lineNum">     411 </span>            :         double const delta              = std::abs(curr_error - prev_error);</a>
<a name="412"><span class="lineNum">     412 </span>            : </a>
<a name="413"><span class="lineNum">     413 </span>            :         // printf(&quot;    @CP Iteration %d, ERROR: %8.8f, DELTA: %8.8f\n&quot;, iter, curr_error, delta);</a>
<a name="414"><span class="lineNum">     414 </span>            : </a>
<a name="415"><span class="lineNum">     415 </span>            :         if (iter &gt;= 1 &amp;&amp; delta &lt; tolerance) {</a>
<a name="416"><span class="lineNum">     416 </span>            :             converged = true;</a>
<a name="417"><span class="lineNum">     417 </span>            :             break;</a>
<a name="418"><span class="lineNum">     418 </span>            :         }</a>
<a name="419"><span class="lineNum">     419 </span>            : </a>
<a name="420"><span class="lineNum">     420 </span>            :         prev_error = curr_error;</a>
<a name="421"><span class="lineNum">     421 </span>            :         iter += 1;</a>
<a name="422"><span class="lineNum">     422 </span>            :     }</a>
<a name="423"><span class="lineNum">     423 </span>            :     if (!converged) {</a>
<a name="424"><span class="lineNum">     424 </span>            :         println_warn(&quot;CP decomposition failed to converge in {} iterations&quot;, n_iter_max);</a>
<a name="425"><span class="lineNum">     425 </span>            :     }</a>
<a name="426"><span class="lineNum">     426 </span>            : </a>
<a name="427"><span class="lineNum">     427 </span>            :     // Return **non-normalized** factors</a>
<a name="428"><span class="lineNum">     428 </span>            :     return factors;</a>
<a name="429"><span class="lineNum">     429 </span>            : }</a>
<a name="430"><span class="lineNum">     430 </span>            : </a>
<a name="431"><span class="lineNum">     431 </span>            : template &lt;template &lt;typename, size_t&gt; typename TTensor, size_t TRank, typename TType = double&gt;</a>
<a name="432"><span class="lineNum">     432 </span><span class="lineCov">          6 : auto tucker_reconstruct(const TTensor&lt;TType, TRank&gt; &amp;g_tensor, const std::vector&lt;TTensor&lt;TType, 2&gt;&gt; &amp;factors) {</span></a>
<a name="433"><span class="lineNum">     433 </span><span class="lineCov">         18 :     LabeledSection0();</span></a>
<a name="434"><span class="lineNum">     434 </span>            : </a>
<a name="435"><span class="lineNum">     435 </span>            :     // Dimension workspace for temps</a>
<a name="436"><span class="lineNum">     436 </span><span class="lineCov">          6 :     Dim&lt;TRank&gt; dims_buffer = g_tensor.dims();</span></a>
<a name="437"><span class="lineNum">     437 </span>            :     // Buffers to hold intermediates while rebuilding the tensor</a>
<a name="438"><span class="lineNum">     438 </span>            :     Tensor&lt;TType, TRank&gt; *old_tensor_buffer;</a>
<a name="439"><span class="lineNum">     439 </span>            :     Tensor&lt;TType, TRank&gt; *new_tensor_buffer;</a>
<a name="440"><span class="lineNum">     440 </span>            : </a>
<a name="441"><span class="lineNum">     441 </span><span class="lineCov">          6 :     old_tensor_buffer  = new Tensor(dims_buffer);</span></a>
<a name="442"><span class="lineNum">     442 </span><span class="lineCov">          6 :     *old_tensor_buffer = g_tensor;</span></a>
<a name="443"><span class="lineNum">     443 </span>            : </a>
<a name="444"><span class="lineNum">     444 </span>            :     // Reform the tensor (with all its intermediates)</a>
<a name="445"><span class="lineNum">     445 </span><span class="lineCov">         26 :     for_sequence&lt;TRank&gt;([&amp;](auto i) {</span></a>
<a name="446"><span class="lineNum">     446 </span><span class="lineCov">         20 :         size_t full_idx   = factors[i].dim(0);</span></a>
<a name="447"><span class="lineNum">     447 </span><span class="lineCov">         20 :         dims_buffer[i]    = full_idx;</span></a>
<a name="448"><span class="lineNum">     448 </span><span class="lineCov">         20 :         new_tensor_buffer = new Tensor(dims_buffer);</span></a>
<a name="449"><span class="lineNum">     449 </span><span class="lineCov">         20 :         new_tensor_buffer-&gt;zero();</span></a>
<a name="450"><span class="lineNum">     450 </span>            : </a>
<a name="451"><span class="lineNum">     451 </span><span class="lineCov">         20 :         auto source_dims = get_dim_ranges&lt;TRank&gt;(*old_tensor_buffer);</span></a>
<a name="452"><span class="lineNum">     452 </span>            : </a>
<a name="453"><span class="lineNum">     453 </span><span class="lineCov">        880 :         for (auto source_combination : std::apply(ranges::views::cartesian_product, source_dims)) {</span></a>
<a name="454"><span class="lineNum">     454 </span><span class="lineCov">       1404 :             for (size_t n = 0; n &lt; full_idx; n++) {</span></a>
<a name="455"><span class="lineNum">     455 </span><span class="lineCov">       1020 :                 auto target_combination         = source_combination;</span></a>
<a name="456"><span class="lineNum">     456 </span><span class="lineCov">       1020 :                 std::get&lt;i&gt;(target_combination) = n;</span></a>
<a name="457"><span class="lineNum">     457 </span>            : </a>
<a name="458"><span class="lineNum">     458 </span><span class="lineCov">       1020 :                 TType &amp;source = std::apply(*old_tensor_buffer, source_combination);</span></a>
<a name="459"><span class="lineNum">     459 </span><span class="lineCov">       1020 :                 TType &amp;target = std::apply(*new_tensor_buffer, target_combination);</span></a>
<a name="460"><span class="lineNum">     460 </span>            : </a>
<a name="461"><span class="lineNum">     461 </span><span class="lineCov">       1020 :                 target += source * factors[i](n, std::get&lt;i&gt;(source_combination));</span></a>
<a name="462"><span class="lineNum">     462 </span>            :             }</a>
<a name="463"><span class="lineNum">     463 </span>            :         }</a>
<a name="464"><span class="lineNum">     464 </span>            : </a>
<a name="465"><span class="lineNum">     465 </span><span class="lineCov">         20 :         delete old_tensor_buffer;</span></a>
<a name="466"><span class="lineNum">     466 </span><span class="lineCov">         20 :         old_tensor_buffer = new_tensor_buffer;</span></a>
<a name="467"><span class="lineNum">     467 </span>            :     });</a>
<a name="468"><span class="lineNum">     468 </span>            : </a>
<a name="469"><span class="lineNum">     469 </span><span class="lineCov">          6 :     Tensor&lt;TType, TRank&gt; new_tensor = *(new_tensor_buffer);</span></a>
<a name="470"><span class="lineNum">     470 </span>            :     // Only delete one of the buffers, to avoid a double free</a>
<a name="471"><span class="lineNum">     471 </span><span class="lineCov">          6 :     delete new_tensor_buffer;</span></a>
<a name="472"><span class="lineNum">     472 </span>            : </a>
<a name="473"><span class="lineNum">     473 </span><span class="lineCov">          6 :     return new_tensor;</span></a>
<a name="474"><span class="lineNum">     474 </span><span class="lineCov">          6 : }</span></a>
<a name="475"><span class="lineNum">     475 </span>            : </a>
<a name="476"><span class="lineNum">     476 </span>            : template &lt;size_t TRank, typename TType = double&gt;</a>
<a name="477"><span class="lineNum">     477 </span><span class="lineCov">         39 : auto initialize_tucker(std::vector&lt;Tensor&lt;TType, 2&gt;&gt; &amp;folds, std::vector&lt;size_t&gt; &amp;ranks) -&gt; std::vector&lt;Tensor&lt;TType, 2&gt;&gt; {</span></a>
<a name="478"><span class="lineNum">     478 </span><span class="lineCov">         78 :     LabeledSection0();</span></a>
<a name="479"><span class="lineNum">     479 </span>            : </a>
<a name="480"><span class="lineNum">     480 </span><span class="lineCov">         39 :     std::vector&lt;Tensor&lt;TType, 2&gt;&gt; factors;</span></a>
<a name="481"><span class="lineNum">     481 </span>            : </a>
<a name="482"><span class="lineNum">     482 </span>            :     // Perform compile-time looping.</a>
<a name="483"><span class="lineNum">     483 </span><span class="lineCov">        165 :     for_sequence&lt;TRank&gt;([&amp;](auto i) {</span></a>
<a name="484"><span class="lineNum">     484 </span><span class="lineCov">        126 :         size_t rank    = ranks[i];</span></a>
<a name="485"><span class="lineNum">     485 </span><span class="lineCov">        126 :         auto [U, S, _] = linear_algebra::svd_dd(folds[i]);</span></a>
<a name="486"><span class="lineNum">     486 </span>            : </a>
<a name="487"><span class="lineNum">     487 </span>            :         // println(tensor_algebra::unfold&lt;i&gt;(tensor));</a>
<a name="488"><span class="lineNum">     488 </span>            :         // println(S);</a>
<a name="489"><span class="lineNum">     489 </span>            : </a>
<a name="490"><span class="lineNum">     490 </span><span class="lineCov">        126 :         if (folds[i].dim(0) &lt; rank) {</span></a>
<a name="491"><span class="lineNum">     491 </span>            :             // i is an std::integral_constant the &quot;()&quot; obtains the underlying value.</a>
<a name="492"><span class="lineNum">     492 </span><span class="lineNoCov">          0 :             println_warn(&quot;dimension {} size {} is less than the requested decomposition rank {}&quot;, i(), folds[i].dim(0), rank);</span></a>
<a name="493"><span class="lineNum">     493 </span>            :             // TODO: Need to pad U up to rank</a>
<a name="494"><span class="lineNum">     494 </span>            :         }</a>
<a name="495"><span class="lineNum">     495 </span>            : </a>
<a name="496"><span class="lineNum">     496 </span>            :         // Need to save the factors</a>
<a name="497"><span class="lineNum">     497 </span><span class="lineCov">        126 :         factors.emplace_back(Tensor{U(All, Range{0, rank})});</span></a>
<a name="498"><span class="lineNum">     498 </span><span class="lineCov">        126 :     });</span></a>
<a name="499"><span class="lineNum">     499 </span>            : </a>
<a name="500"><span class="lineNum">     500 </span><span class="lineCov">         39 :     return factors;</span></a>
<a name="501"><span class="lineNum">     501 </span><span class="lineCov">         39 : }</span></a>
<a name="502"><span class="lineNum">     502 </span>            : </a>
<a name="503"><span class="lineNum">     503 </span>            : /**</a>
<a name="504"><span class="lineNum">     504 </span>            :  * Tucker decomposition of a tensor via Higher-Order SVD (HO-SVD).</a>
<a name="505"><span class="lineNum">     505 </span>            :  * Computes a rank-`rank` decomposition of `tensor` such that:</a>
<a name="506"><span class="lineNum">     506 </span>            :  *</a>
<a name="507"><span class="lineNum">     507 </span>            :  *   tensor = [|weights[r0][r1]...; factor[0][r0], ..., factors[-1][rn] |]</a>
<a name="508"><span class="lineNum">     508 </span>            :  */</a>
<a name="509"><span class="lineNum">     509 </span>            : template &lt;template &lt;typename, size_t&gt; typename TTensor, size_t TRank, typename TType = double&gt;</a>
<a name="510"><span class="lineNum">     510 </span><span class="lineCov">         39 : auto tucker_ho_svd(const TTensor&lt;TType, TRank&gt; &amp;tensor, std::vector&lt;size_t&gt; &amp;ranks,</span></a>
<a name="511"><span class="lineNum">     511 </span>            :                    const std::vector&lt;Tensor&lt;TType, 2&gt;&gt; &amp;folds = std::vector&lt;Tensor&lt;TType, 2&gt;&gt;())</a>
<a name="512"><span class="lineNum">     512 </span>            :     -&gt; std::tuple&lt;Tensor&lt;TType, TRank&gt;, std::vector&lt;Tensor&lt;TType, 2&gt;&gt;&gt; {</a>
<a name="513"><span class="lineNum">     513 </span><span class="lineCov">         78 :     LabeledSection0();</span></a>
<a name="514"><span class="lineNum">     514 </span>            : </a>
<a name="515"><span class="lineNum">     515 </span>            :     using namespace einsums::tensor_algebra;</a>
<a name="516"><span class="lineNum">     516 </span>            :     using namespace einsums::tensor_algebra::index;</a>
<a name="517"><span class="lineNum">     517 </span>            : </a>
<a name="518"><span class="lineNum">     518 </span>            :     // Compute set of unfolded matrices</a>
<a name="519"><span class="lineNum">     519 </span><span class="lineCov">         39 :     std::vector&lt;Tensor&lt;TType, 2&gt;&gt; unfolded_matrices;</span></a>
<a name="520"><span class="lineNum">     520 </span><span class="lineCov">         39 :     if (!folds.size()) {</span></a>
<a name="521"><span class="lineNum">     521 </span><span class="lineCov">         52 :         for_sequence&lt;TRank&gt;([&amp;](auto i) { unfolded_matrices.push_back(tensor_algebra::unfold&lt;i&gt;(tensor)); });</span></a>
<a name="522"><span class="lineNum">     522 </span>            :     } else {</a>
<a name="523"><span class="lineNum">     523 </span><span class="lineCov">         33 :         unfolded_matrices = folds;</span></a>
<a name="524"><span class="lineNum">     524 </span>            :     }</a>
<a name="525"><span class="lineNum">     525 </span>            : </a>
<a name="526"><span class="lineNum">     526 </span>            :     // Perform SVD guess for tucker decomposition procedure</a>
<a name="527"><span class="lineNum">     527 </span><span class="lineCov">         39 :     std::vector&lt;Tensor&lt;TType, 2&gt;&gt; factors = initialize_tucker&lt;TRank&gt;(unfolded_matrices, ranks);</span></a>
<a name="528"><span class="lineNum">     528 </span>            : </a>
<a name="529"><span class="lineNum">     529 </span>            :     // Get the dimension workspace for temps</a>
<a name="530"><span class="lineNum">     530 </span><span class="lineCov">         39 :     Dim&lt;TRank&gt; dims_buffer = tensor.dims();</span></a>
<a name="531"><span class="lineNum">     531 </span>            :     // Make buffers to hold intermediates while forming G</a>
<a name="532"><span class="lineNum">     532 </span>            :     Tensor&lt;TType, TRank&gt; *old_g_buffer;</a>
<a name="533"><span class="lineNum">     533 </span>            :     Tensor&lt;TType, TRank&gt; *new_g_buffer;</a>
<a name="534"><span class="lineNum">     534 </span>            : </a>
<a name="535"><span class="lineNum">     535 </span><span class="lineCov">         39 :     old_g_buffer  = new Tensor(dims_buffer);</span></a>
<a name="536"><span class="lineNum">     536 </span><span class="lineCov">         39 :     *old_g_buffer = tensor;</span></a>
<a name="537"><span class="lineNum">     537 </span>            : </a>
<a name="538"><span class="lineNum">     538 </span>            :     // Form G (with all of its intermediates)</a>
<a name="539"><span class="lineNum">     539 </span><span class="lineCov">        165 :     for_sequence&lt;TRank&gt;([&amp;](auto i) {</span></a>
<a name="540"><span class="lineNum">     540 </span><span class="lineCov">        126 :         size_t rank    = ranks[i];</span></a>
<a name="541"><span class="lineNum">     541 </span><span class="lineCov">        126 :         dims_buffer[i] = rank;</span></a>
<a name="542"><span class="lineNum">     542 </span><span class="lineCov">        126 :         new_g_buffer   = new Tensor(dims_buffer);</span></a>
<a name="543"><span class="lineNum">     543 </span><span class="lineCov">        126 :         new_g_buffer-&gt;zero();</span></a>
<a name="544"><span class="lineNum">     544 </span>            : </a>
<a name="545"><span class="lineNum">     545 </span><span class="lineCov">        126 :         auto source_dims = get_dim_ranges&lt;TRank&gt;(*old_g_buffer);</span></a>
<a name="546"><span class="lineNum">     546 </span>            : </a>
<a name="547"><span class="lineNum">     547 </span><span class="lineCov">       5886 :         for (auto source_combination : std::apply(ranges::views::cartesian_product, source_dims)) {</span></a>
<a name="548"><span class="lineNum">     548 </span><span class="lineCov">       7840 :             for (size_t r = 0; r &lt; rank; r++) {</span></a>
<a name="549"><span class="lineNum">     549 </span><span class="lineCov">       5280 :                 auto target_combination         = source_combination;</span></a>
<a name="550"><span class="lineNum">     550 </span><span class="lineCov">       5280 :                 std::get&lt;i&gt;(target_combination) = r;</span></a>
<a name="551"><span class="lineNum">     551 </span>            : </a>
<a name="552"><span class="lineNum">     552 </span><span class="lineCov">       5280 :                 TType &amp;source = std::apply(*old_g_buffer, source_combination);</span></a>
<a name="553"><span class="lineNum">     553 </span><span class="lineCov">       5280 :                 TType &amp;target = std::apply(*new_g_buffer, target_combination);</span></a>
<a name="554"><span class="lineNum">     554 </span>            : </a>
<a name="555"><span class="lineNum">     555 </span><span class="lineCov">       5280 :                 target += source * factors[i](std::get&lt;i&gt;(source_combination), r);</span></a>
<a name="556"><span class="lineNum">     556 </span>            :             }</a>
<a name="557"><span class="lineNum">     557 </span>            :         }</a>
<a name="558"><span class="lineNum">     558 </span>            : </a>
<a name="559"><span class="lineNum">     559 </span><span class="lineCov">        126 :         delete old_g_buffer;</span></a>
<a name="560"><span class="lineNum">     560 </span><span class="lineCov">        126 :         old_g_buffer = new_g_buffer;</span></a>
<a name="561"><span class="lineNum">     561 </span>            :     });</a>
<a name="562"><span class="lineNum">     562 </span>            : </a>
<a name="563"><span class="lineNum">     563 </span><span class="lineCov">         39 :     Tensor&lt;TType, TRank&gt; g_tensor = *(new_g_buffer);</span></a>
<a name="564"><span class="lineNum">     564 </span>            :     // ONLY delete one of the buffers, to avoid a double free</a>
<a name="565"><span class="lineNum">     565 </span><span class="lineCov">         39 :     delete new_g_buffer;</span></a>
<a name="566"><span class="lineNum">     566 </span>            : </a>
<a name="567"><span class="lineNum">     567 </span><span class="lineCov">         39 :     return std::make_tuple(g_tensor, factors);</span></a>
<a name="568"><span class="lineNum">     568 </span><span class="lineCov">         39 : }</span></a>
<a name="569"><span class="lineNum">     569 </span>            : </a>
<a name="570"><span class="lineNum">     570 </span>            : /**</a>
<a name="571"><span class="lineNum">     571 </span>            :  * Tucker decomposition via Higher-Order Orthogonal Inversion (HO-OI).</a>
<a name="572"><span class="lineNum">     572 </span>            :  * Computes a rank-`rank` decomposition of `tensor` such that:</a>
<a name="573"><span class="lineNum">     573 </span>            :  *</a>
<a name="574"><span class="lineNum">     574 </span>            :  *   tensor = [|weights[r0][r1]...; factor[0][r1], ..., factors[-1][rn] |].</a>
<a name="575"><span class="lineNum">     575 </span>            :  */</a>
<a name="576"><span class="lineNum">     576 </span>            : template &lt;template &lt;typename, size_t&gt; typename TTensor, size_t TRank, typename TType = double&gt;</a>
<a name="577"><span class="lineNum">     577 </span><span class="lineCov">          3 : auto tucker_ho_oi(const TTensor&lt;TType, TRank&gt; &amp;tensor, std::vector&lt;size_t&gt; &amp;ranks, int n_iter_max = 100,</span></a>
<a name="578"><span class="lineNum">     578 </span>            :                   double tolerance = 1.e-8) -&gt; std::tuple&lt;TTensor&lt;TType, TRank&gt;, std::vector&lt;Tensor&lt;TType, 2&gt;&gt;&gt; {</a>
<a name="579"><span class="lineNum">     579 </span><span class="lineCov">          6 :     LabeledSection0();</span></a>
<a name="580"><span class="lineNum">     580 </span>            : </a>
<a name="581"><span class="lineNum">     581 </span>            :     // Use HO SVD as a starting guess</a>
<a name="582"><span class="lineNum">     582 </span><span class="lineCov">          3 :     auto ho_svd_guess = tucker_ho_svd(tensor, ranks);</span></a>
<a name="583"><span class="lineNum">     583 </span><span class="lineCov">          3 :     auto g_tensor     = std::get&lt;0&gt;(ho_svd_guess);</span></a>
<a name="584"><span class="lineNum">     584 </span><span class="lineCov">          3 :     auto factors      = std::get&lt;1&gt;(ho_svd_guess);</span></a>
<a name="585"><span class="lineNum">     585 </span>            : </a>
<a name="586"><span class="lineNum">     586 </span>            :     int  iter      = 0;</a>
<a name="587"><span class="lineNum">     587 </span><span class="lineCov">         33 :     bool converged = false;</span></a>
<a name="588"><span class="lineNum">     588 </span><span class="lineCov">         99 :     while (iter &lt; n_iter_max) {</span></a>
<a name="589"><span class="lineNum">     589 </span><span class="lineCov">         33 :         std::vector&lt;Tensor&lt;TType, 2&gt;&gt; new_folds;</span></a>
<a name="590"><span class="lineNum">     590 </span>            : </a>
<a name="591"><span class="lineNum">     591 </span><span class="lineCov">        245 :         for_sequence&lt;TRank&gt;([&amp;](auto i) {</span></a>
<a name="592"><span class="lineNum">     592 </span>            :             // Make the workspace for the contraction</a>
<a name="593"><span class="lineNum">     593 </span><span class="lineCov">        106 :             Dim&lt;TRank&gt; dims_buffer = tensor.dims();</span></a>
<a name="594"><span class="lineNum">     594 </span>            :             // Make buffers to form intermediates while forming new folds</a>
<a name="595"><span class="lineNum">     595 </span>            :             Tensor&lt;TType, TRank&gt; *old_fold_buffer;</a>
<a name="596"><span class="lineNum">     596 </span>            :             Tensor&lt;TType, TRank&gt; *new_fold_buffer;</a>
<a name="597"><span class="lineNum">     597 </span>            : </a>
<a name="598"><span class="lineNum">     598 </span>            :             // Initialize old fold buffer to the tensor</a>
<a name="599"><span class="lineNum">     599 </span><span class="lineCov">        106 :             old_fold_buffer  = new Tensor(dims_buffer);</span></a>
<a name="600"><span class="lineNum">     600 </span><span class="lineCov">        106 :             *old_fold_buffer = tensor;</span></a>
<a name="601"><span class="lineNum">     601 </span>            : </a>
<a name="602"><span class="lineNum">     602 </span><span class="lineCov">        419 :             for_sequence&lt;TRank&gt;([&amp;](auto j) {</span></a>
<a name="603"><span class="lineNum">     603 </span>            :                 if (j != i) {</a>
<a name="604"><span class="lineNum">     604 </span><span class="lineCov">        240 :                     size_t rank     = ranks[j];</span></a>
<a name="605"><span class="lineNum">     605 </span><span class="lineCov">        240 :                     dims_buffer[j]  = rank;</span></a>
<a name="606"><span class="lineNum">     606 </span><span class="lineCov">        240 :                     new_fold_buffer = new Tensor(dims_buffer);</span></a>
<a name="607"><span class="lineNum">     607 </span><span class="lineCov">        240 :                     new_fold_buffer-&gt;zero();</span></a>
<a name="608"><span class="lineNum">     608 </span>            : </a>
<a name="609"><span class="lineNum">     609 </span><span class="lineCov">        240 :                     auto source_dims = get_dim_ranges&lt;TRank&gt;(*old_fold_buffer);</span></a>
<a name="610"><span class="lineNum">     610 </span>            : </a>
<a name="611"><span class="lineNum">     611 </span><span class="lineCov">      13110 :                     for (auto source_combination : std::apply(ranges::views::cartesian_product, source_dims)) {</span></a>
<a name="612"><span class="lineNum">     612 </span><span class="lineCov">      17678 :                         for (size_t r = 0; r &lt; rank; r++) {</span></a>
<a name="613"><span class="lineNum">     613 </span><span class="lineCov">      11892 :                             auto target_combination         = source_combination;</span></a>
<a name="614"><span class="lineNum">     614 </span><span class="lineCov">      11892 :                             std::get&lt;j&gt;(target_combination) = r;</span></a>
<a name="615"><span class="lineNum">     615 </span>            : </a>
<a name="616"><span class="lineNum">     616 </span><span class="lineCov">      11892 :                             TType &amp;source = std::apply(*old_fold_buffer, source_combination);</span></a>
<a name="617"><span class="lineNum">     617 </span><span class="lineCov">      11892 :                             TType &amp;target = std::apply(*new_fold_buffer, target_combination);</span></a>
<a name="618"><span class="lineNum">     618 </span>            : </a>
<a name="619"><span class="lineNum">     619 </span><span class="lineCov">      11892 :                             target += source * factors[j](std::get&lt;j&gt;(source_combination), r);</span></a>
<a name="620"><span class="lineNum">     620 </span>            :                         }</a>
<a name="621"><span class="lineNum">     621 </span>            :                     }</a>
<a name="622"><span class="lineNum">     622 </span>            : </a>
<a name="623"><span class="lineNum">     623 </span><span class="lineCov">        240 :                     delete old_fold_buffer;</span></a>
<a name="624"><span class="lineNum">     624 </span><span class="lineCov">        240 :                     old_fold_buffer = new_fold_buffer;</span></a>
<a name="625"><span class="lineNum">     625 </span>            :                 }</a>
<a name="626"><span class="lineNum">     626 </span>            :             });</a>
<a name="627"><span class="lineNum">     627 </span>            : </a>
<a name="628"><span class="lineNum">     628 </span><span class="lineCov">        106 :             Tensor&lt;TType, 2&gt; new_fold = tensor_algebra::unfold&lt;i&gt;(*new_fold_buffer);</span></a>
<a name="629"><span class="lineNum">     629 </span><span class="lineCov">        106 :             new_folds.push_back(new_fold);</span></a>
<a name="630"><span class="lineNum">     630 </span>            : </a>
<a name="631"><span class="lineNum">     631 </span>            :             // Only delete once to avoid a double free</a>
<a name="632"><span class="lineNum">     632 </span><span class="lineCov">        106 :             delete new_fold_buffer;</span></a>
<a name="633"><span class="lineNum">     633 </span><span class="lineCov">        106 :         });</span></a>
<a name="634"><span class="lineNum">     634 </span>            : </a>
<a name="635"><span class="lineNum">     635 </span>            :         // Reformulate guess based on HO SVD of new_folds</a>
<a name="636"><span class="lineNum">     636 </span><span class="lineCov">         33 :         auto new_ho_svd   = tucker_ho_svd(tensor, ranks, new_folds);</span></a>
<a name="637"><span class="lineNum">     637 </span><span class="lineCov">         33 :         auto new_g_tensor = std::get&lt;0&gt;(new_ho_svd);</span></a>
<a name="638"><span class="lineNum">     638 </span><span class="lineCov">         33 :         auto new_factors  = std::get&lt;1&gt;(new_ho_svd);</span></a>
<a name="639"><span class="lineNum">     639 </span>            : </a>
<a name="640"><span class="lineNum">     640 </span>            :         // Check for convergence</a>
<a name="641"><span class="lineNum">     641 </span><span class="lineCov">         33 :         double rmsd_max = rmsd(new_g_tensor, g_tensor);</span></a>
<a name="642"><span class="lineNum">     642 </span><span class="lineCov">        139 :         for_sequence&lt;TRank&gt;([&amp;](auto n) { rmsd_max = std::max(rmsd_max, rmsd(new_factors[n], factors[n])); });</span></a>
<a name="643"><span class="lineNum">     643 </span>            : </a>
<a name="644"><span class="lineNum">     644 </span>            :         // Update G and factors</a>
<a name="645"><span class="lineNum">     645 </span><span class="lineCov">         33 :         g_tensor = new_g_tensor;</span></a>
<a name="646"><span class="lineNum">     646 </span><span class="lineCov">         33 :         factors  = new_factors;</span></a>
<a name="647"><span class="lineNum">     647 </span>            : </a>
<a name="648"><span class="lineNum">     648 </span><span class="lineCov">         33 :         if (rmsd_max &lt; tolerance) {</span></a>
<a name="649"><span class="lineNum">     649 </span><span class="lineCov">          3 :             converged = true;</span></a>
<a name="650"><span class="lineNum">     650 </span>            :             break;</a>
<a name="651"><span class="lineNum">     651 </span>            :         }</a>
<a name="652"><span class="lineNum">     652 </span>            : </a>
<a name="653"><span class="lineNum">     653 </span><span class="lineCov">         30 :         iter += 1;</span></a>
<a name="654"><span class="lineNum">     654 </span>            :     }</a>
<a name="655"><span class="lineNum">     655 </span>            :     if (!converged) {</a>
<a name="656"><span class="lineNum">     656 </span><span class="lineNoCov">          0 :         println_warn(&quot;Tucker HO-OI decomposition failed to converge in {} iterations&quot;, n_iter_max);</span></a>
<a name="657"><span class="lineNum">     657 </span>            :     }</a>
<a name="658"><span class="lineNum">     658 </span>            : </a>
<a name="659"><span class="lineNum">     659 </span><span class="lineCov">          3 :     return std::make_tuple(g_tensor, factors);</span></a>
<a name="660"><span class="lineNum">     660 </span><span class="lineCov">          6 : }</span></a>
<a name="661"><span class="lineNum">     661 </span>            : </a>
<a name="662"><span class="lineNum">     662 </span>            : END_EINSUMS_NAMESPACE_HPP(einsums::decomposition)</a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../../../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="https://github.com/linux-test-project/lcov" target="_parent">LCOV version 1.16</a></td></tr>
  </table>
  <br>

</body>
</html>
